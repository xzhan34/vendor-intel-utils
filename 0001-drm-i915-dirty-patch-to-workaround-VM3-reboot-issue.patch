From 301ed3847819ae7f3f48de955f21f3b3a25c58f7 Mon Sep 17 00:00:00 2001
From: "Zhang, Xiaolin" <xiaolin.zhang@intel.com>
Date: Fri, 24 Nov 2023 21:20:13 -0500
Subject: [PATCH] drm/i915: dirty patch to workaround VM3 reboot issue.

this is a very very dirty patch to workaround VM3 reboot issue
quickly to unblock demo.

this is test purpose only.

Signed-off-by: Zhang, Xiaolin <xiaolin.zhang@intel.com>
---
 drivers/gpu/drm/i915/Makefile                 |  19 --
 drivers/gpu/drm/i915/display/icl_dsi.c        |  20 +-
 drivers/gpu/drm/i915/display/intel_bios.c     |  33 +--
 drivers/gpu/drm/i915/display/intel_cdclk.c    |   2 +-
 drivers/gpu/drm/i915/display/intel_ddi.c      |   8 +-
 drivers/gpu/drm/i915/display/intel_display.c  |   3 +-
 .../drm/i915/display/intel_display_power.c    |  15 --
 .../drm/i915/display/intel_display_power.h    |   1 -
 .../i915/display/intel_display_power_map.c    |  21 --
 .../i915/display/intel_display_power_map.h    |   2 -
 .../drm/i915/display/intel_display_types.h    |   2 -
 drivers/gpu/drm/i915/display/intel_dp.c       |  69 ++++-
 drivers/gpu/drm/i915/display/intel_dp_aux.c   |  32 +--
 drivers/gpu/drm/i915/display/intel_dpt.c      |   5 +
 drivers/gpu/drm/i915/display/intel_dsi_vbt.c  |   4 +-
 drivers/gpu/drm/i915/display/intel_opregion.c |   3 -
 drivers/gpu/drm/i915/display/intel_psr.c      |  67 +----
 drivers/gpu/drm/i915/display/intel_quirks.c   |   2 -
 drivers/gpu/drm/i915/display/intel_sdvo.c     |  58 ++--
 drivers/gpu/drm/i915/display/intel_snps_phy.c |  62 -----
 drivers/gpu/drm/i915/display/intel_tc.c       |  70 ++---
 drivers/gpu/drm/i915/display/intel_tc.h       |   3 +-
 .../drm/i915/display/skl_universal_plane.c    |   2 +-
 drivers/gpu/drm/i915/gem/i915_gem_context.c   |   8 +-
 drivers/gpu/drm/i915/gem/i915_gem_dmabuf.c    |   4 +-
 .../gpu/drm/i915/gem/i915_gem_execbuffer.c    |  15 +-
 drivers/gpu/drm/i915/gem/i915_gem_internal.c  |  15 --
 drivers/gpu/drm/i915/gem/i915_gem_shmem.c     |   2 +-
 drivers/gpu/drm/i915/gem/i915_gem_tiling.c    |  13 +-
 .../drm/i915/gem/selftests/i915_gem_context.c | 130 ++++-----
 drivers/gpu/drm/i915/gt/intel_context.c       |  11 +-
 drivers/gpu/drm/i915/gt/intel_context.h       |   6 +-
 drivers/gpu/drm/i915/gt/intel_context_types.h |   3 -
 drivers/gpu/drm/i915/gt/intel_engine_cs.c     |   7 -
 drivers/gpu/drm/i915/gt/intel_engine_types.h  |  15 --
 .../drm/i915/gt/intel_execlists_submission.c  |  21 +-
 drivers/gpu/drm/i915/gt/intel_ggtt.c          | 213 +--------------
 drivers/gpu/drm/i915/gt/intel_gt.c            |  62 +----
 .../gpu/drm/i915/gt/intel_gt_clock_utils.c    |   9 -
 .../gpu/drm/i915/gt/intel_gt_clock_utils.h    |   1 -
 drivers/gpu/drm/i915/gt/intel_gt_debugfs.c    |   2 -
 drivers/gpu/drm/i915/gt/intel_gt_pm.c         |   3 -
 drivers/gpu/drm/i915/gt/intel_gt_requests.c   |   2 +-
 drivers/gpu/drm/i915/gt/intel_gt_types.h      |   2 -
 drivers/gpu/drm/i915/gt/intel_gtt.c           |   6 -
 drivers/gpu/drm/i915/gt/intel_gtt.h           |  33 +--
 drivers/gpu/drm/i915/gt/intel_lrc.c           |   3 +-
 drivers/gpu/drm/i915/gt/intel_mocs.c          |   5 +-
 drivers/gpu/drm/i915/gt/intel_reset.c         |  79 +-----
 drivers/gpu/drm/i915/gt/intel_workarounds.c   |  12 -
 drivers/gpu/drm/i915/gt/selftest_execlists.c  |  60 ++---
 drivers/gpu/drm/i915/gt/selftest_hangcheck.c  |  51 ++--
 .../gt/uc/abi/guc_communication_ctb_abi.h     |  23 +-
 .../gpu/drm/i915/gt/uc/abi/guc_errors_abi.h   |   1 -
 drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h | 193 --------------
 .../gpu/drm/i915/gt/uc/abi/guc_messages_abi.h |  30 ---
 drivers/gpu/drm/i915/gt/uc/intel_guc.c        | 129 +--------
 drivers/gpu/drm/i915/gt/uc/intel_guc.h        |  27 +-
 drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c    |   3 -
 drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c     |  62 +----
 drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h     |   1 -
 .../gpu/drm/i915/gt/uc/intel_guc_debugfs.c    |  69 -----
 drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h   |  27 --
 .../gpu/drm/i915/gt/uc/intel_guc_submission.c | 114 +-------
 .../gpu/drm/i915/gt/uc/intel_guc_submission.h |   1 -
 drivers/gpu/drm/i915/gt/uc/intel_huc.h        |   3 +-
 drivers/gpu/drm/i915/gt/uc/intel_uc.c         | 112 +-------
 drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c      |  18 --
 drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h      |  22 +-
 drivers/gpu/drm/i915/gvt/debugfs.c            |  17 +-
 drivers/gpu/drm/i915/gvt/gtt.c                |  14 +-
 drivers/gpu/drm/i915/gvt/scheduler.c          |   1 -
 drivers/gpu/drm/i915/i915_active.c            |  31 +--
 drivers/gpu/drm/i915/i915_debugfs.c           |  11 -
 drivers/gpu/drm/i915/i915_debugfs_params.c    |  11 -
 drivers/gpu/drm/i915/i915_driver.c            |  72 +----
 drivers/gpu/drm/i915/i915_drm_client.c        |   6 +-
 drivers/gpu/drm/i915/i915_drv.h               |  15 --
 drivers/gpu/drm/i915/i915_gpu_error.c         |   4 +-
 drivers/gpu/drm/i915/i915_params.c            |  20 +-
 drivers/gpu/drm/i915/i915_params.h            |   7 +-
 drivers/gpu/drm/i915/i915_pci.c               |  60 +----
 drivers/gpu/drm/i915/i915_perf.c              |   9 +-
 drivers/gpu/drm/i915/i915_pmu.c               |   2 +-
 drivers/gpu/drm/i915/i915_reg.h               |   8 -
 drivers/gpu/drm/i915/i915_switcheroo.c        |   6 +-
 drivers/gpu/drm/i915/i915_sysfs.c             |   9 -
 drivers/gpu/drm/i915/i915_utils.c             |  58 ----
 drivers/gpu/drm/i915/i915_utils.h             |   5 -
 drivers/gpu/drm/i915/intel_device_info.h      |   1 -
 drivers/gpu/drm/i915/intel_pci_config.h       |   5 -
 drivers/gpu/drm/i915/intel_uncore.c           |  44 +--
 .../drm/i915/selftests/i915_live_selftests.h  |   3 -
 .../drm/i915/selftests/i915_mock_selftests.h  |   1 -
 .../drm/i915/selftests/i915_perf_selftests.h  |   1 -
 drivers/gpu/drm/i915/selftests/i915_request.c | 252 +++++++-----------
 .../i915/selftests/intel_scheduler_helpers.c  |   3 +-
 .../gpu/drm/i915/selftests/mock_gem_device.c  |   1 -
 98 files changed, 480 insertions(+), 2328 deletions(-)

diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index e202ea8a86bb..1f50b3cead94 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -216,25 +216,6 @@ i915-y += gt/uc/intel_uc.o \
 # graphics system controller (GSC) support
 i915-y += gt/intel_gsc.o
 
-# Virtualization support
-iov-y += \
-	i915_sriov.o \
-	i915_sriov_sysfs.o \
-	gt/iov/intel_iov.o \
-	gt/iov/intel_iov_debugfs.o \
-	gt/iov/intel_iov_event.o \
-	gt/iov/intel_iov_provisioning.o \
-	gt/iov/intel_iov_query.o \
-	gt/iov/intel_iov_relay.o \
-	gt/iov/intel_iov_service.o \
-	gt/iov/intel_iov_state.o \
-	gt/iov/intel_iov_sysfs.o
-i915-y += $(iov-y)
-
-iov-$(CONFIG_DRM_I915_SELFTEST) += \
-	gt/iov/selftests/iov_selftest_actions.o
-i915-y += $(iov-$(CONFIG_DRM_I915_SELFTEST))
-
 # modesetting core code
 i915-y += \
 	display/hsw_ips.o \
diff --git a/drivers/gpu/drm/i915/display/icl_dsi.c b/drivers/gpu/drm/i915/display/icl_dsi.c
index f90353a999be..ed4d93942dbd 100644
--- a/drivers/gpu/drm/i915/display/icl_dsi.c
+++ b/drivers/gpu/drm/i915/display/icl_dsi.c
@@ -299,21 +299,9 @@ static void configure_dual_link_mode(struct intel_encoder *encoder,
 {
 	struct drm_i915_private *dev_priv = to_i915(encoder->base.dev);
 	struct intel_dsi *intel_dsi = enc_to_intel_dsi(encoder);
-	i915_reg_t dss_ctl1_reg, dss_ctl2_reg;
 	u32 dss_ctl1;
 
-	/* FIXME: Move all DSS handling to intel_vdsc.c */
-	if (DISPLAY_VER(dev_priv) >= 12) {
-		struct intel_crtc *crtc = to_intel_crtc(pipe_config->uapi.crtc);
-
-		dss_ctl1_reg = ICL_PIPE_DSS_CTL1(crtc->pipe);
-		dss_ctl2_reg = ICL_PIPE_DSS_CTL2(crtc->pipe);
-	} else {
-		dss_ctl1_reg = DSS_CTL1;
-		dss_ctl2_reg = DSS_CTL2;
-	}
-
-	dss_ctl1 = intel_de_read(dev_priv, dss_ctl1_reg);
+	dss_ctl1 = intel_de_read(dev_priv, DSS_CTL1);
 	dss_ctl1 |= SPLITTER_ENABLE;
 	dss_ctl1 &= ~OVERLAP_PIXELS_MASK;
 	dss_ctl1 |= OVERLAP_PIXELS(intel_dsi->pixel_overlap);
@@ -334,16 +322,16 @@ static void configure_dual_link_mode(struct intel_encoder *encoder,
 
 		dss_ctl1 &= ~LEFT_DL_BUF_TARGET_DEPTH_MASK;
 		dss_ctl1 |= LEFT_DL_BUF_TARGET_DEPTH(dl_buffer_depth);
-		dss_ctl2 = intel_de_read(dev_priv, dss_ctl2_reg);
+		dss_ctl2 = intel_de_read(dev_priv, DSS_CTL2);
 		dss_ctl2 &= ~RIGHT_DL_BUF_TARGET_DEPTH_MASK;
 		dss_ctl2 |= RIGHT_DL_BUF_TARGET_DEPTH(dl_buffer_depth);
-		intel_de_write(dev_priv, dss_ctl2_reg, dss_ctl2);
+		intel_de_write(dev_priv, DSS_CTL2, dss_ctl2);
 	} else {
 		/* Interleave */
 		dss_ctl1 |= DUAL_LINK_MODE_INTERLEAVE;
 	}
 
-	intel_de_write(dev_priv, dss_ctl1_reg, dss_ctl1);
+	intel_de_write(dev_priv, DSS_CTL1, dss_ctl1);
 }
 
 /* aka DSI 8X clock */
diff --git a/drivers/gpu/drm/i915/display/intel_bios.c b/drivers/gpu/drm/i915/display/intel_bios.c
index d42d2f58e52c..28bdb936cd1f 100644
--- a/drivers/gpu/drm/i915/display/intel_bios.c
+++ b/drivers/gpu/drm/i915/display/intel_bios.c
@@ -2466,22 +2466,6 @@ static enum port dvo_port_to_port(struct drm_i915_private *i915,
 					  dvo_port);
 }
 
-static enum port
-dsi_dvo_port_to_port(struct drm_i915_private *i915, u8 dvo_port)
-{
-	switch (dvo_port) {
-	case DVO_PORT_MIPIA:
-		return PORT_A;
-	case DVO_PORT_MIPIC:
-		if (DISPLAY_VER(i915) >= 11)
-			return PORT_B;
-		else
-			return PORT_C;
-	default:
-		return PORT_NONE;
-	}
-}
-
 static int parse_bdb_230_dp_max_link_rate(const int vbt_max_link_rate)
 {
 	switch (vbt_max_link_rate) {
@@ -3422,16 +3406,19 @@ bool intel_bios_is_dsi_present(struct drm_i915_private *i915,
 
 		dvo_port = child->dvo_port;
 
-		if (dsi_dvo_port_to_port(i915, dvo_port) == PORT_NONE) {
+		if (dvo_port == DVO_PORT_MIPIA ||
+		    (dvo_port == DVO_PORT_MIPIB && DISPLAY_VER(i915) >= 11) ||
+		    (dvo_port == DVO_PORT_MIPIC && DISPLAY_VER(i915) < 11)) {
+			if (port)
+				*port = dvo_port - DVO_PORT_MIPIA;
+			return true;
+		} else if (dvo_port == DVO_PORT_MIPIB ||
+			   dvo_port == DVO_PORT_MIPIC ||
+			   dvo_port == DVO_PORT_MIPID) {
 			drm_dbg_kms(&i915->drm,
 				    "VBT has unsupported DSI port %c\n",
 				    port_name(dvo_port - DVO_PORT_MIPIA));
-			continue;
 		}
-
-		if (port)
-			*port = dsi_dvo_port_to_port(i915, dvo_port);
-		return true;
 	}
 
 	return false;
@@ -3516,7 +3503,7 @@ bool intel_bios_get_dsc_params(struct intel_encoder *encoder,
 		if (!(child->device_type & DEVICE_TYPE_MIPI_OUTPUT))
 			continue;
 
-		if (dsi_dvo_port_to_port(i915, child->dvo_port) == encoder->port) {
+		if (child->dvo_port - DVO_PORT_MIPIA == encoder->port) {
 			if (!devdata->dsc)
 				return false;
 
diff --git a/drivers/gpu/drm/i915/display/intel_cdclk.c b/drivers/gpu/drm/i915/display/intel_cdclk.c
index 92925f0f7239..ed05070b7307 100644
--- a/drivers/gpu/drm/i915/display/intel_cdclk.c
+++ b/drivers/gpu/drm/i915/display/intel_cdclk.c
@@ -1323,7 +1323,7 @@ static const struct intel_cdclk_vals adlp_cdclk_table[] = {
 	{ .refclk = 24000, .cdclk = 192000, .divider = 2, .ratio = 16 },
 	{ .refclk = 24000, .cdclk = 312000, .divider = 2, .ratio = 26 },
 	{ .refclk = 24000, .cdclk = 552000, .divider = 2, .ratio = 46 },
-	{ .refclk = 24000, .cdclk = 648000, .divider = 2, .ratio = 54 },
+	{ .refclk = 24400, .cdclk = 648000, .divider = 2, .ratio = 54 },
 
 	{ .refclk = 38400, .cdclk = 179200, .divider = 3, .ratio = 14 },
 	{ .refclk = 38400, .cdclk = 192000, .divider = 2, .ratio = 10 },
diff --git a/drivers/gpu/drm/i915/display/intel_ddi.c b/drivers/gpu/drm/i915/display/intel_ddi.c
index caee701f6100..da8472cdc135 100644
--- a/drivers/gpu/drm/i915/display/intel_ddi.c
+++ b/drivers/gpu/drm/i915/display/intel_ddi.c
@@ -3591,7 +3591,7 @@ static void intel_ddi_sync_state(struct intel_encoder *encoder,
 	enum phy phy = intel_port_to_phy(i915, encoder->port);
 
 	if (intel_phy_is_tc(i915, phy))
-		intel_tc_port_sanitize_mode(enc_to_dig_port(encoder));
+		intel_tc_port_sanitize(enc_to_dig_port(encoder));
 
 	if (crtc_state && intel_crtc_has_dp_encoder(crtc_state))
 		intel_dp_sync_state(encoder, crtc_state);
@@ -3801,17 +3801,11 @@ static void intel_ddi_encoder_destroy(struct drm_encoder *encoder)
 
 static void intel_ddi_encoder_reset(struct drm_encoder *encoder)
 {
-	struct drm_i915_private *i915 = to_i915(encoder->dev);
 	struct intel_dp *intel_dp = enc_to_intel_dp(to_intel_encoder(encoder));
-	struct intel_digital_port *dig_port = enc_to_dig_port(to_intel_encoder(encoder));
-	enum phy phy = intel_port_to_phy(i915, dig_port->base.port);
 
 	intel_dp->reset_link_params = true;
 
 	intel_pps_encoder_reset(intel_dp);
-
-	if (intel_phy_is_tc(i915, phy))
-		intel_tc_port_init_mode(dig_port);
 }
 
 static const struct drm_encoder_funcs intel_ddi_funcs = {
diff --git a/drivers/gpu/drm/i915/display/intel_display.c b/drivers/gpu/drm/i915/display/intel_display.c
index 92f5b1b643a3..461c62c88413 100644
--- a/drivers/gpu/drm/i915/display/intel_display.c
+++ b/drivers/gpu/drm/i915/display/intel_display.c
@@ -1079,7 +1079,7 @@ intel_get_crtc_new_encoder(const struct intel_atomic_state *state,
 		num_encoders++;
 	}
 
-	drm_WARN(state->base.dev, num_encoders != 1,
+	drm_WARN(encoder->base.dev, num_encoders != 1,
 		 "%d encoders for pipe %c\n",
 		 num_encoders, pipe_name(master_crtc->pipe));
 
@@ -5182,7 +5182,6 @@ intel_crtc_prepare_cleared_state(struct intel_atomic_state *state,
 	 * only fields that are know to not cause problems are preserved. */
 
 	saved_state->uapi = crtc_state->uapi;
-	saved_state->inherited = crtc_state->inherited;
 	saved_state->scaler_state = crtc_state->scaler_state;
 	saved_state->shared_dpll = crtc_state->shared_dpll;
 	saved_state->dpll_hw_state = crtc_state->dpll_hw_state;
diff --git a/drivers/gpu/drm/i915/display/intel_display_power.c b/drivers/gpu/drm/i915/display/intel_display_power.c
index 1465fcbbedc8..b7b9a8d43134 100644
--- a/drivers/gpu/drm/i915/display/intel_display_power.c
+++ b/drivers/gpu/drm/i915/display/intel_display_power.c
@@ -996,21 +996,6 @@ int intel_power_domains_init(struct drm_i915_private *dev_priv)
 	return intel_display_power_map_init(power_domains);
 }
 
-/**
- * intel_power_domains_prune - prunes the power domain structures
- * @dev_priv: i915 device instance
- *
- * We might have detected that power domain initialization done earlier
- * requires some additional tweaks.
- */
-void intel_power_domains_prune(struct drm_i915_private *dev_priv)
-{
-	struct i915_power_domains *power_domains = &dev_priv->display.power.domains;
-
-	if (IS_SRIOV_VF(dev_priv))
-		intel_display_power_map_prune(power_domains);
-}
-
 /**
  * intel_power_domains_cleanup - clean up power domains resources
  * @dev_priv: i915 device instance
diff --git a/drivers/gpu/drm/i915/display/intel_display_power.h b/drivers/gpu/drm/i915/display/intel_display_power.h
index e4391c231875..7136ea3f233e 100644
--- a/drivers/gpu/drm/i915/display/intel_display_power.h
+++ b/drivers/gpu/drm/i915/display/intel_display_power.h
@@ -155,7 +155,6 @@ struct intel_display_power_domain_set {
 		for_each_if(test_bit((__domain), (__mask)->bits))
 
 int intel_power_domains_init(struct drm_i915_private *dev_priv);
-void intel_power_domains_prune(struct drm_i915_private *dev_priv);
 void intel_power_domains_cleanup(struct drm_i915_private *dev_priv);
 void intel_power_domains_init_hw(struct drm_i915_private *dev_priv, bool resume);
 void intel_power_domains_driver_remove(struct drm_i915_private *dev_priv);
diff --git a/drivers/gpu/drm/i915/display/intel_display_power_map.c b/drivers/gpu/drm/i915/display/intel_display_power_map.c
index 9883e279e3df..dc04afc6cc8f 100644
--- a/drivers/gpu/drm/i915/display/intel_display_power_map.c
+++ b/drivers/gpu/drm/i915/display/intel_display_power_map.c
@@ -1602,27 +1602,6 @@ int intel_display_power_map_init(struct i915_power_domains *power_domains)
 		return set_power_wells(power_domains, i9xx_power_wells);
 }
 
-/**
- * intel_display_power_map_prune - prune power domain -> power well mappings
- * @power_domains: power domain state
- *
- * Replace the current mapping with all domains pointing to the always-on
- * power well, in practice disabling the display power domain functionality.
- * Needed for an SRIOV_VF initialization quirk, not to be used elsewhere.
- *
- * FIXME: fix this by setting up the correct mapping in
- * intel_display_power_map_init() instead.
- */
-void intel_display_power_map_prune(struct i915_power_domains *power_domains)
-{
-	struct drm_i915_private *i915 = container_of(power_domains,
-						     struct drm_i915_private,
-						     display.power.domains);
-
-	kfree(power_domains->power_wells);
-	drm_WARN_ON(&i915->drm, set_power_wells(power_domains, i9xx_power_wells) < 0);
-}
-
 /**
  * intel_display_power_map_cleanup - clean up power domain -> power well mappings
  * @power_domains: power domain state
diff --git a/drivers/gpu/drm/i915/display/intel_display_power_map.h b/drivers/gpu/drm/i915/display/intel_display_power_map.h
index 465b3f998ceb..da8f7055a44c 100644
--- a/drivers/gpu/drm/i915/display/intel_display_power_map.h
+++ b/drivers/gpu/drm/i915/display/intel_display_power_map.h
@@ -10,7 +10,5 @@ struct i915_power_domains;
 
 int intel_display_power_map_init(struct i915_power_domains *power_domains);
 void intel_display_power_map_cleanup(struct i915_power_domains *power_domains);
-void intel_display_power_map_prune(struct i915_power_domains *power_domains);
-
 
 #endif
diff --git a/drivers/gpu/drm/i915/display/intel_display_types.h b/drivers/gpu/drm/i915/display/intel_display_types.h
index 708e7c14c17c..298d00a11f47 100644
--- a/drivers/gpu/drm/i915/display/intel_display_types.h
+++ b/drivers/gpu/drm/i915/display/intel_display_types.h
@@ -1604,8 +1604,6 @@ struct intel_psr {
 	bool psr2_sel_fetch_cff_enabled;
 	bool req_psr2_sdp_prior_scanline;
 	u8 sink_sync_latency;
-	u8 io_wake_lines;
-	u8 fast_wake_lines;
 	ktime_t last_entry_attempt;
 	ktime_t last_exit;
 	bool sink_not_reliable;
diff --git a/drivers/gpu/drm/i915/display/intel_dp.c b/drivers/gpu/drm/i915/display/intel_dp.c
index 5679f16ab0ac..c9be61d2348e 100644
--- a/drivers/gpu/drm/i915/display/intel_dp.c
+++ b/drivers/gpu/drm/i915/display/intel_dp.c
@@ -1069,8 +1069,7 @@ bool intel_dp_source_supports_tps3(struct drm_i915_private *i915)
 
 bool intel_dp_source_supports_tps4(struct drm_i915_private *i915)
 {
-	/* Skipping TPS4 to avoid Link training failure issue */
-	return false;
+	return DISPLAY_VER(i915) >= 10;
 }
 
 static void snprintf_int_array(char *str, size_t len,
@@ -1513,11 +1512,6 @@ static int intel_dp_dsc_compute_config(struct intel_dp *intel_dp,
 		pipe_config->dsc.slice_count =
 			drm_dp_dsc_sink_max_slice_count(intel_dp->dsc_dpcd,
 							true);
-		if (!pipe_config->dsc.slice_count) {
-			drm_dbg_kms(&dev_priv->drm, "Unsupported Slice Count %d\n",
-				    pipe_config->dsc.slice_count);
-			return -EINVAL;
-		}
 	} else {
 		u16 dsc_max_output_bpp;
 		u8 dsc_dp_slice_count;
@@ -3681,6 +3675,61 @@ static void intel_dp_phy_pattern_update(struct intel_dp *intel_dp,
 	}
 }
 
+static void
+intel_dp_autotest_phy_ddi_disable(struct intel_dp *intel_dp,
+				  const struct intel_crtc_state *crtc_state)
+{
+	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = to_i915(dev);
+	struct intel_crtc *crtc = to_intel_crtc(dig_port->base.base.crtc);
+	enum pipe pipe = crtc->pipe;
+	u32 trans_ddi_func_ctl_value, trans_conf_value, dp_tp_ctl_value;
+
+	trans_ddi_func_ctl_value = intel_de_read(dev_priv,
+						 TRANS_DDI_FUNC_CTL(pipe));
+	trans_conf_value = intel_de_read(dev_priv, PIPECONF(pipe));
+	dp_tp_ctl_value = intel_de_read(dev_priv, TGL_DP_TP_CTL(pipe));
+
+	trans_ddi_func_ctl_value &= ~(TRANS_DDI_FUNC_ENABLE |
+				      TGL_TRANS_DDI_PORT_MASK);
+	trans_conf_value &= ~PIPECONF_ENABLE;
+	dp_tp_ctl_value &= ~DP_TP_CTL_ENABLE;
+
+	intel_de_write(dev_priv, PIPECONF(pipe), trans_conf_value);
+	intel_de_write(dev_priv, TRANS_DDI_FUNC_CTL(pipe),
+		       trans_ddi_func_ctl_value);
+	intel_de_write(dev_priv, TGL_DP_TP_CTL(pipe), dp_tp_ctl_value);
+}
+
+static void
+intel_dp_autotest_phy_ddi_enable(struct intel_dp *intel_dp,
+				 const struct intel_crtc_state *crtc_state)
+{
+	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = to_i915(dev);
+	enum port port = dig_port->base.port;
+	struct intel_crtc *crtc = to_intel_crtc(dig_port->base.base.crtc);
+	enum pipe pipe = crtc->pipe;
+	u32 trans_ddi_func_ctl_value, trans_conf_value, dp_tp_ctl_value;
+
+	trans_ddi_func_ctl_value = intel_de_read(dev_priv,
+						 TRANS_DDI_FUNC_CTL(pipe));
+	trans_conf_value = intel_de_read(dev_priv, PIPECONF(pipe));
+	dp_tp_ctl_value = intel_de_read(dev_priv, TGL_DP_TP_CTL(pipe));
+
+	trans_ddi_func_ctl_value |= TRANS_DDI_FUNC_ENABLE |
+				    TGL_TRANS_DDI_SELECT_PORT(port);
+	trans_conf_value |= PIPECONF_ENABLE;
+	dp_tp_ctl_value |= DP_TP_CTL_ENABLE;
+
+	intel_de_write(dev_priv, PIPECONF(pipe), trans_conf_value);
+	intel_de_write(dev_priv, TGL_DP_TP_CTL(pipe), dp_tp_ctl_value);
+	intel_de_write(dev_priv, TRANS_DDI_FUNC_CTL(pipe),
+		       trans_ddi_func_ctl_value);
+}
+
 static void intel_dp_process_phy_request(struct intel_dp *intel_dp,
 					 const struct intel_crtc_state *crtc_state)
 {
@@ -3699,10 +3748,14 @@ static void intel_dp_process_phy_request(struct intel_dp *intel_dp,
 	intel_dp_get_adjust_train(intel_dp, crtc_state, DP_PHY_DPRX,
 				  link_status);
 
+	intel_dp_autotest_phy_ddi_disable(intel_dp, crtc_state);
+
 	intel_dp_set_signal_levels(intel_dp, crtc_state, DP_PHY_DPRX);
 
 	intel_dp_phy_pattern_update(intel_dp, crtc_state);
 
+	intel_dp_autotest_phy_ddi_enable(intel_dp, crtc_state);
+
 	drm_dp_dpcd_write(&intel_dp->aux, DP_TRAINING_LANE0_SET,
 			  intel_dp->train_set, crtc_state->lane_count);
 
@@ -3904,8 +3957,6 @@ intel_dp_handle_hdmi_link_status_change(struct intel_dp *intel_dp)
 
 		drm_dp_pcon_hdmi_frl_link_error_count(&intel_dp->aux, &intel_dp->attached_connector->base);
 
-		intel_dp->frl.is_trained = false;
-
 		/* Restart FRL training or fall back to TMDS mode */
 		intel_dp_check_frl_training(intel_dp);
 	}
diff --git a/drivers/gpu/drm/i915/display/intel_dp_aux.c b/drivers/gpu/drm/i915/display/intel_dp_aux.c
index ab357161ccc3..48c375c65a41 100644
--- a/drivers/gpu/drm/i915/display/intel_dp_aux.c
+++ b/drivers/gpu/drm/i915/display/intel_dp_aux.c
@@ -119,32 +119,6 @@ static u32 skl_get_aux_clock_divider(struct intel_dp *intel_dp, int index)
 	return index ? 0 : 1;
 }
 
-static int intel_dp_aux_sync_len(void)
-{
-	int precharge = 16; /* 10-16 */
-	int preamble = 16;
-
-	return precharge + preamble;
-}
-
-static int intel_dp_aux_fw_sync_len(void)
-{
-	int precharge = 10; /* 10-16 */
-	int preamble = 8;
-
-	return precharge + preamble;
-}
-
-static int g4x_dp_aux_precharge_len(void)
-{
-	int precharge_min = 10;
-	int preamble = 16;
-
-	/* HW wants the length of the extra precharge in 2us units */
-	return (intel_dp_aux_sync_len() -
-		precharge_min - preamble) / 2;
-}
-
 static u32 g4x_get_aux_send_ctl(struct intel_dp *intel_dp,
 				int send_bytes,
 				u32 aux_clock_divider)
@@ -167,7 +141,7 @@ static u32 g4x_get_aux_send_ctl(struct intel_dp *intel_dp,
 	       timeout |
 	       DP_AUX_CH_CTL_RECEIVE_ERROR |
 	       (send_bytes << DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) |
-	       (g4x_dp_aux_precharge_len() << DP_AUX_CH_CTL_PRECHARGE_2US_SHIFT) |
+	       (3 << DP_AUX_CH_CTL_PRECHARGE_2US_SHIFT) |
 	       (aux_clock_divider << DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT);
 }
 
@@ -191,8 +165,8 @@ static u32 skl_get_aux_send_ctl(struct intel_dp *intel_dp,
 	      DP_AUX_CH_CTL_TIME_OUT_MAX |
 	      DP_AUX_CH_CTL_RECEIVE_ERROR |
 	      (send_bytes << DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) |
-	      DP_AUX_CH_CTL_FW_SYNC_PULSE_SKL(intel_dp_aux_fw_sync_len()) |
-	      DP_AUX_CH_CTL_SYNC_PULSE_SKL(intel_dp_aux_sync_len());
+	      DP_AUX_CH_CTL_FW_SYNC_PULSE_SKL(32) |
+	      DP_AUX_CH_CTL_SYNC_PULSE_SKL(32);
 
 	if (intel_tc_port_in_tbt_alt_mode(dig_port))
 		ret |= DP_AUX_CH_CTL_TBT_IO;
diff --git a/drivers/gpu/drm/i915/display/intel_dpt.c b/drivers/gpu/drm/i915/display/intel_dpt.c
index 3d91e785b22c..ac587647e1f5 100644
--- a/drivers/gpu/drm/i915/display/intel_dpt.c
+++ b/drivers/gpu/drm/i915/display/intel_dpt.c
@@ -32,6 +32,11 @@ i915_vm_to_dpt(struct i915_address_space *vm)
 
 #define dpt_total_entries(dpt) ((dpt)->vm.total >> PAGE_SHIFT)
 
+static void gen8_set_pte(void __iomem *addr, gen8_pte_t pte)
+{
+	writeq(pte, addr);
+}
+
 static void dpt_insert_page(struct i915_address_space *vm,
 			    dma_addr_t addr,
 			    u64 offset,
diff --git a/drivers/gpu/drm/i915/display/intel_dsi_vbt.c b/drivers/gpu/drm/i915/display/intel_dsi_vbt.c
index fce69fa446d5..75e8cc4337c9 100644
--- a/drivers/gpu/drm/i915/display/intel_dsi_vbt.c
+++ b/drivers/gpu/drm/i915/display/intel_dsi_vbt.c
@@ -137,9 +137,9 @@ static enum port intel_dsi_seq_port_to_port(struct intel_dsi *intel_dsi,
 		return ffs(intel_dsi->ports) - 1;
 
 	if (seq_port) {
-		if (intel_dsi->ports & BIT(PORT_B))
+		if (intel_dsi->ports & PORT_B)
 			return PORT_B;
-		else if (intel_dsi->ports & BIT(PORT_C))
+		else if (intel_dsi->ports & PORT_C)
 			return PORT_C;
 	}
 
diff --git a/drivers/gpu/drm/i915/display/intel_opregion.c b/drivers/gpu/drm/i915/display/intel_opregion.c
index df4c14be7f61..caa07ef34f21 100644
--- a/drivers/gpu/drm/i915/display/intel_opregion.c
+++ b/drivers/gpu/drm/i915/display/intel_opregion.c
@@ -894,9 +894,6 @@ int intel_opregion_setup(struct drm_i915_private *dev_priv)
 	BUILD_BUG_ON(sizeof(struct opregion_asle) != 0x100);
 	BUILD_BUG_ON(sizeof(struct opregion_asle_ext) != 0x400);
 
-	if (IS_SRIOV_VF(dev_priv))
-		return 0;
-
 	pci_read_config_dword(pdev, ASLS, &asls);
 	drm_dbg(&dev_priv->drm, "graphic opregion physical addr: 0x%x\n",
 		asls);
diff --git a/drivers/gpu/drm/i915/display/intel_psr.c b/drivers/gpu/drm/i915/display/intel_psr.c
index 6b456443d05e..d4cce627d7a8 100644
--- a/drivers/gpu/drm/i915/display/intel_psr.c
+++ b/drivers/gpu/drm/i915/display/intel_psr.c
@@ -542,14 +542,6 @@ static void hsw_activate_psr2(struct intel_dp *intel_dp)
 	val |= EDP_PSR2_FRAME_BEFORE_SU(max_t(u8, intel_dp->psr.sink_sync_latency + 1, 2));
 	val |= intel_psr2_get_tp_time(intel_dp);
 
-	if (DISPLAY_VER(dev_priv) >= 12) {
-		if (intel_dp->psr.io_wake_lines < 9 &&
-		    intel_dp->psr.fast_wake_lines < 9)
-			val |= TGL_EDP_PSR2_BLOCK_COUNT_NUM_2;
-		else
-			val |= TGL_EDP_PSR2_BLOCK_COUNT_NUM_3;
-	}
-
 	/* Wa_22012278275:adl-p */
 	if (IS_ADLP_DISPLAY_STEP(dev_priv, STEP_A0, STEP_E0)) {
 		static const u8 map[] = {
@@ -566,13 +558,15 @@ static void hsw_activate_psr2(struct intel_dp *intel_dp)
 		 * Still using the default IO_BUFFER_WAKE and FAST_WAKE, see
 		 * comments bellow for more information
 		 */
-		u32 tmp;
+		u32 tmp, lines = 7;
+
+		val |= TGL_EDP_PSR2_BLOCK_COUNT_NUM_2;
 
-		tmp = map[intel_dp->psr.io_wake_lines - TGL_EDP_PSR2_IO_BUFFER_WAKE_MIN_LINES];
+		tmp = map[lines - TGL_EDP_PSR2_IO_BUFFER_WAKE_MIN_LINES];
 		tmp = tmp << TGL_EDP_PSR2_IO_BUFFER_WAKE_SHIFT;
 		val |= tmp;
 
-		tmp = map[intel_dp->psr.fast_wake_lines - TGL_EDP_PSR2_FAST_WAKE_MIN_LINES];
+		tmp = map[lines - TGL_EDP_PSR2_FAST_WAKE_MIN_LINES];
 		tmp = tmp << TGL_EDP_PSR2_FAST_WAKE_MIN_SHIFT;
 		val |= tmp;
 	} else if (DISPLAY_VER(dev_priv) >= 12) {
@@ -587,8 +581,8 @@ static void hsw_activate_psr2(struct intel_dp *intel_dp)
 		val |= TGL_EDP_PSR2_IO_BUFFER_WAKE(7);
 		val |= TGL_EDP_PSR2_FAST_WAKE(7);
 	} else if (DISPLAY_VER(dev_priv) >= 9) {
-		val |= EDP_PSR2_IO_BUFFER_WAKE(intel_dp->psr.io_wake_lines);
-		val |= EDP_PSR2_FAST_WAKE(intel_dp->psr.fast_wake_lines);
+		val |= EDP_PSR2_IO_BUFFER_WAKE(7);
+		val |= EDP_PSR2_FAST_WAKE(7);
 	}
 
 	if (intel_dp->psr.req_psr2_sdp_prior_scanline)
@@ -843,46 +837,6 @@ static bool _compute_psr2_sdp_prior_scanline_indication(struct intel_dp *intel_d
 	return true;
 }
 
-static bool _compute_psr2_wake_times(struct intel_dp *intel_dp,
-				     struct intel_crtc_state *crtc_state)
-{
-	struct drm_i915_private *i915 = dp_to_i915(intel_dp);
-	int io_wake_lines, io_wake_time, fast_wake_lines, fast_wake_time;
-	u8 max_wake_lines;
-
-	if (DISPLAY_VER(i915) >= 12) {
-		io_wake_time = 42;
-		/*
-		 * According to Bspec it's 42us, but based on testing
-		 * it is not enough -> use 45 us.
-		 */
-		fast_wake_time = 45;
-		max_wake_lines = 12;
-	} else {
-		io_wake_time = 50;
-		fast_wake_time = 32;
-		max_wake_lines = 8;
-	}
-
-	io_wake_lines = intel_usecs_to_scanlines(
-		&crtc_state->uapi.adjusted_mode, io_wake_time);
-	fast_wake_lines = intel_usecs_to_scanlines(
-		&crtc_state->uapi.adjusted_mode, fast_wake_time);
-
-	if (io_wake_lines > max_wake_lines ||
-	    fast_wake_lines > max_wake_lines)
-		return false;
-
-	if (i915->params.psr_safest_params)
-		io_wake_lines = fast_wake_lines = max_wake_lines;
-
-	/* According to Bspec lower limit should be set as 7 lines. */
-	intel_dp->psr.io_wake_lines = max(io_wake_lines, 7);
-	intel_dp->psr.fast_wake_lines = max(fast_wake_lines, 7);
-
-	return true;
-}
-
 static bool intel_psr2_config_valid(struct intel_dp *intel_dp,
 				    struct intel_crtc_state *crtc_state)
 {
@@ -976,12 +930,6 @@ static bool intel_psr2_config_valid(struct intel_dp *intel_dp,
 		return false;
 	}
 
-	if (!_compute_psr2_wake_times(intel_dp, crtc_state)) {
-		drm_dbg_kms(&dev_priv->drm,
-			    "PSR2 not enabled, Unable to use long enough wake times\n");
-		return false;
-	}
-
 	if (HAS_PSR2_SEL_FETCH(dev_priv)) {
 		if (!intel_psr2_sel_fetch_config_valid(intel_dp, crtc_state) &&
 		    !HAS_PSR_HW_TRACKING(dev_priv)) {
@@ -1824,7 +1772,6 @@ int intel_psr2_sel_fetch_update(struct intel_atomic_state *state,
 				clip_area_update(&pipe_clip, &damaged_area,
 						 &crtc_state->pipe_src);
 			}
-
 			continue;
 		} else if (new_plane_state->uapi.alpha != old_plane_state->uapi.alpha) {
 			/* If alpha changed mark the whole plane area as damaged */
diff --git a/drivers/gpu/drm/i915/display/intel_quirks.c b/drivers/gpu/drm/i915/display/intel_quirks.c
index a280448df771..6e48d3bcdfec 100644
--- a/drivers/gpu/drm/i915/display/intel_quirks.c
+++ b/drivers/gpu/drm/i915/display/intel_quirks.c
@@ -199,8 +199,6 @@ static struct intel_quirk intel_quirks[] = {
 	/* ECS Liva Q2 */
 	{ 0x3185, 0x1019, 0xa94d, quirk_increase_ddi_disabled_time },
 	{ 0x3184, 0x1019, 0xa94d, quirk_increase_ddi_disabled_time },
-	/* HP Notebook - 14-r206nv */
-	{ 0x0f31, 0x103c, 0x220f, quirk_invert_brightness },
 };
 
 void intel_init_quirks(struct drm_i915_private *i915)
diff --git a/drivers/gpu/drm/i915/display/intel_sdvo.c b/drivers/gpu/drm/i915/display/intel_sdvo.c
index 8046d02a8ad0..f5b744bef18f 100644
--- a/drivers/gpu/drm/i915/display/intel_sdvo.c
+++ b/drivers/gpu/drm/i915/display/intel_sdvo.c
@@ -2747,10 +2747,13 @@ intel_sdvo_dvi_init(struct intel_sdvo *intel_sdvo, int device)
 	if (!intel_sdvo_connector)
 		return false;
 
-	if (device == 0)
+	if (device == 0) {
+		intel_sdvo->controlled_output |= SDVO_OUTPUT_TMDS0;
 		intel_sdvo_connector->output_flag = SDVO_OUTPUT_TMDS0;
-	else if (device == 1)
+	} else if (device == 1) {
+		intel_sdvo->controlled_output |= SDVO_OUTPUT_TMDS1;
 		intel_sdvo_connector->output_flag = SDVO_OUTPUT_TMDS1;
+	}
 
 	intel_connector = &intel_sdvo_connector->base;
 	connector = &intel_connector->base;
@@ -2805,6 +2808,7 @@ intel_sdvo_tv_init(struct intel_sdvo *intel_sdvo, int type)
 	encoder->encoder_type = DRM_MODE_ENCODER_TVDAC;
 	connector->connector_type = DRM_MODE_CONNECTOR_SVIDEO;
 
+	intel_sdvo->controlled_output |= type;
 	intel_sdvo_connector->output_flag = type;
 
 	if (intel_sdvo_connector_init(intel_sdvo_connector, intel_sdvo) < 0) {
@@ -2845,10 +2849,13 @@ intel_sdvo_analog_init(struct intel_sdvo *intel_sdvo, int device)
 	encoder->encoder_type = DRM_MODE_ENCODER_DAC;
 	connector->connector_type = DRM_MODE_CONNECTOR_VGA;
 
-	if (device == 0)
+	if (device == 0) {
+		intel_sdvo->controlled_output |= SDVO_OUTPUT_RGB0;
 		intel_sdvo_connector->output_flag = SDVO_OUTPUT_RGB0;
-	else if (device == 1)
+	} else if (device == 1) {
+		intel_sdvo->controlled_output |= SDVO_OUTPUT_RGB1;
 		intel_sdvo_connector->output_flag = SDVO_OUTPUT_RGB1;
+	}
 
 	if (intel_sdvo_connector_init(intel_sdvo_connector, intel_sdvo) < 0) {
 		kfree(intel_sdvo_connector);
@@ -2878,10 +2885,13 @@ intel_sdvo_lvds_init(struct intel_sdvo *intel_sdvo, int device)
 	encoder->encoder_type = DRM_MODE_ENCODER_LVDS;
 	connector->connector_type = DRM_MODE_CONNECTOR_LVDS;
 
-	if (device == 0)
+	if (device == 0) {
+		intel_sdvo->controlled_output |= SDVO_OUTPUT_LVDS0;
 		intel_sdvo_connector->output_flag = SDVO_OUTPUT_LVDS0;
-	else if (device == 1)
+	} else if (device == 1) {
+		intel_sdvo->controlled_output |= SDVO_OUTPUT_LVDS1;
 		intel_sdvo_connector->output_flag = SDVO_OUTPUT_LVDS1;
+	}
 
 	if (intel_sdvo_connector_init(intel_sdvo_connector, intel_sdvo) < 0) {
 		kfree(intel_sdvo_connector);
@@ -2916,39 +2926,16 @@ intel_sdvo_lvds_init(struct intel_sdvo *intel_sdvo, int device)
 	return false;
 }
 
-static u16 intel_sdvo_filter_output_flags(u16 flags)
-{
-	flags &= SDVO_OUTPUT_MASK;
-
-	/* SDVO requires XXX1 function may not exist unless it has XXX0 function.*/
-	if (!(flags & SDVO_OUTPUT_TMDS0))
-		flags &= ~SDVO_OUTPUT_TMDS1;
-
-	if (!(flags & SDVO_OUTPUT_RGB0))
-		flags &= ~SDVO_OUTPUT_RGB1;
-
-	if (!(flags & SDVO_OUTPUT_LVDS0))
-		flags &= ~SDVO_OUTPUT_LVDS1;
-
-	return flags;
-}
-
 static bool
 intel_sdvo_output_setup(struct intel_sdvo *intel_sdvo, u16 flags)
 {
-	struct drm_i915_private *i915 = to_i915(intel_sdvo->base.base.dev);
-
-	flags = intel_sdvo_filter_output_flags(flags);
-
-	intel_sdvo->controlled_output = flags;
-
-	intel_sdvo_select_ddc_bus(i915, intel_sdvo);
+	/* SDVO requires XXX1 function may not exist unless it has XXX0 function.*/
 
 	if (flags & SDVO_OUTPUT_TMDS0)
 		if (!intel_sdvo_dvi_init(intel_sdvo, 0))
 			return false;
 
-	if (flags & SDVO_OUTPUT_TMDS1)
+	if ((flags & SDVO_TMDS_MASK) == SDVO_TMDS_MASK)
 		if (!intel_sdvo_dvi_init(intel_sdvo, 1))
 			return false;
 
@@ -2969,7 +2956,7 @@ intel_sdvo_output_setup(struct intel_sdvo *intel_sdvo, u16 flags)
 		if (!intel_sdvo_analog_init(intel_sdvo, 0))
 			return false;
 
-	if (flags & SDVO_OUTPUT_RGB1)
+	if ((flags & SDVO_RGB_MASK) == SDVO_RGB_MASK)
 		if (!intel_sdvo_analog_init(intel_sdvo, 1))
 			return false;
 
@@ -2977,13 +2964,14 @@ intel_sdvo_output_setup(struct intel_sdvo *intel_sdvo, u16 flags)
 		if (!intel_sdvo_lvds_init(intel_sdvo, 0))
 			return false;
 
-	if (flags & SDVO_OUTPUT_LVDS1)
+	if ((flags & SDVO_LVDS_MASK) == SDVO_LVDS_MASK)
 		if (!intel_sdvo_lvds_init(intel_sdvo, 1))
 			return false;
 
-	if (flags == 0) {
+	if ((flags & SDVO_OUTPUT_MASK) == 0) {
 		unsigned char bytes[2];
 
+		intel_sdvo->controlled_output = 0;
 		memcpy(bytes, &intel_sdvo->caps.output_flags, 2);
 		DRM_DEBUG_KMS("%s: Unknown SDVO output type (0x%02x%02x)\n",
 			      SDVO_NAME(intel_sdvo),
@@ -3395,6 +3383,8 @@ bool intel_sdvo_init(struct drm_i915_private *dev_priv,
 	 */
 	intel_sdvo->base.cloneable = 0;
 
+	intel_sdvo_select_ddc_bus(dev_priv, intel_sdvo);
+
 	/* Set the input timing to the screen. Assume always input 0. */
 	if (!intel_sdvo_set_target_input(intel_sdvo))
 		goto err_output;
diff --git a/drivers/gpu/drm/i915/display/intel_snps_phy.c b/drivers/gpu/drm/i915/display/intel_snps_phy.c
index 3326c79c78a0..937cefd6f78f 100644
--- a/drivers/gpu/drm/i915/display/intel_snps_phy.c
+++ b/drivers/gpu/drm/i915/display/intel_snps_phy.c
@@ -1418,36 +1418,6 @@ static const struct intel_mpllb_state dg2_hdmi_262750 = {
 		REG_FIELD_PREP(SNPS_PHY_MPLLB_SSC_UP_SPREAD, 1),
 };
 
-static const struct intel_mpllb_state dg2_hdmi_267300 = {
-	.clock = 267300,
-	.ref_control =
-		REG_FIELD_PREP(SNPS_PHY_REF_CONTROL_REF_RANGE, 3),
-	.mpllb_cp =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_CP_INT, 7) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_CP_PROP, 14) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_CP_INT_GS, 64) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_CP_PROP_GS, 124),
-	.mpllb_div =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_DIV5_CLK_EN, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_TX_CLK_DIV, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_PMIX_EN, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_V2I, 2) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FREQ_VCO, 3),
-	.mpllb_div2 =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_REF_CLK_DIV, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_MULTIPLIER, 74) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_HDMI_DIV, 1),
-	.mpllb_fracn1 =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_CGG_UPDATE_EN, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_EN, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_DEN, 65535),
-	.mpllb_fracn2 =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_QUOT, 30146) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_REM, 36699),
-	.mpllb_sscen =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_SSC_UP_SPREAD, 1),
-};
-
 static const struct intel_mpllb_state dg2_hdmi_268500 = {
 	.clock = 268500,
 	.ref_control =
@@ -1538,36 +1508,6 @@ static const struct intel_mpllb_state dg2_hdmi_241500 = {
 		REG_FIELD_PREP(SNPS_PHY_MPLLB_SSC_UP_SPREAD, 1),
 };
 
-static const struct intel_mpllb_state dg2_hdmi_319890 = {
-	.clock = 319890,
-	.ref_control =
-		REG_FIELD_PREP(SNPS_PHY_REF_CONTROL_REF_RANGE, 3),
-	.mpllb_cp =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_CP_INT, 6) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_CP_PROP, 14) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_CP_INT_GS, 64) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_CP_PROP_GS, 124),
-	.mpllb_div =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_DIV5_CLK_EN, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_TX_CLK_DIV, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_PMIX_EN, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_V2I, 2) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FREQ_VCO, 2),
-	.mpllb_div2 =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_REF_CLK_DIV, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_MULTIPLIER, 94) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_HDMI_DIV, 1),
-	.mpllb_fracn1 =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_CGG_UPDATE_EN, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_EN, 1) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_DEN, 65535),
-	.mpllb_fracn2 =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_QUOT, 64094) |
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_FRACN_REM, 13631),
-	.mpllb_sscen =
-		REG_FIELD_PREP(SNPS_PHY_MPLLB_SSC_UP_SPREAD, 1),
-};
-
 static const struct intel_mpllb_state dg2_hdmi_497750 = {
 	.clock = 497750,
 	.ref_control =
@@ -1755,10 +1695,8 @@ static const struct intel_mpllb_state * const dg2_hdmi_tables[] = {
 	&dg2_hdmi_209800,
 	&dg2_hdmi_241500,
 	&dg2_hdmi_262750,
-	&dg2_hdmi_267300,
 	&dg2_hdmi_268500,
 	&dg2_hdmi_296703,
-	&dg2_hdmi_319890,
 	&dg2_hdmi_497750,
 	&dg2_hdmi_592000,
 	&dg2_hdmi_593407,
diff --git a/drivers/gpu/drm/i915/display/intel_tc.c b/drivers/gpu/drm/i915/display/intel_tc.c
index eecdf5abb41c..e5af955b5600 100644
--- a/drivers/gpu/drm/i915/display/intel_tc.c
+++ b/drivers/gpu/drm/i915/display/intel_tc.c
@@ -440,9 +440,9 @@ static bool icl_tc_phy_is_owned(struct intel_digital_port *dig_port)
 				PORT_TX_DFLEXDPCSSS(dig_port->tc_phy_fia));
 	if (val == 0xffffffff) {
 		drm_dbg_kms(&i915->drm,
-			    "Port %s: PHY in TCCOLD, assume not owned\n",
+			    "Port %s: PHY in TCCOLD, assume safe mode\n",
 			    dig_port->tc_port_name);
-		return false;
+		return true;
 	}
 
 	return val & DP_PHY_MODE_STATUS_NOT_SAFE(dig_port->tc_phy_fia_idx);
@@ -686,59 +686,19 @@ static void intel_tc_port_update_mode(struct intel_digital_port *dig_port,
 static void
 intel_tc_port_link_init_refcount(struct intel_digital_port *dig_port,
 				 int refcount)
-{
-	dig_port->tc_link_refcount = refcount;
-}
-
-/**
- * intel_tc_port_init_mode: Read out HW state and init the given port's TypeC mode
- * @dig_port: digital port
- *
- * Read out the HW state and initialize the TypeC mode of @dig_port. The mode
- * will be locked until intel_tc_port_sanitize_mode() is called.
- */
-void intel_tc_port_init_mode(struct intel_digital_port *dig_port)
 {
 	struct drm_i915_private *i915 = to_i915(dig_port->base.base.dev);
-	intel_wakeref_t tc_cold_wref;
-	enum intel_display_power_domain domain;
 
-	mutex_lock(&dig_port->tc_lock);
-
-	drm_WARN_ON(&i915->drm, dig_port->tc_mode != TC_PORT_DISCONNECTED);
-	drm_WARN_ON(&i915->drm, dig_port->tc_lock_wakeref);
 	drm_WARN_ON(&i915->drm, dig_port->tc_link_refcount);
-
-	tc_cold_wref = tc_cold_block(dig_port, &domain);
-
-	dig_port->tc_mode = intel_tc_port_get_current_mode(dig_port);
-	/* Prevent changing dig_port->tc_mode until intel_tc_port_sanitize_mode() is called. */
-	intel_tc_port_link_init_refcount(dig_port, 1);
-	dig_port->tc_lock_wakeref = tc_cold_block(dig_port, &dig_port->tc_lock_power_domain);
-
-	tc_cold_unblock(dig_port, domain, tc_cold_wref);
-
-	drm_dbg_kms(&i915->drm, "Port %s: init mode (%s)\n",
-		    dig_port->tc_port_name,
-		    tc_port_mode_name(dig_port->tc_mode));
-
-	mutex_unlock(&dig_port->tc_lock);
+	dig_port->tc_link_refcount = refcount;
 }
 
-/**
- * intel_tc_port_sanitize_mode: Sanitize the given port's TypeC mode
- * @dig_port: digital port
- *
- * Sanitize @dig_port's TypeC mode wrt. the encoder's state right after driver
- * loading and system resume:
- * If the encoder is enabled keep the TypeC mode/PHY connected state locked until
- * the encoder is disabled.
- * If the encoder is disabled make sure the PHY is disconnected.
- */
-void intel_tc_port_sanitize_mode(struct intel_digital_port *dig_port)
+void intel_tc_port_sanitize(struct intel_digital_port *dig_port)
 {
 	struct drm_i915_private *i915 = to_i915(dig_port->base.base.dev);
 	struct intel_encoder *encoder = &dig_port->base;
+	intel_wakeref_t tc_cold_wref;
+	enum intel_display_power_domain domain;
 	int active_links = 0;
 
 	mutex_lock(&dig_port->tc_lock);
@@ -748,14 +708,21 @@ void intel_tc_port_sanitize_mode(struct intel_digital_port *dig_port)
 	else if (encoder->base.crtc)
 		active_links = to_intel_crtc(encoder->base.crtc)->active;
 
-	drm_WARN_ON(&i915->drm, dig_port->tc_link_refcount != 1);
-	intel_tc_port_link_init_refcount(dig_port, active_links);
+	drm_WARN_ON(&i915->drm, dig_port->tc_mode != TC_PORT_DISCONNECTED);
+	drm_WARN_ON(&i915->drm, dig_port->tc_lock_wakeref);
+
+	tc_cold_wref = tc_cold_block(dig_port, &domain);
 
+	dig_port->tc_mode = intel_tc_port_get_current_mode(dig_port);
 	if (active_links) {
 		if (!icl_tc_phy_is_connected(dig_port))
 			drm_dbg_kms(&i915->drm,
 				    "Port %s: PHY disconnected with %d active link(s)\n",
 				    dig_port->tc_port_name, active_links);
+		intel_tc_port_link_init_refcount(dig_port, active_links);
+
+		dig_port->tc_lock_wakeref = tc_cold_block(dig_port,
+							  &dig_port->tc_lock_power_domain);
 	} else {
 		/*
 		 * TBT-alt is the default mode in any case the PHY ownership is not
@@ -769,11 +736,10 @@ void intel_tc_port_sanitize_mode(struct intel_digital_port *dig_port)
 				    dig_port->tc_port_name,
 				    tc_port_mode_name(dig_port->tc_mode));
 		icl_tc_phy_disconnect(dig_port);
-
-		tc_cold_unblock(dig_port, dig_port->tc_lock_power_domain,
-				fetch_and_zero(&dig_port->tc_lock_wakeref));
 	}
 
+	tc_cold_unblock(dig_port, domain, tc_cold_wref);
+
 	drm_dbg_kms(&i915->drm, "Port %s: sanitize mode (%s)\n",
 		    dig_port->tc_port_name,
 		    tc_port_mode_name(dig_port->tc_mode));
@@ -957,6 +923,4 @@ void intel_tc_port_init(struct intel_digital_port *dig_port, bool is_legacy)
 	dig_port->tc_mode = TC_PORT_DISCONNECTED;
 	dig_port->tc_link_refcount = 0;
 	tc_port_load_fia_params(i915, dig_port);
-
-	intel_tc_port_init_mode(dig_port);
 }
diff --git a/drivers/gpu/drm/i915/display/intel_tc.h b/drivers/gpu/drm/i915/display/intel_tc.h
index d54082e2d5e8..6b47b29f551c 100644
--- a/drivers/gpu/drm/i915/display/intel_tc.h
+++ b/drivers/gpu/drm/i915/display/intel_tc.h
@@ -24,8 +24,7 @@ int intel_tc_port_fia_max_lane_count(struct intel_digital_port *dig_port);
 void intel_tc_port_set_fia_lane_count(struct intel_digital_port *dig_port,
 				      int required_lanes);
 
-void intel_tc_port_init_mode(struct intel_digital_port *dig_port);
-void intel_tc_port_sanitize_mode(struct intel_digital_port *dig_port);
+void intel_tc_port_sanitize(struct intel_digital_port *dig_port);
 void intel_tc_port_lock(struct intel_digital_port *dig_port);
 void intel_tc_port_unlock(struct intel_digital_port *dig_port);
 void intel_tc_port_flush_work(struct intel_digital_port *dig_port);
diff --git a/drivers/gpu/drm/i915/display/skl_universal_plane.c b/drivers/gpu/drm/i915/display/skl_universal_plane.c
index bc523a3d1d42..7cb713043408 100644
--- a/drivers/gpu/drm/i915/display/skl_universal_plane.c
+++ b/drivers/gpu/drm/i915/display/skl_universal_plane.c
@@ -1620,7 +1620,7 @@ static int skl_check_main_surface(struct intel_plane_state *plane_state)
 	u32 offset;
 	int ret;
 
-	if (w > max_width || w < min_width || h > max_height || h < 1) {
+	if (w > max_width || w < min_width || h > max_height) {
 		drm_dbg_kms(&dev_priv->drm,
 			    "requested Y/RGB source size %dx%d outside limits (min: %dx1 max: %dx%d)\n",
 			    w, h, min_width, max_width, max_height);
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_context.c b/drivers/gpu/drm/i915/gem/i915_gem_context.c
index 0bcde53c50c6..dabdfe09f5e5 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_context.c
@@ -1269,10 +1269,6 @@ static void i915_gem_context_release_work(struct work_struct *work)
 	trace_i915_context_free(ctx);
 	GEM_BUG_ON(!i915_gem_context_is_closed(ctx));
 
-	spin_lock(&ctx->i915->gem.contexts.lock);
-	list_del(&ctx->link);
-	spin_unlock(&ctx->i915->gem.contexts.lock);
-
 	if (ctx->syncobj)
 		drm_syncobj_put(ctx->syncobj);
 
@@ -1525,6 +1521,10 @@ static void context_close(struct i915_gem_context *ctx)
 
 	ctx->file_priv = ERR_PTR(-EBADF);
 
+	spin_lock(&ctx->i915->gem.contexts.lock);
+	list_del(&ctx->link);
+	spin_unlock(&ctx->i915->gem.contexts.lock);
+
 	client = ctx->client;
 	if (client) {
 		spin_lock(&client->ctx_lock);
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_dmabuf.c b/drivers/gpu/drm/i915/gem/i915_gem_dmabuf.c
index 824971a1ceec..f5062d0c6333 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_dmabuf.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_dmabuf.c
@@ -40,13 +40,13 @@ static struct sg_table *i915_gem_map_dma_buf(struct dma_buf_attachment *attachme
 		goto err;
 	}
 
-	ret = sg_alloc_table(st, obj->mm.pages->orig_nents, GFP_KERNEL);
+	ret = sg_alloc_table(st, obj->mm.pages->nents, GFP_KERNEL);
 	if (ret)
 		goto err_free;
 
 	src = obj->mm.pages->sgl;
 	dst = st->sgl;
-	for (i = 0; i < obj->mm.pages->orig_nents; i++) {
+	for (i = 0; i < obj->mm.pages->nents; i++) {
 		sg_set_page(dst, sg_page(src), src->length, 0);
 		dst = sg_next(dst);
 		src = sg_next(src);
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
index 0a5a1291b4d4..9a90b1979841 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
@@ -3337,17 +3337,12 @@ i915_gem_do_execbuffer(struct drm_device *dev,
 
 	eb.batch_flags = 0;
 	if (args->flags & I915_EXEC_SECURE) {
-		if (!i915->params.enable_secure_batch) {
-			if (GRAPHICS_VER(i915) >= 11)
-				return -ENODEV;
+		if (GRAPHICS_VER(i915) >= 11)
+			return -ENODEV;
 
-			/*
-			 * Return -EPERM to trigger fallback code on old
-			 * binaries.
-			 */
-			if (!HAS_SECURE_BATCHES(i915))
-				return -EPERM;
-		}
+		/* Return -EPERM to trigger fallback code on old binaries. */
+		if (!HAS_SECURE_BATCHES(i915))
+			return -EPERM;
 
 		if (!drm_is_current_master(file) || !capable(CAP_SYS_ADMIN))
 			return -EPERM;
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_internal.c b/drivers/gpu/drm/i915/gem/i915_gem_internal.c
index 0ff7325224c1..c698f95af15f 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_internal.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_internal.c
@@ -146,21 +146,6 @@ static const struct drm_i915_gem_object_ops i915_gem_object_internal_ops = {
 	.put_pages = i915_gem_object_put_pages_internal,
 };
 
-/**
- * i915_gem_object_create_internal: create an object with volatile pages
- * @i915: the i915 device
- * @size: the size in bytes of backing storage to allocate for the object
- *
- * Creates a new object that wraps some internal memory for private use.
- * This object is not backed by swappable storage, and as such its contents
- * are volatile and only valid whilst pinned. If the object is reaped by the
- * shrinker, its pages and data will be discarded. Equally, it is not a full
- * GEM object and so not valid for access from userspace. This makes it useful
- * for hardware interfaces like ringbuffers (which are pinned from the time
- * the request is written to the time the hardware stops accessing it), but
- * not for contexts (which need to be preserved when not active for later
- * reuse). Note that it is not cleared upon allocation.
- */
 struct drm_i915_gem_object *
 __i915_gem_object_create_internal(struct drm_i915_private *i915,
 				  const struct drm_i915_gem_object_ops *ops,
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_shmem.c b/drivers/gpu/drm/i915/gem/i915_gem_shmem.c
index 870a59d121a7..81738a25e431 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_shmem.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_shmem.c
@@ -579,7 +579,7 @@ static int shmem_object_init(struct intel_memory_region *mem,
 	mapping_set_gfp_mask(mapping, mask);
 	GEM_BUG_ON(!(mapping_gfp_mask(mapping) & __GFP_RECLAIM));
 
-	i915_gem_object_init(obj, &i915_gem_shmem_ops, &lock_class, flags);
+	i915_gem_object_init(obj, &i915_gem_shmem_ops, &lock_class, 0);
 	obj->mem_flags |= I915_BO_FLAG_STRUCT_PAGE;
 	obj->write_domain = I915_GEM_DOMAIN_CPU;
 	obj->read_domains = I915_GEM_DOMAIN_CPU;
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_tiling.c b/drivers/gpu/drm/i915/gem/i915_gem_tiling.c
index 4f3f12196d8e..fd42b89b7162 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_tiling.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_tiling.c
@@ -305,6 +305,10 @@ i915_gem_object_set_tiling(struct drm_i915_gem_object *obj,
 	spin_unlock(&obj->vma.lock);
 
 	obj->tiling_and_stride = tiling | stride;
+	i915_gem_object_unlock(obj);
+
+	/* Force the fence to be reacquired for GTT access */
+	i915_gem_object_release_mmap_gtt(obj);
 
 	/* Try to preallocate memory required to save swizzling on put-pages */
 	if (i915_gem_object_needs_bit17_swizzle(obj)) {
@@ -317,11 +321,6 @@ i915_gem_object_set_tiling(struct drm_i915_gem_object *obj,
 		obj->bit_17 = NULL;
 	}
 
-	i915_gem_object_unlock(obj);
-
-	/* Force the fence to be reacquired for GTT access */
-	i915_gem_object_release_mmap_gtt(obj);
-
 	return 0;
 }
 
@@ -348,7 +347,7 @@ i915_gem_set_tiling_ioctl(struct drm_device *dev, void *data,
 	struct drm_i915_gem_object *obj;
 	int err;
 
-	if (!to_gt(dev_priv)->ggtt->num_fences && !IS_SRIOV_VF(dev_priv))
+	if (!to_gt(dev_priv)->ggtt->num_fences)
 		return -EOPNOTSUPP;
 
 	obj = i915_gem_object_lookup(file, args->handle);
@@ -431,7 +430,7 @@ i915_gem_get_tiling_ioctl(struct drm_device *dev, void *data,
 	struct drm_i915_gem_object *obj;
 	int err = -ENOENT;
 
-	if (!to_gt(dev_priv)->ggtt->num_fences && !IS_SRIOV_VF(dev_priv))
+	if (!to_gt(dev_priv)->ggtt->num_fences)
 		return -EOPNOTSUPP;
 
 	rcu_read_lock();
diff --git a/drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c b/drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c
index a4858be12ee7..c6ad67b90e8a 100644
--- a/drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c
@@ -179,108 +179,97 @@ static int live_nop_switch(void *arg)
 }
 
 struct parallel_switch {
-	struct kthread_worker *worker;
-	struct kthread_work work;
+	struct task_struct *tsk;
 	struct intel_context *ce[2];
-	int result;
 };
 
-static void __live_parallel_switch1(struct kthread_work *work)
+static int __live_parallel_switch1(void *data)
 {
-	struct parallel_switch *arg =
-		container_of(work, typeof(*arg), work);
+	struct parallel_switch *arg = data;
 	IGT_TIMEOUT(end_time);
 	unsigned long count;
 
 	count = 0;
-	arg->result = 0;
 	do {
 		struct i915_request *rq = NULL;
-		int n;
+		int err, n;
 
-		for (n = 0; !arg->result && n < ARRAY_SIZE(arg->ce); n++) {
+		err = 0;
+		for (n = 0; !err && n < ARRAY_SIZE(arg->ce); n++) {
 			struct i915_request *prev = rq;
 
 			rq = i915_request_create(arg->ce[n]);
 			if (IS_ERR(rq)) {
 				i915_request_put(prev);
-				arg->result = PTR_ERR(rq);
-				break;
+				return PTR_ERR(rq);
 			}
 
 			i915_request_get(rq);
 			if (prev) {
-				arg->result =
-					i915_request_await_dma_fence(rq,
-								     &prev->fence);
+				err = i915_request_await_dma_fence(rq, &prev->fence);
 				i915_request_put(prev);
 			}
 
 			i915_request_add(rq);
 		}
-
-		if (IS_ERR_OR_NULL(rq))
-			break;
-
 		if (i915_request_wait(rq, 0, HZ) < 0)
-			arg->result = -ETIME;
-
+			err = -ETIME;
 		i915_request_put(rq);
+		if (err)
+			return err;
 
 		count++;
-	} while (!arg->result && !__igt_timeout(end_time, NULL));
+	} while (!__igt_timeout(end_time, NULL));
 
-	pr_info("%s: %lu switches (sync) <%d>\n",
-		arg->ce[0]->engine->name, count, arg->result);
+	pr_info("%s: %lu switches (sync)\n", arg->ce[0]->engine->name, count);
+	return 0;
 }
 
-static void __live_parallel_switchN(struct kthread_work *work)
+static int __live_parallel_switchN(void *data)
 {
-	struct parallel_switch *arg =
-		container_of(work, typeof(*arg), work);
+	struct parallel_switch *arg = data;
 	struct i915_request *rq = NULL;
 	IGT_TIMEOUT(end_time);
 	unsigned long count;
 	int n;
 
 	count = 0;
-	arg->result = 0;
 	do {
-		for (n = 0; !arg->result && n < ARRAY_SIZE(arg->ce); n++) {
+		for (n = 0; n < ARRAY_SIZE(arg->ce); n++) {
 			struct i915_request *prev = rq;
+			int err = 0;
 
 			rq = i915_request_create(arg->ce[n]);
 			if (IS_ERR(rq)) {
 				i915_request_put(prev);
-				arg->result = PTR_ERR(rq);
-				break;
+				return PTR_ERR(rq);
 			}
 
 			i915_request_get(rq);
 			if (prev) {
-				arg->result =
-					i915_request_await_dma_fence(rq,
-								     &prev->fence);
+				err = i915_request_await_dma_fence(rq, &prev->fence);
 				i915_request_put(prev);
 			}
 
 			i915_request_add(rq);
+			if (err) {
+				i915_request_put(rq);
+				return err;
+			}
 		}
 
 		count++;
-	} while (!arg->result && !__igt_timeout(end_time, NULL));
-
-	if (!IS_ERR_OR_NULL(rq))
-		i915_request_put(rq);
+	} while (!__igt_timeout(end_time, NULL));
+	i915_request_put(rq);
 
-	pr_info("%s: %lu switches (many) <%d>\n",
-		arg->ce[0]->engine->name, count, arg->result);
+	pr_info("%s: %lu switches (many)\n", arg->ce[0]->engine->name, count);
+	return 0;
 }
 
 static int live_parallel_switch(void *arg)
 {
 	struct drm_i915_private *i915 = arg;
-	static void (* const func[])(struct kthread_work *) = {
+	static int (* const func[])(void *arg) = {
 		__live_parallel_switch1,
 		__live_parallel_switchN,
 		NULL,
@@ -288,7 +277,7 @@ static int live_parallel_switch(void *arg)
 	struct parallel_switch *data = NULL;
 	struct i915_gem_engines *engines;
 	struct i915_gem_engines_iter it;
-	void (* const *fn)(struct kthread_work *);
+	int (* const *fn)(void *arg);
 	struct i915_gem_context *ctx;
 	struct intel_context *ce;
 	struct file *file;
@@ -346,10 +335,8 @@ static int live_parallel_switch(void *arg)
 				continue;
 
 			ce = intel_context_create(data[m].ce[0]->engine);
-			if (IS_ERR(ce)) {
-				err = PTR_ERR(ce);
+			if (IS_ERR(ce))
 				goto out;
-			}
 
 			err = intel_context_pin(ce);
 			if (err) {
@@ -361,24 +348,9 @@ static int live_parallel_switch(void *arg)
 		}
 	}
 
-	for (n = 0; n < count; n++) {
-		struct kthread_worker *worker;
-
-		if (!data[n].ce[0])
-			continue;
-
-		worker = kthread_create_worker(0, "igt/parallel:%s",
-					       data[n].ce[0]->engine->name);
-		if (IS_ERR(worker)) {
-			err = PTR_ERR(worker);
-			goto out;
-		}
-
-		data[n].worker = worker;
-	}
-
 	for (fn = func; !err && *fn; fn++) {
 		struct igt_live_test t;
+		int n;
 
 		err = igt_live_test_begin(&t, i915, __func__, "");
 		if (err)
@@ -388,23 +360,34 @@ static int live_parallel_switch(void *arg)
 			if (!data[n].ce[0])
 				continue;
 
-			data[n].result = 0;
-			kthread_init_work(&data[n].work, *fn);
-			kthread_queue_work(data[n].worker, &data[n].work);
+			data[n].tsk = kthread_run(*fn, &data[n],
+						  "igt/parallel:%s",
+						  data[n].ce[0]->engine->name);
+			if (IS_ERR(data[n].tsk)) {
+				err = PTR_ERR(data[n].tsk);
+				break;
+			}
+			get_task_struct(data[n].tsk);
 		}
 
+		yield(); /* start all threads before we kthread_stop() */
+
 		for (n = 0; n < count; n++) {
-			if (data[n].ce[0]) {
-				kthread_flush_work(&data[n].work);
-				if (data[n].result && !err)
-					err = data[n].result;
-			}
-		}
+			int status;
 
-		if (igt_live_test_end(&t)) {
-			err = err ?: -EIO;
-			break;
+			if (IS_ERR_OR_NULL(data[n].tsk))
+				continue;
+
+			status = kthread_stop(data[n].tsk);
+			if (status && !err)
+				err = status;
+
+			put_task_struct(data[n].tsk);
+			data[n].tsk = NULL;
 		}
+
+		if (igt_live_test_end(&t))
+			err = -EIO;
 	}
 
 out:
@@ -416,9 +399,6 @@ static int live_parallel_switch(void *arg)
 			intel_context_unpin(data[n].ce[m]);
 			intel_context_put(data[n].ce[m]);
 		}
-
-		if (data[n].worker)
-			kthread_destroy_worker(data[n].worker);
 	}
 	kfree(data);
 out_file:
diff --git a/drivers/gpu/drm/i915/gt/intel_context.c b/drivers/gpu/drm/i915/gt/intel_context.c
index 77b3b3823f8c..654a092ed3d6 100644
--- a/drivers/gpu/drm/i915/gt/intel_context.c
+++ b/drivers/gpu/drm/i915/gt/intel_context.c
@@ -576,23 +576,16 @@ void intel_context_bind_parent_child(struct intel_context *parent,
 	child->parallel.parent = parent;
 }
 
-u64 intel_context_get_total_runtime_ns(struct intel_context *ce)
+u64 intel_context_get_total_runtime_ns(const struct intel_context *ce)
 {
 	u64 total, active;
 
-	if (ce->ops->update_stats)
-		ce->ops->update_stats(ce);
-
 	total = ce->stats.runtime.total;
 	if (ce->ops->flags & COPS_RUNTIME_CYCLES)
 		total *= ce->engine->gt->clock_period_ns;
 
 	active = READ_ONCE(ce->stats.active);
-	/*
-	 * GuC backend returns the actual time the context was active, so skip
-	 * the calculation here for GuC.
-	 */
-	if (active && !intel_engine_uses_guc(ce->engine))
+	if (active)
 		active = intel_context_clock() - active;
 
 	return total + active;
diff --git a/drivers/gpu/drm/i915/gt/intel_context.h b/drivers/gpu/drm/i915/gt/intel_context.h
index 3d1d7436c1a4..8e2d70630c49 100644
--- a/drivers/gpu/drm/i915/gt/intel_context.h
+++ b/drivers/gpu/drm/i915/gt/intel_context.h
@@ -58,7 +58,7 @@ static inline bool intel_context_is_parent(struct intel_context *ce)
 	return !!ce->parallel.number_children;
 }
 
-static inline bool intel_context_is_pinned(const struct intel_context *ce);
+static inline bool intel_context_is_pinned(struct intel_context *ce);
 
 static inline struct intel_context *
 intel_context_to_parent(struct intel_context *ce)
@@ -118,7 +118,7 @@ static inline int intel_context_lock_pinned(struct intel_context *ce)
  * Returns: true if the context is currently pinned for use by the GPU.
  */
 static inline bool
-intel_context_is_pinned(const struct intel_context *ce)
+intel_context_is_pinned(struct intel_context *ce)
 {
 	return atomic_read(&ce->pin_count);
 }
@@ -362,7 +362,7 @@ intel_context_clear_nopreempt(struct intel_context *ce)
 	clear_bit(CONTEXT_NOPREEMPT, &ce->flags);
 }
 
-u64 intel_context_get_total_runtime_ns(struct intel_context *ce);
+u64 intel_context_get_total_runtime_ns(const struct intel_context *ce);
 u64 intel_context_get_avg_runtime_ns(struct intel_context *ce);
 
 static inline u64 intel_context_clock(void)
diff --git a/drivers/gpu/drm/i915/gt/intel_context_types.h b/drivers/gpu/drm/i915/gt/intel_context_types.h
index 85d57d5a2e62..04eacae1aca5 100644
--- a/drivers/gpu/drm/i915/gt/intel_context_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_context_types.h
@@ -56,8 +56,6 @@ struct intel_context_ops {
 
 	void (*sched_disable)(struct intel_context *ce);
 
-	void (*update_stats)(struct intel_context *ce);
-
 	void (*reset)(struct intel_context *ce);
 	void (*destroy)(struct kref *kref);
 
@@ -150,7 +148,6 @@ struct intel_context {
 			struct ewma_runtime avg;
 			u64 total;
 			u32 last;
-			u64 start_gt_clk;
 			I915_SELFTEST_DECLARE(u32 num_underflow);
 			I915_SELFTEST_DECLARE(u32 max_underflow);
 		} runtime;
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_cs.c b/drivers/gpu/drm/i915/gt/intel_engine_cs.c
index e15639d7d4cd..1f7188129cd1 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_cs.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_cs.c
@@ -359,9 +359,6 @@ static void __sprint_engine_name(struct intel_engine_cs *engine)
 
 void intel_engine_set_hwsp_writemask(struct intel_engine_cs *engine, u32 mask)
 {
-	if (IS_SRIOV_VF(engine->i915))
-		return;
-
 	/*
 	 * Though they added more rings on g4x/ilk, they did not add
 	 * per-engine HWSTAM until gen6.
@@ -1839,10 +1836,6 @@ static void intel_engine_print_registers(struct intel_engine_cs *engine,
 	struct intel_engine_execlists * const execlists = &engine->execlists;
 	u64 addr;
 
-	/* VF can't access these registers */
-	if (IS_SRIOV_VF(dev_priv))
-		return;
-
 	if (engine->id == RENDER_CLASS && IS_GRAPHICS_VER(dev_priv, 4, 7))
 		drm_printf(m, "\tCCID: 0x%08x\n", ENGINE_READ(engine, CCID));
 	if (HAS_EXECLISTS(dev_priv)) {
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 2c2333c40428..03dd71475106 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -165,21 +165,6 @@ struct intel_engine_execlists {
 	 */
 	struct timer_list preempt;
 
-	/**
-	 * @preempt_target: active request at the time of the preemption request
-	 *
-	 * We force a preemption to occur if the pending contexts have not
-	 * been promoted to active upon receipt of the CS ack event within
-	 * the timeout. This timeout maybe chosen based on the target,
-	 * using a very short timeout if the context is no longer schedulable.
-	 * That short timeout may not be applicable to other contexts, so
-	 * if a context switch should happen within before the preemption
-	 * timeout, we may shoot early at an innocent context. To prevent this,
-	 * we record which context was active at the time of the preemption
-	 * request and only reset that context upon the timeout.
-	 */
-	const struct i915_request *preempt_target;
-
 	/**
 	 * @ccid: identifier for contexts submitted to this engine
 	 */
diff --git a/drivers/gpu/drm/i915/gt/intel_execlists_submission.c b/drivers/gpu/drm/i915/gt/intel_execlists_submission.c
index 363c833e14a4..b18c762a3c1b 100644
--- a/drivers/gpu/drm/i915/gt/intel_execlists_submission.c
+++ b/drivers/gpu/drm/i915/gt/intel_execlists_submission.c
@@ -1241,9 +1241,6 @@ static unsigned long active_preempt_timeout(struct intel_engine_cs *engine,
 	if (!rq)
 		return 0;
 
-	/* Only allow ourselves to force reset the currently active context */
-	engine->execlists.preempt_target = rq;
-
 	/* Force a fast reset for terminated contexts (ignoring sysfs!) */
 	if (unlikely(intel_context_is_banned(rq->context) || bad_request(rq)))
 		return INTEL_CONTEXT_BANNED_PREEMPT_TIMEOUT_MS;
@@ -2430,24 +2427,8 @@ static void execlists_submission_tasklet(struct tasklet_struct *t)
 	GEM_BUG_ON(inactive - post > ARRAY_SIZE(post));
 
 	if (unlikely(preempt_timeout(engine))) {
-		const struct i915_request *rq = *engine->execlists.active;
-
-		/*
-		 * If after the preempt-timeout expired, we are still on the
-		 * same active request/context as before we initiated the
-		 * preemption, reset the engine.
-		 *
-		 * However, if we have processed a CS event to switch contexts,
-		 * but not yet processed the CS event for the pending
-		 * preemption, reset the timer allowing the new context to
-		 * gracefully exit.
-		 */
 		cancel_timer(&engine->execlists.preempt);
-		if (rq == engine->execlists.preempt_target)
-			engine->execlists.error_interrupt |= ERROR_PREEMPT;
-		else
-			set_timer_ms(&engine->execlists.preempt,
-				     active_preempt_timeout(engine, rq));
+		engine->execlists.error_interrupt |= ERROR_PREEMPT;
 	}
 
 	if (unlikely(READ_ONCE(engine->execlists.error_interrupt))) {
diff --git a/drivers/gpu/drm/i915/gt/intel_ggtt.c b/drivers/gpu/drm/i915/gt/intel_ggtt.c
index 60ad385fa690..ab74245e231a 100644
--- a/drivers/gpu/drm/i915/gt/intel_ggtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_ggtt.c
@@ -22,7 +22,6 @@
 #include "i915_scatterlist.h"
 #include "i915_utils.h"
 #include "i915_vgpu.h"
-#include "iov/intel_iov.h"
 
 #include "intel_gtt.h"
 #include "gen8_ppgtt.h"
@@ -238,19 +237,6 @@ static void guc_ggtt_invalidate(struct i915_ggtt *ggtt)
 		intel_uncore_write_fw(uncore, GEN8_GTCR, GEN8_GTCR_INVALIDATE);
 }
 
-static void gen12vf_ggtt_invalidate(struct i915_ggtt *ggtt)
-{
-	struct intel_gt *gt = ggtt->vm.gt;
-	struct intel_guc *guc = &gt->uc.guc;
-	intel_wakeref_t wakeref;
-
-	if (!guc->ct.enabled)
-		return;
-
-	with_intel_runtime_pm(gt->uncore->rpm, wakeref)
-		intel_guc_invalidate_tlb_guc(guc, INTEL_GUC_TLB_INVAL_MODE_HEAVY);
-}
-
 u64 gen8_ggtt_pte_encode(dma_addr_t addr,
 			 enum i915_cache_level level,
 			 u32 flags)
@@ -263,23 +249,11 @@ u64 gen8_ggtt_pte_encode(dma_addr_t addr,
 	return pte;
 }
 
-void gen8_set_pte(void __iomem *addr, gen8_pte_t pte)
+static void gen8_set_pte(void __iomem *addr, gen8_pte_t pte)
 {
 	writeq(pte, addr);
 }
 
-gen8_pte_t gen8_get_pte(void __iomem *addr)
-{
-	return readq(addr);
-}
-
-u64 ggtt_addr_to_pte_offset(u64 ggtt_addr)
-{
-	GEM_BUG_ON(!IS_ALIGNED(ggtt_addr, I915_GTT_PAGE_SIZE_4K));
-
-	return (ggtt_addr / I915_GTT_PAGE_SIZE_4K) * sizeof(gen8_pte_t);
-}
-
 static void gen8_ggtt_insert_page(struct i915_address_space *vm,
 				  dma_addr_t addr,
 				  u64 offset,
@@ -581,10 +555,6 @@ static int init_ggtt(struct i915_ggtt *ggtt)
 	if (ret)
 		return ret;
 
-	ret = intel_iov_init_ggtt(&ggtt->vm.gt->iov);
-	if (ret)
-		return ret;
-
 	mutex_init(&ggtt->error_mutex);
 	if (ggtt->mappable_end) {
 		/*
@@ -793,7 +763,6 @@ static void ggtt_cleanup_hw(struct i915_ggtt *ggtt)
 	mutex_destroy(&ggtt->error_mutex);
 
 	ggtt_release_guc_top(ggtt);
-	intel_iov_fini_ggtt(&ggtt->vm.gt->iov);
 	intel_vgt_deballoon(ggtt);
 
 	ggtt->vm.cleanup(&ggtt->vm);
@@ -1160,38 +1129,6 @@ static int gen6_gmch_probe(struct i915_ggtt *ggtt)
 	return ggtt_probe_common(ggtt, size);
 }
 
-static int gen12vf_ggtt_probe(struct i915_ggtt *ggtt)
-{
-	struct drm_i915_private *i915 = ggtt->vm.i915;
-
-	GEM_BUG_ON(!IS_SRIOV_VF(i915));
-	GEM_BUG_ON(GRAPHICS_VER(i915) < 12);
-
-	/* there is no apperture on VFs */
-	ggtt->gmadr = (struct resource) DEFINE_RES_MEM(0, 0);
-	ggtt->mappable_end = 0;
-
-	ggtt->vm.alloc_pt_dma = alloc_pt_dma;
-	ggtt->vm.alloc_scratch_dma = alloc_pt_dma;
-
-	/* safe guess as native expects the same minimum */
-	ggtt->vm.total = 1ULL << (ilog2(GUC_GGTT_TOP - 1) + 1); /* roundup_pow_of_two(GUC_GGTT_TOP); */
-
-	ggtt->vm.pte_encode = gen8_ggtt_pte_encode;
-	ggtt->vm.clear_range = nop_clear_range;
-	ggtt->vm.insert_page = gen8_ggtt_insert_page;
-	ggtt->vm.insert_entries = gen8_ggtt_insert_entries;
-	ggtt->vm.cleanup = gen6_gmch_remove;
-
-	ggtt->vm.vma_ops.bind_vma    = intel_ggtt_bind_vma;
-	ggtt->vm.vma_ops.unbind_vma  = intel_ggtt_unbind_vma;
-
-	ggtt->invalidate = gen12vf_ggtt_invalidate;
-
-	return ggtt_probe_common(ggtt, sizeof(gen8_pte_t) *
-				 (ggtt->vm.total >> PAGE_SHIFT));
-}
-
 static int ggtt_probe_hw(struct i915_ggtt *ggtt, struct intel_gt *gt)
 {
 	struct drm_i915_private *i915 = gt->i915;
@@ -1202,14 +1139,13 @@ static int ggtt_probe_hw(struct i915_ggtt *ggtt, struct intel_gt *gt)
 	ggtt->vm.dma = i915->drm.dev;
 	dma_resv_init(&ggtt->vm._resv);
 
-	if (GRAPHICS_VER(i915) <= 5)
-		ret = intel_ggtt_gmch_probe(ggtt);
-	else if (GRAPHICS_VER(i915) < 8)
+	if (GRAPHICS_VER(i915) >= 8)
+		ret = gen8_gmch_probe(ggtt);
+	else if (GRAPHICS_VER(i915) >= 6)
 		ret = gen6_gmch_probe(ggtt);
-	else if (IS_SRIOV_VF(i915))
-		ret = gen12vf_ggtt_probe(ggtt);
 	else
-		ret = gen8_gmch_probe(ggtt);
+		ret = intel_ggtt_gmch_probe(ggtt);
+
 	if (ret) {
 		dma_resv_fini(&ggtt->vm._resv);
 		return ret;
@@ -1417,143 +1353,6 @@ void i915_ggtt_deballoon(struct i915_ggtt *ggtt, struct drm_mm_node *node)
 	drm_mm_remove_node(node);
 }
 
-static gen8_pte_t tgl_prepare_vf_pte_vfid(u16 vfid)
-{
-	GEM_BUG_ON(!FIELD_FIT(TGL_GGTT_PTE_VFID_MASK, vfid));
-
-	return FIELD_PREP(TGL_GGTT_PTE_VFID_MASK, vfid);
-}
-
-static gen8_pte_t prepare_vf_pte(u16 vfid)
-{
-	return tgl_prepare_vf_pte_vfid(vfid) | GEN8_PAGE_PRESENT;
-}
-
-void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
-			       const struct drm_mm_node *node)
-{
-	gen8_pte_t __iomem *gtt_entries = ggtt->gsm;
-	const gen8_pte_t pte = prepare_vf_pte(vfid);
-	u64 base = node->start;
-	u64 size = node->size;
-
-	GEM_BUG_ON(!IS_SRIOV_PF(ggtt->vm.i915));
-	GEM_BUG_ON(base % PAGE_SIZE);
-	GEM_BUG_ON(size % PAGE_SIZE);
-
-	drm_dbg(&ggtt->vm.i915->drm, "GGTT VF%u [%#llx-%#llx] %lluK\n",
-		vfid, base, base + size, size / SZ_1K);
-
-	gtt_entries += base >> PAGE_SHIFT;
-	while (size) {
-		gen8_set_pte(gtt_entries++, pte);
-		size -= PAGE_SIZE;
-	}
-
-	ggtt->invalidate(ggtt);
-}
-
-static inline unsigned int __ggtt_size_to_ptes_size(u64 ggtt_size)
-{
-	GEM_BUG_ON(!IS_ALIGNED(ggtt_size, I915_GTT_MIN_ALIGNMENT));
-
-	return (ggtt_size >> PAGE_SHIFT) * sizeof(gen8_pte_t);
-}
-
-static void ggtt_pte_clear_vfid(void *buf, u64 size)
-{
-	while (size) {
-		*(gen8_pte_t *)buf &= ~TGL_GGTT_PTE_VFID_MASK;
-
-		buf += sizeof(gen8_pte_t);
-		size -= sizeof(gen8_pte_t);
-	}
-}
-
-/**
- * i915_ggtt_save_ptes - copy GGTT PTEs to preallocated buffer
- * @ggtt: the &struct i915_ggtt
- * @node: the &struct drm_mm_node - the @node->start is used as the start offset for save
- * @buf: preallocated buffer in which PTEs will be saved
- * @size: size of prealocated buffer (in bytes)
- *        - must be sizeof(gen8_pte_t) aligned
- * @flags: function flags:
- *         - #I915_GGTT_SAVE_PTES_NO_VFID BIT - save PTEs without VFID
- *
- * Returns: size of the buffer used (or needed if both @buf and @size are (0)) to store all PTEs
- *          for a given node, -EINVAL if one of @buf or @size is 0.
- */
-int i915_ggtt_save_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node, void *buf,
-			unsigned int size, unsigned int flags)
-{
-	gen8_pte_t __iomem *gtt_entries = ggtt->gsm;
-
-	if (!buf && !size)
-		return __ggtt_size_to_ptes_size(node->size);
-
-	if (!buf || !size)
-		return -EINVAL;
-
-	GEM_BUG_ON(!IS_ALIGNED(size, sizeof(gen8_pte_t)));
-	GEM_WARN_ON(size > __ggtt_size_to_ptes_size(SZ_4G));
-
-	if (size < __ggtt_size_to_ptes_size(node->size))
-		return -ENOSPC;
-	size = __ggtt_size_to_ptes_size(node->size);
-
-	gtt_entries += node->start >> PAGE_SHIFT;
-
-	memcpy_fromio(buf, gtt_entries, size);
-
-	if (flags & I915_GGTT_SAVE_PTES_NO_VFID)
-		ggtt_pte_clear_vfid(buf, size);
-
-	return size;
-}
-
-/**
- * i915_ggtt_restore_ptes() -  restore GGTT PTEs from buffer
- * @ggtt: the &struct i915_ggtt
- * @node: the &struct drm_mm_node - the @node->start is used as the start offset for restore
- * @buf: buffer from which PTEs will be restored
- * @size: size of prealocated buffer (in bytes)
- *        - must be sizeof(gen8_pte_t) aligned
- * @flags: function flags:
- *         - #I915_GGTT_RESTORE_PTES_VFID_MASK - VFID for restored PTEs
- *         - #I915_GGTT_RESTORE_PTES_NEW_VFID - restore PTEs with new VFID
- *           (from #I915_GGTT_RESTORE_PTES_VFID_MASK)
- *
- * Returns: 0 on success, -ENOSPC if @node->size is less than size.
- */
-int i915_ggtt_restore_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node, const void *buf,
-			   unsigned int size, unsigned int flags)
-{
-	gen8_pte_t __iomem *gtt_entries = ggtt->gsm;
-	u32 vfid = FIELD_GET(I915_GGTT_RESTORE_PTES_VFID_MASK, flags);
-	gen8_pte_t pte;
-
-	GEM_BUG_ON(!size);
-	GEM_BUG_ON(!IS_ALIGNED(size, sizeof(gen8_pte_t)));
-
-	if (size > __ggtt_size_to_ptes_size(node->size))
-		return -ENOSPC;
-
-	gtt_entries += node->start >> PAGE_SHIFT;
-
-	while (size) {
-		pte = *(gen8_pte_t *)buf;
-		if (flags & I915_GGTT_RESTORE_PTES_NEW_VFID)
-			pte |= tgl_prepare_vf_pte_vfid(vfid);
-		gen8_set_pte(gtt_entries++, pte);
-
-		buf += sizeof(gen8_pte_t);
-		size -= sizeof(gen8_pte_t);
-	}
-
-	ggtt->invalidate(ggtt);
-
-	return 0;
-}
 void i915_ggtt_mark_pte_lost(struct drm_i915_private *i915, bool val)
 {
 	to_gt(i915)->ggtt->pte_lost = val;
diff --git a/drivers/gpu/drm/i915/gt/intel_gt.c b/drivers/gpu/drm/i915/gt/intel_gt.c
index f199e71cf88f..7716204a3cc1 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt.c
@@ -34,10 +34,7 @@
 #include "intel_sa_media.h"
 #include "intel_gt_sysfs.h"
 #include "intel_uncore.h"
-#include "intel_pm.h"
-#include "iov/intel_iov.h"
 #include "shmem_utils.h"
-#include "uc/intel_guc.h"
 
 void intel_gt_common_init_early(struct intel_gt *gt)
 {
@@ -120,15 +117,9 @@ int intel_gt_assign_ggtt(struct intel_gt *gt)
 
 int intel_gt_init_mmio(struct intel_gt *gt)
 {
-	int ret;
-
-	ret = intel_iov_init_mmio(&gt->iov);
-	if (ret)
-		return ret;
-
 	intel_gt_init_clock_frequency(gt);
-	intel_uc_init_mmio(&gt->uc);
 
+	intel_uc_init_mmio(&gt->uc);
 	intel_sseu_info_init(gt);
 	intel_gt_mcr_init(gt);
 
@@ -212,13 +203,6 @@ int intel_gt_init_hw(struct intel_gt *gt)
 		goto out;
 	}
 
-	ret = intel_iov_init_hw(&gt->iov);
-	if (unlikely(ret)) {
-		i915_probe_error(i915, "Enabling IOV failed (%pe)\n",
-				 ERR_PTR(ret));
-		goto out;
-	}
-
 	intel_mocs_init(gt);
 
 out:
@@ -360,9 +344,6 @@ void intel_gt_check_and_clear_faults(struct intel_gt *gt)
 {
 	struct drm_i915_private *i915 = gt->i915;
 
-	if (IS_SRIOV_VF(i915))
-		return;
-
 	/* From GEN8 onwards we only have one 'All Engine Fault Register' */
 	if (GRAPHICS_VER(i915) >= 8)
 		gen8_check_faults(gt);
@@ -645,13 +626,8 @@ int intel_gt_wait_for_idle(struct intel_gt *gt, long timeout)
 			return -EINTR;
 	}
 
-	if (timeout)
-		return timeout;
-
-	if (remaining_timeout < 0)
-		remaining_timeout = 0;
-
-	return intel_uc_wait_for_idle(&gt->uc, remaining_timeout);
+	return timeout ? timeout : intel_uc_wait_for_idle(&gt->uc,
+							  remaining_timeout);
 }
 
 int intel_gt_init(struct intel_gt *gt)
@@ -673,14 +649,10 @@ int intel_gt_init(struct intel_gt *gt)
 	 */
 	intel_uncore_forcewake_get(gt->uncore, FORCEWAKE_ALL);
 
-	err = intel_iov_init(&gt->iov);
-	if (unlikely(err))
-		goto out_fw;
-
 	err = intel_gt_init_scratch(gt,
 				    GRAPHICS_VER(gt->i915) == 2 ? SZ_256K : SZ_4K);
 	if (err)
-		goto err_iov;
+		goto out_fw;
 
 	intel_gt_pm_init(gt);
 
@@ -709,10 +681,6 @@ int intel_gt_init(struct intel_gt *gt)
 		drm_err(&gt->i915->drm, "Failed to retrieve hwconfig table: %pe\n",
 			ERR_PTR(err));
 
-	err = intel_iov_init_late(&gt->iov);
-	if (err)
-		goto err_gt;
-
 	err = __engines_record_defaults(gt);
 	if (err)
 		goto err_gt;
@@ -721,12 +689,12 @@ int intel_gt_init(struct intel_gt *gt)
 	if (err)
 		goto err_gt;
 
+	intel_uc_init_late(&gt->uc);
+
 	err = i915_inject_probe_error(gt->i915, -EIO);
 	if (err)
 		goto err_gt;
 
-	intel_uc_init_late(&gt->uc);
-
 	intel_migrate_init(&gt->migrate, gt);
 
 	intel_pxp_init(&gt->pxp);
@@ -743,8 +711,6 @@ int intel_gt_init(struct intel_gt *gt)
 err_pm:
 	intel_gt_pm_fini(gt);
 	intel_gt_fini_scratch(gt);
-err_iov:
-	intel_iov_fini(&gt->iov);
 out_fw:
 	if (err)
 		intel_gt_set_wedged_on_init(gt);
@@ -754,10 +720,6 @@ int intel_gt_init(struct intel_gt *gt)
 
 void intel_gt_driver_remove(struct intel_gt *gt)
 {
-	intel_gt_fini_clock_frequency(gt);
-
-	intel_iov_fini_hw(&gt->iov);
-
 	__intel_gt_disable(gt);
 
 	intel_migrate_fini(&gt->migrate);
@@ -803,7 +765,6 @@ void intel_gt_driver_release(struct intel_gt *gt)
 	intel_gt_fini_scratch(gt);
 	intel_gt_fini_buffer_pool(gt);
 	intel_gt_fini_hwconfig(gt);
-	intel_iov_fini(&gt->iov);
 }
 
 void intel_gt_driver_late_release_all(struct drm_i915_private *i915)
@@ -815,7 +776,6 @@ void intel_gt_driver_late_release_all(struct drm_i915_private *i915)
 	rcu_barrier();
 
 	for_each_gt(gt, i915, id) {
-		intel_iov_release(&gt->iov);
 		intel_uc_driver_late_release(&gt->uc);
 		intel_gt_fini_requests(gt);
 		intel_gt_fini_reset(gt);
@@ -1058,10 +1018,6 @@ static void mmio_invalidate_full(struct intel_gt *gt)
 		if (!i915_mmio_reg_offset(rb.reg))
 			continue;
 
-		if (GRAPHICS_VER(i915) == 12 && (engine->class == VIDEO_DECODE_CLASS ||
-		    engine->class == VIDEO_ENHANCEMENT_CLASS))
-			rb.bit = _MASKED_BIT_ENABLE(rb.bit);
-
 		intel_uncore_write_fw(uncore, rb.reg, rb.bit);
 		awake |= engine->mask;
 	}
@@ -1125,15 +1081,11 @@ void intel_gt_invalidate_tlb(struct intel_gt *gt, u32 seqno)
 		return;
 
 	with_intel_gt_pm_if_awake(gt, wakeref) {
-		struct intel_guc *guc = &gt->uc.guc;
 		mutex_lock(&gt->tlb.invalidate_lock);
 		if (tlb_seqno_passed(gt, seqno))
 			goto unlock;
 
-		if (INTEL_GUC_SUPPORTS_TLB_INVALIDATION(guc)) {
-			intel_guc_invalidate_tlb_guc(guc, INTEL_GUC_TLB_INVAL_MODE_HEAVY);
-		} else
-			mmio_invalidate_full(gt);
+		mmio_invalidate_full(gt);
 
 		write_seqcount_invalidate(&gt->tlb.seqno);
 unlock:
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.c b/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.c
index 698b05971784..3f656d3dba9a 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.c
@@ -161,18 +161,9 @@ void intel_gt_init_clock_frequency(struct intel_gt *gt)
 			 USEC_PER_SEC));
 }
 
-void intel_gt_fini_clock_frequency(struct intel_gt *gt)
-{
-	/* Clock registers no longer accessible, stop checking */
-	gt->clock_frequency = 0;
-}
-
 #if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)
 void intel_gt_check_clock_frequency(const struct intel_gt *gt)
 {
-	if (!gt->clock_frequency)
-		return;
-
 	if (gt->clock_frequency != read_clock_frequency(gt->uncore)) {
 		dev_err(gt->i915->drm.dev,
 			"GT clock frequency changed, was %uHz, now %uHz!\n",
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.h b/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.h
index c923b1866b08..8b03e97a85df 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.h
+++ b/drivers/gpu/drm/i915/gt/intel_gt_clock_utils.h
@@ -11,7 +11,6 @@
 struct intel_gt;
 
 void intel_gt_init_clock_frequency(struct intel_gt *gt);
-void intel_gt_fini_clock_frequency(struct intel_gt *gt);
 
 #if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)
 void intel_gt_check_clock_frequency(const struct intel_gt *gt);
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_debugfs.c b/drivers/gpu/drm/i915/gt/intel_gt_debugfs.c
index fa07485b2109..dd53641f3637 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_debugfs.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_debugfs.c
@@ -13,7 +13,6 @@
 #include "intel_gt_pm_debugfs.h"
 #include "intel_sseu_debugfs.h"
 #include "pxp/intel_pxp_debugfs.h"
-#include "iov/intel_iov_debugfs.h"
 #include "uc/intel_uc_debugfs.h"
 
 int intel_gt_debugfs_reset_show(struct intel_gt *gt, u64 *val)
@@ -101,7 +100,6 @@ void intel_gt_debugfs_register(struct intel_gt *gt)
 
 	intel_uc_debugfs_register(&gt->uc, root);
 	intel_pxp_debugfs_register(&gt->pxp, root);
-	intel_iov_debugfs_register(&gt->iov, root);
 }
 
 void intel_gt_debugfs_register_files(struct dentry *root,
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_pm.c b/drivers/gpu/drm/i915/gt/intel_gt_pm.c
index 6ec762a049c2..ff306f18a522 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_pm.c
@@ -19,7 +19,6 @@
 #include "intel_rc6.h"
 #include "intel_rps.h"
 #include "intel_wakeref.h"
-#include "iov/intel_iov.h"
 #include "pxp/intel_pxp_pm.h"
 
 #define I915_GT_SUSPEND_IDLE_TIMEOUT (HZ / 2)
@@ -313,8 +312,6 @@ static void wait_for_suspend(struct intel_gt *gt)
 
 void intel_gt_suspend_prepare(struct intel_gt *gt)
 {
-	intel_iov_suspend(&gt->iov);
-
 	user_forcewake(gt, true);
 	wait_for_suspend(gt);
 
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_requests.c b/drivers/gpu/drm/i915/gt/intel_gt_requests.c
index 1dfd01668c79..edb881d75630 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_requests.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_requests.c
@@ -199,7 +199,7 @@ out_active:	spin_lock(&timelines->lock);
 	if (remaining_timeout)
 		*remaining_timeout = timeout;
 
-	return active_count ? timeout ?: -ETIME : 0;
+	return active_count ? timeout : 0;
 }
 
 static void retire_work_handler(struct work_struct *work)
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_types.h b/drivers/gpu/drm/i915/gt/intel_gt_types.h
index a440eb571a96..f19c2de77ff6 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_gt_types.h
@@ -16,7 +16,6 @@
 #include <linux/types.h>
 #include <linux/workqueue.h>
 
-#include "iov/intel_iov_types.h"
 #include "uc/intel_uc.h"
 #include "intel_gsc.h"
 
@@ -98,7 +97,6 @@ struct intel_gt {
 
 	struct intel_uc uc;
 	struct intel_gsc gsc;
-	struct intel_iov iov;
 
 	struct {
 		/* Serialize global tlb invalidations */
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.c b/drivers/gpu/drm/i915/gt/intel_gtt.c
index 6c3f80c44c48..13e411187fd5 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.c
@@ -402,9 +402,6 @@ void gtt_write_workarounds(struct intel_gt *gt)
 	struct drm_i915_private *i915 = gt->i915;
 	struct intel_uncore *uncore = gt->uncore;
 
-	if (IS_SRIOV_VF(i915))
-		return;
-
 	/*
 	 * This function is for gtt related workarounds. This function is
 	 * called on driver load and after a GPU reset, so you can place
@@ -579,9 +576,6 @@ void setup_private_pat(struct intel_uncore *uncore)
 
 	GEM_BUG_ON(GRAPHICS_VER(i915) < 8);
 
-	if (IS_SRIOV_VF(i915))
-		return;
-
 	if (GRAPHICS_VER(i915) >= 12)
 		tgl_setup_private_ppat(uncore);
 	else if (GRAPHICS_VER(i915) >= 11)
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.h b/drivers/gpu/drm/i915/gt/intel_gtt.h
index d5118f67eda3..2eb89c847eed 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.h
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.h
@@ -90,19 +90,7 @@ typedef u64 gen8_pte_t;
 
 #define GEN12_PPGTT_PTE_LM	BIT_ULL(11)
 
-/*
- *  DOC: GEN12 GGTT Table Entry format
- *
- * +----------+---------+---------+-----------------+--------------+---------+
- * |    63:46 |   45:12 |    11:5 |             4:2 |            1 |       0 |
- * +==========+=========+=========+=================+==============+=========+
- * |  Ignored | Address | Ignored | Function Number | Local Memory | Present |
- * +----------+---------+---------+-----------------+--------------+---------+
- */
-
-#define GEN12_GGTT_PTE_LM		BIT_ULL(1)
-#define TGL_GGTT_PTE_VFID_MASK		GENMASK_ULL(4, 2)
-#define GEN12_GGTT_PTE_ADDR_MASK	GENMASK_ULL(45, 12)
+#define GEN12_GGTT_PTE_LM	BIT_ULL(1)
 
 #define GEN12_PDE_64K BIT(6)
 #define GEN12_PTE_PS64 BIT(8)
@@ -607,20 +595,6 @@ int i915_ggtt_balloon(struct i915_ggtt *ggtt, u64 start, u64 end,
 		      struct drm_mm_node *node);
 void i915_ggtt_deballoon(struct i915_ggtt *ggtt, struct drm_mm_node *node);
 
-void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
-			       const struct drm_mm_node *node);
-
-#define I915_GGTT_SAVE_PTES_NO_VFID BIT(31)
-
-int i915_ggtt_save_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node, void *buf,
-			unsigned int size, unsigned int flags);
-
-#define I915_GGTT_RESTORE_PTES_NEW_VFID  BIT(31)
-#define I915_GGTT_RESTORE_PTES_VFID_MASK GENMASK(19, 0)
-
-int i915_ggtt_restore_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node, const void *buf,
-			   unsigned int size, unsigned int flags);
-
 int i915_ppgtt_init_hw(struct intel_gt *gt);
 
 struct i915_ppgtt *i915_ppgtt_create(struct intel_gt *gt,
@@ -689,11 +663,6 @@ release_pd_entry(struct i915_page_directory * const pd,
 		 const struct drm_i915_gem_object * const scratch);
 void gen6_ggtt_invalidate(struct i915_ggtt *ggtt);
 
-void gen8_set_pte(void __iomem *addr, gen8_pte_t pte);
-gen8_pte_t gen8_get_pte(void __iomem *addr);
-
-u64 ggtt_addr_to_pte_offset(u64 ggtt_addr);
-
 void ppgtt_bind_vma(struct i915_address_space *vm,
 		    struct i915_vm_pt_stash *stash,
 		    struct i915_vma_resource *vma_res,
diff --git a/drivers/gpu/drm/i915/gt/intel_lrc.c b/drivers/gpu/drm/i915/gt/intel_lrc.c
index 61442b9fb88c..3955292483a6 100644
--- a/drivers/gpu/drm/i915/gt/intel_lrc.c
+++ b/drivers/gpu/drm/i915/gt/intel_lrc.c
@@ -732,8 +732,7 @@ static int lrc_ring_cmd_buf_cctl(const struct intel_engine_cs *engine)
 		 * simply to match the RCS context image layout.
 		 */
 		return 0xc6;
-	else if (engine->class != RENDER_CLASS &&
-		 engine->class != COMPUTE_CLASS)
+	else if (engine->class != RENDER_CLASS)
 		return -1;
 	else if (GRAPHICS_VER(engine->i915) >= 12)
 		return 0xb6;
diff --git a/drivers/gpu/drm/i915/gt/intel_mocs.c b/drivers/gpu/drm/i915/gt/intel_mocs.c
index aa78efa2563e..152244d7f62a 100644
--- a/drivers/gpu/drm/i915/gt/intel_mocs.c
+++ b/drivers/gpu/drm/i915/gt/intel_mocs.c
@@ -475,7 +475,7 @@ static unsigned int get_mocs_settings(const struct drm_i915_private *i915,
 		table->n_entries = GEN9_NUM_MOCS_ENTRIES;
 		table->uc_index = 1;
 		table->unused_entries_index = 5;
-	} else if (IS_TIGERLAKE(i915) || IS_ALDERLAKE_S(i915) || IS_ROCKETLAKE(i915)) {
+	} else if (IS_TIGERLAKE(i915) || IS_ROCKETLAKE(i915)) {
 		/* For TGL/RKL, Can't be changed now for ABI reasons */
 		table->size  = ARRAY_SIZE(tgl_mocs_table);
 		table->table = tgl_mocs_table;
@@ -508,9 +508,6 @@ static unsigned int get_mocs_settings(const struct drm_i915_private *i915,
 	if (GEM_DEBUG_WARN_ON(table->size > table->n_entries))
 		return 0;
 
-	if (IS_SRIOV_VF(i915))
-		return 0;
-
 	/* WaDisableSkipCaching:skl,bxt,kbl,glk */
 	if (GRAPHICS_VER(i915) == 9) {
 		int i;
diff --git a/drivers/gpu/drm/i915/gt/intel_reset.c b/drivers/gpu/drm/i915/gt/intel_reset.c
index ce0754931669..b36674356986 100644
--- a/drivers/gpu/drm/i915/gt/intel_reset.c
+++ b/drivers/gpu/drm/i915/gt/intel_reset.c
@@ -278,7 +278,6 @@ static int ilk_do_reset(struct intel_gt *gt, intel_engine_mask_t engine_mask,
 static int gen6_hw_domain_reset(struct intel_gt *gt, u32 hw_domain_mask)
 {
 	struct intel_uncore *uncore = gt->uncore;
-	int loops = 2;
 	int err;
 
 	/*
@@ -286,39 +285,18 @@ static int gen6_hw_domain_reset(struct intel_gt *gt, u32 hw_domain_mask)
 	 * for fifo space for the write or forcewake the chip for
 	 * the read
 	 */
-	do {
-		intel_uncore_write_fw(uncore, GEN6_GDRST, hw_domain_mask);
+	intel_uncore_write_fw(uncore, GEN6_GDRST, hw_domain_mask);
 
-		/*
-		 * Wait for the device to ack the reset requests.
-		 *
-		 * On some platforms, e.g. Jasperlake, we see that the
-		 * engine register state is not cleared until shortly after
-		 * GDRST reports completion, causing a failure as we try
-		 * to immediately resume while the internal state is still
-		 * in flux. If we immediately repeat the reset, the second
-		 * reset appears to serialise with the first, and since
-		 * it is a no-op, the registers should retain their reset
-		 * value. However, there is still a concern that upon
-		 * leaving the second reset, the internal engine state
-		 * is still in flux and not ready for resuming.
-		 */
-		err = __intel_wait_for_register_fw(uncore, GEN6_GDRST,
-						   hw_domain_mask, 0,
-						   2000, 0,
-						   NULL);
-	} while (err == 0 && --loops);
+	/* Wait for the device to ack the reset requests */
+	err = __intel_wait_for_register_fw(uncore,
+					   GEN6_GDRST, hw_domain_mask, 0,
+					   500, 0,
+					   NULL);
 	if (err)
 		GT_TRACE(gt,
 			 "Wait for 0x%08x engines reset failed\n",
 			 hw_domain_mask);
 
-	/*
-	 * As we have observed that the engine state is still volatile
-	 * after GDRST is acked, impose a small delay to let everything settle.
-	 */
-	udelay(50);
-
 	return err;
 }
 
@@ -667,49 +645,6 @@ static int gen8_reset_engines(struct intel_gt *gt,
 	return ret;
 }
 
-static int gen12_vf_reset(struct intel_gt *gt,
-			  intel_engine_mask_t mask,
-			  unsigned int retry)
-{
-	struct intel_uncore *uncore = gt->uncore;
-	u32 request[VF2GUC_VF_RESET_REQUEST_MSG_LEN] = {
-		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
-		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
-		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_VF2GUC_VF_RESET),
-	};
-	const i915_reg_t reg = GEN11_SOFT_SCRATCH(0);
-	u32 response;
-	int err;
-
-	/* No engine reset since VFs always run with GuC submission enabled */
-	if (GEM_WARN_ON(mask != ALL_ENGINES))
-		return -ENODEV;
-
-	/*
-	 * Can't use intel_guc_send_mmio() since it uses mutex,
-	 * but we don't expect any other MMIO action in flight,
-	 * as we use them only during init and teardown.
-	 */
-	GEM_WARN_ON(mutex_is_locked(&gt->uc.guc.send_mutex));
-
-	intel_uncore_write_fw(uncore, reg, request[0]);
-	intel_uncore_write_fw(uncore, GEN11_GUC_HOST_INTERRUPT, 1);
-
-	err = __intel_wait_for_register_fw(uncore, reg,
-					   GUC_HXG_MSG_0_ORIGIN,
-					   FIELD_PREP(GUC_HXG_MSG_0_ORIGIN,
-						      GUC_HXG_ORIGIN_GUC),
-					   1000, 0, &response);
-	if (unlikely(err)) {
-		drm_dbg(&gt->i915->drm, "VF reset not completed (%pe)\n",
-			ERR_PTR(err));
-	} else if (FIELD_GET(GUC_HXG_MSG_0_TYPE, response) != GUC_HXG_TYPE_RESPONSE_SUCCESS) {
-		drm_dbg(&gt->i915->drm, "VF reset not completed (%#x)\n",
-			response);
-	}
-	return 0;
-}
-
 static int mock_reset(struct intel_gt *gt,
 		      intel_engine_mask_t mask,
 		      unsigned int retry)
@@ -727,8 +662,6 @@ static reset_func intel_get_gpu_reset(const struct intel_gt *gt)
 
 	if (is_mock_gt(gt))
 		return mock_reset;
-	else if (IS_SRIOV_VF(i915))
-		return gen12_vf_reset;
 	else if (GRAPHICS_VER(i915) >= 8)
 		return gen8_reset_engines;
 	else if (GRAPHICS_VER(i915) >= 6)
diff --git a/drivers/gpu/drm/i915/gt/intel_workarounds.c b/drivers/gpu/drm/i915/gt/intel_workarounds.c
index bfd061a90776..6d2003d598e6 100644
--- a/drivers/gpu/drm/i915/gt/intel_workarounds.c
+++ b/drivers/gpu/drm/i915/gt/intel_workarounds.c
@@ -768,9 +768,6 @@ __intel_engine_init_ctx_wa(struct intel_engine_cs *engine,
 {
 	struct drm_i915_private *i915 = engine->i915;
 
-	if (IS_SRIOV_VF(i915))
-		return;
-
 	wa_init_start(wal, name, engine->name);
 
 	/* Applies to all engines */
@@ -1575,9 +1572,6 @@ void intel_gt_init_workarounds(struct intel_gt *gt)
 {
 	struct i915_wa_list *wal = &gt->wa_list;
 
-	if (IS_SRIOV_VF(gt->i915))
-		return;
-
 	wa_init_start(wal, "GT", "global");
 	gt_init_workarounds(gt, wal);
 	wa_init_finish(wal);
@@ -1993,9 +1987,6 @@ void intel_engine_init_whitelist(struct intel_engine_cs *engine)
 	struct drm_i915_private *i915 = engine->i915;
 	struct i915_wa_list *w = &engine->whitelist;
 
-	if (IS_SRIOV_VF(engine->i915))
-		return;
-
 	wa_init_start(w, "whitelist", engine->name);
 
 	if (IS_PONTEVECCHIO(i915))
@@ -2819,9 +2810,6 @@ void intel_engine_init_workarounds(struct intel_engine_cs *engine)
 {
 	struct i915_wa_list *wal = &engine->wa_list;
 
-	if (IS_SRIOV_VF(engine->i915))
-		return;
-
 	if (GRAPHICS_VER(engine->i915) < 4)
 		return;
 
diff --git a/drivers/gpu/drm/i915/gt/selftest_execlists.c b/drivers/gpu/drm/i915/gt/selftest_execlists.c
index b370411d4362..1e08b2473b99 100644
--- a/drivers/gpu/drm/i915/gt/selftest_execlists.c
+++ b/drivers/gpu/drm/i915/gt/selftest_execlists.c
@@ -1532,8 +1532,8 @@ static int live_busywait_preempt(void *arg)
 	struct drm_i915_gem_object *obj;
 	struct i915_vma *vma;
 	enum intel_engine_id id;
+	int err = -ENOMEM;
 	u32 *map;
-	int err;
 
 	/*
 	 * Verify that even without HAS_LOGICAL_RING_PREEMPTION, we can
@@ -1541,17 +1541,13 @@ static int live_busywait_preempt(void *arg)
 	 */
 
 	ctx_hi = kernel_context(gt->i915, NULL);
-	if (IS_ERR(ctx_hi))
-		return PTR_ERR(ctx_hi);
-
+	if (!ctx_hi)
+		return -ENOMEM;
 	ctx_hi->sched.priority = I915_CONTEXT_MAX_USER_PRIORITY;
 
 	ctx_lo = kernel_context(gt->i915, NULL);
-	if (IS_ERR(ctx_lo)) {
-		err = PTR_ERR(ctx_lo);
+	if (!ctx_lo)
 		goto err_ctx_hi;
-	}
-
 	ctx_lo->sched.priority = I915_CONTEXT_MIN_USER_PRIORITY;
 
 	obj = i915_gem_object_create_internal(gt->i915, PAGE_SIZE);
@@ -3479,14 +3475,12 @@ static int random_priority(struct rnd_state *rnd)
 
 struct preempt_smoke {
 	struct intel_gt *gt;
-	struct kthread_work work;
 	struct i915_gem_context **contexts;
 	struct intel_engine_cs *engine;
 	struct drm_i915_gem_object *batch;
 	unsigned int ncontext;
 	struct rnd_state prng;
 	unsigned long count;
-	int result;
 };
 
 static struct i915_gem_context *smoke_context(struct preempt_smoke *smoke)
@@ -3546,31 +3540,34 @@ static int smoke_submit(struct preempt_smoke *smoke,
 	return err;
 }
 
-static void smoke_crescendo_work(struct kthread_work *work)
+static int smoke_crescendo_thread(void *arg)
 {
-	struct preempt_smoke *smoke = container_of(work, typeof(*smoke), work);
+	struct preempt_smoke *smoke = arg;
 	IGT_TIMEOUT(end_time);
 	unsigned long count;
 
 	count = 0;
 	do {
 		struct i915_gem_context *ctx = smoke_context(smoke);
+		int err;
 
-		smoke->result = smoke_submit(smoke, ctx,
-					     count % I915_PRIORITY_MAX,
-					     smoke->batch);
+		err = smoke_submit(smoke,
+				   ctx, count % I915_PRIORITY_MAX,
+				   smoke->batch);
+		if (err)
+			return err;
 
 		count++;
-	} while (!smoke->result && count < smoke->ncontext &&
-		 !__igt_timeout(end_time, NULL));
+	} while (count < smoke->ncontext && !__igt_timeout(end_time, NULL));
 
 	smoke->count = count;
+	return 0;
 }
 
 static int smoke_crescendo(struct preempt_smoke *smoke, unsigned int flags)
 #define BATCH BIT(0)
 {
-	struct kthread_worker *worker[I915_NUM_ENGINES] = {};
+	struct task_struct *tsk[I915_NUM_ENGINES] = {};
 	struct preempt_smoke *arg;
 	struct intel_engine_cs *engine;
 	enum intel_engine_id id;
@@ -3581,8 +3578,6 @@ static int smoke_crescendo(struct preempt_smoke *smoke, unsigned int flags)
 	if (!arg)
 		return -ENOMEM;
 
-	memset(arg, 0, I915_NUM_ENGINES * sizeof(*arg));
-
 	for_each_engine(engine, smoke->gt, id) {
 		arg[id] = *smoke;
 		arg[id].engine = engine;
@@ -3590,28 +3585,31 @@ static int smoke_crescendo(struct preempt_smoke *smoke, unsigned int flags)
 			arg[id].batch = NULL;
 		arg[id].count = 0;
 
-		worker[id] = kthread_create_worker(0, "igt/smoke:%d", id);
-		if (IS_ERR(worker[id])) {
-			err = PTR_ERR(worker[id]);
+		tsk[id] = kthread_run(smoke_crescendo_thread, arg,
+				      "igt/smoke:%d", id);
+		if (IS_ERR(tsk[id])) {
+			err = PTR_ERR(tsk[id]);
 			break;
 		}
-
-		kthread_init_work(&arg[id].work, smoke_crescendo_work);
-		kthread_queue_work(worker[id], &arg[id].work);
+		get_task_struct(tsk[id]);
 	}
 
+	yield(); /* start all threads before we kthread_stop() */
+
 	count = 0;
 	for_each_engine(engine, smoke->gt, id) {
-		if (IS_ERR_OR_NULL(worker[id]))
+		int status;
+
+		if (IS_ERR_OR_NULL(tsk[id]))
 			continue;
 
-		kthread_flush_work(&arg[id].work);
-		if (arg[id].result && !err)
-			err = arg[id].result;
+		status = kthread_stop(tsk[id]);
+		if (status && !err)
+			err = status;
 
 		count += arg[id].count;
 
-		kthread_destroy_worker(worker[id]);
+		put_task_struct(tsk[id]);
 	}
 
 	pr_info("Submitted %lu crescendo:%x requests across %d engines and %d contexts\n",
diff --git a/drivers/gpu/drm/i915/gt/selftest_hangcheck.c b/drivers/gpu/drm/i915/gt/selftest_hangcheck.c
index 71263058a7b0..7f3bb1d34dfb 100644
--- a/drivers/gpu/drm/i915/gt/selftest_hangcheck.c
+++ b/drivers/gpu/drm/i915/gt/selftest_hangcheck.c
@@ -866,13 +866,10 @@ static int igt_reset_active_engine(void *arg)
 }
 
 struct active_engine {
-	struct kthread_worker *worker;
-	struct kthread_work work;
+	struct task_struct *task;
 	struct intel_engine_cs *engine;
 	unsigned long resets;
 	unsigned int flags;
-	bool stop;
-	int result;
 };
 
 #define TEST_ACTIVE	BIT(0)
@@ -903,10 +900,10 @@ static int active_request_put(struct i915_request *rq)
 	return err;
 }
 
-static void active_engine(struct kthread_work *work)
+static int active_engine(void *data)
 {
 	I915_RND_STATE(prng);
-	struct active_engine *arg = container_of(work, typeof(*arg), work);
+	struct active_engine *arg = data;
 	struct intel_engine_cs *engine = arg->engine;
 	struct i915_request *rq[8] = {};
 	struct intel_context *ce[ARRAY_SIZE(rq)];
@@ -916,17 +913,16 @@ static void active_engine(struct kthread_work *work)
 	for (count = 0; count < ARRAY_SIZE(ce); count++) {
 		ce[count] = intel_context_create(engine);
 		if (IS_ERR(ce[count])) {
-			arg->result = PTR_ERR(ce[count]);
-			pr_err("[%s] Create context #%ld failed: %d!\n",
-			       engine->name, count, arg->result);
+			err = PTR_ERR(ce[count]);
+			pr_err("[%s] Create context #%ld failed: %d!\n", engine->name, count, err);
 			while (--count)
 				intel_context_put(ce[count]);
-			return;
+			return err;
 		}
 	}
 
 	count = 0;
-	while (!READ_ONCE(arg->stop)) {
+	while (!kthread_should_stop()) {
 		unsigned int idx = count++ & (ARRAY_SIZE(rq) - 1);
 		struct i915_request *old = rq[idx];
 		struct i915_request *new;
@@ -971,7 +967,7 @@ static void active_engine(struct kthread_work *work)
 		intel_context_put(ce[count]);
 	}
 
-	arg->result = err;
+	return err;
 }
 
 static int __igt_reset_engines(struct intel_gt *gt,
@@ -1026,7 +1022,7 @@ static int __igt_reset_engines(struct intel_gt *gt,
 
 		memset(threads, 0, sizeof(*threads) * I915_NUM_ENGINES);
 		for_each_engine(other, gt, tmp) {
-			struct kthread_worker *worker;
+			struct task_struct *tsk;
 
 			threads[tmp].resets =
 				i915_reset_engine_count(global, other);
@@ -1040,22 +1036,20 @@ static int __igt_reset_engines(struct intel_gt *gt,
 			threads[tmp].engine = other;
 			threads[tmp].flags = flags;
 
-			worker = kthread_create_worker(0, "igt/%s",
-						       other->name);
-			if (IS_ERR(worker)) {
-				err = PTR_ERR(worker);
-				pr_err("[%s] Worker create failed: %d!\n",
-				       engine->name, err);
+			tsk = kthread_run(active_engine, &threads[tmp],
+					  "igt/%s", other->name);
+			if (IS_ERR(tsk)) {
+				err = PTR_ERR(tsk);
+				pr_err("[%s] Thread spawn failed: %d!\n", engine->name, err);
 				goto unwind;
 			}
 
-			threads[tmp].worker = worker;
-
-			kthread_init_work(&threads[tmp].work, active_engine);
-			kthread_queue_work(threads[tmp].worker,
-					   &threads[tmp].work);
+			threads[tmp].task = tsk;
+			get_task_struct(tsk);
 		}
 
+		yield(); /* start all threads before we begin */
+
 		st_engine_heartbeat_disable_no_pm(engine);
 		GEM_BUG_ON(test_and_set_bit(I915_RESET_ENGINE + id,
 					    &gt->reset.flags));
@@ -1203,20 +1197,17 @@ static int __igt_reset_engines(struct intel_gt *gt,
 		for_each_engine(other, gt, tmp) {
 			int ret;
 
-			if (!threads[tmp].worker)
+			if (!threads[tmp].task)
 				continue;
 
-			WRITE_ONCE(threads[tmp].stop, true);
-			kthread_flush_work(&threads[tmp].work);
-			ret = READ_ONCE(threads[tmp].result);
+			ret = kthread_stop(threads[tmp].task);
 			if (ret) {
 				pr_err("kthread for other engine %s failed, err=%d\n",
 				       other->name, ret);
 				if (!err)
 					err = ret;
 			}
-
-			kthread_destroy_worker(threads[tmp].worker);
+			put_task_struct(threads[tmp].task);
 
 			/* GuC based resets are not logged per engine */
 			if (!using_guc) {
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_communication_ctb_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_communication_ctb_abi.h
index 16bd4ef81dee..28b8387f97b7 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_communication_ctb_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_communication_ctb_abi.h
@@ -38,7 +38,6 @@
  *  |   |       |   - _`GUC_CTB_STATUS_UNDERFLOW` = 2 (truncated message)      |
  *  |   |       |   - _`GUC_CTB_STATUS_MISMATCH` = 4 (head/tail modified)      |
  *  |   |       |   - _`GUC_CTB_STATUS_UNUSED` = 8 (CTB is not in use)         |
- *  |   |       |   - _`GUC_CTB_STATUS_MIGRATED` = 16 (VF was migrated)        |
  *  +---+-------+--------------------------------------------------------------+
  *  |...|       | RESERVED = MBZ                                               |
  *  +---+-------+--------------------------------------------------------------+
@@ -55,7 +54,6 @@ struct guc_ct_buffer_desc {
 #define GUC_CTB_STATUS_UNDERFLOW			BIT(1)
 #define GUC_CTB_STATUS_MISMATCH				BIT(2)
 #define GUC_CTB_STATUS_UNUSED				BIT(3)
-#define GUC_CTB_STATUS_MIGRATED				BIT(4)
 	u32 reserved[13];
 } __packed;
 static_assert(sizeof(struct guc_ct_buffer_desc) == 64);
@@ -169,4 +167,25 @@ static_assert(sizeof(struct guc_ct_buffer_desc) == 64);
  * - **flags**, holds various bits to control message handling
  */
 
+/*
+ * Definition of the command transport message header (DW0)
+ *
+ * bit[4..0]	message len (in dwords)
+ * bit[7..5]	reserved
+ * bit[8]	response (G2H only)
+ * bit[8]	write fence to desc (H2G only)
+ * bit[9]	write status to H2G buff (H2G only)
+ * bit[10]	send status back via G2H (H2G only)
+ * bit[15..11]	reserved
+ * bit[31..16]	action code
+ */
+#define GUC_CT_MSG_LEN_SHIFT			0
+#define GUC_CT_MSG_LEN_MASK			0x1F
+#define GUC_CT_MSG_IS_RESPONSE			(1 << 8)
+#define GUC_CT_MSG_WRITE_FENCE_TO_DESC		(1 << 8)
+#define GUC_CT_MSG_WRITE_STATUS_TO_BUFF		(1 << 9)
+#define GUC_CT_MSG_SEND_STATUS			(1 << 10)
+#define GUC_CT_MSG_ACTION_SHIFT			16
+#define GUC_CT_MSG_ACTION_MASK			0xFFFF
+
 #endif /* _ABI_GUC_COMMUNICATION_CTB_ABI_H */
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_errors_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_errors_abi.h
index 721384a7c39e..8085fb181274 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_errors_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_errors_abi.h
@@ -9,7 +9,6 @@
 enum intel_guc_response_status {
 	INTEL_GUC_RESPONSE_STATUS_SUCCESS = 0x0,
 	INTEL_GUC_RESPONSE_NOT_SUPPORTED = 0x20,
-	INTEL_GUC_RESPONSE_VF_MIGRATED = 0x107,
 	INTEL_GUC_RESPONSE_NO_ATTRIBUTE_TABLE = 0x201,
 	INTEL_GUC_RESPONSE_NO_DECRYPTION_KEY = 0x202,
 	INTEL_GUC_RESPONSE_DECRYPTION_FAILED = 0x204,
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
index 641a9fed0779..4a59478c3b5c 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
@@ -16,8 +16,6 @@
  *  +===+=======+==============================================================+
  *  | 0 | 31:16 | **KEY** - KLV key identifier                                 |
  *  |   |       |   - `GuC Self Config KLVs`_                                  |
- *  |   |       |   - `GuC VGT Policy KLVs`_                                   |
- *  |   |       |   - `GuC VF Configuration KLVs`_                             |
  *  |   |       |                                                              |
  *  |   +-------+--------------------------------------------------------------+
  *  |   |  15:0 | **LEN** - length of VALUE (in 32bit dwords)                  |
@@ -96,195 +94,4 @@ enum  {
 	GUC_CONTEXT_POLICIES_KLV_NUM_IDS = 5,
 };
 
-/**
- * DOC: GuC VGT Policy KLVs
- *
- * `GuC KLV`_ keys available for use with PF2GUC_UPDATE_VGT_POLICY.
- *
- * _`GUC_KLV_VGT_POLICY_SCHED_IF_IDLE` : 0x8001
- *      This config sets whether strict scheduling is enabled whereby any VF
- *      that doesnt have work to submit is still allocated a fixed execution
- *      time-slice to ensure active VFs execution is always consitent even
- *      during other VF reprovisiong / rebooting events. Changing this KLV
- *      impacts all VFs and takes effect on the next VF-Switch event.
- *
- *      :0: don't schedule idle (default)
- *      :1: schedule if idle
- *
- * _`GUC_KLV_VGT_POLICY_ADVERSE_SAMPLE_PERIOD` : 0x8002
- *      This config sets the sample period for tracking adverse event counters.
- *       A sample period is the period in millisecs during which events are counted.
- *       This is applicable for all the VFs.
- *
- *      :0: adverse events are not counted (default)
- *      :n: sample period in milliseconds
- *
- * _`GUC_KLV_VGT_POLICY_RESET_AFTER_VF_SWITCH` : 0x8D00
- *      This enum is to reset utilized HW engine after VF Switch (i.e to clean
- *      up Stale HW register left behind by previous VF)
- *
- *      :0: don't reset (default)
- *      :1: reset
- */
-
-#define GUC_KLV_VGT_POLICY_SCHED_IF_IDLE_KEY		0x8001
-#define GUC_KLV_VGT_POLICY_SCHED_IF_IDLE_LEN		1u
-
-#define GUC_KLV_VGT_POLICY_ADVERSE_SAMPLE_PERIOD_KEY	0x8002
-#define GUC_KLV_VGT_POLICY_ADVERSE_SAMPLE_PERIOD_LEN	1u
-
-#define GUC_KLV_VGT_POLICY_RESET_AFTER_VF_SWITCH_KEY	0x8D00
-#define GUC_KLV_VGT_POLICY_RESET_AFTER_VF_SWITCH_LEN	1u
-
-/**
- * DOC: GuC VF Configuration KLVs
- *
- * `GuC KLV`_ keys available for use with PF2GUC_UPDATE_VF_CFG.
- *
- * _`GUC_KLV_VF_CFG_GGTT_START` : 0x0001
- *      A 4K aligned start GTT address/offset assigned to VF.
- *      Value is 64 bits.
- *
- * _`GUC_KLV_VF_CFG_GGTT_SIZE` : 0x0002
- *      A 4K aligned size of GGTT assigned to VF.
- *      Value is 64 bits.
- *
- * _`GUC_KLV_VF_CFG_NUM_CONTEXTS` : 0x0004
- *      Refers to the number of contexts allocated to this VF.
- *
- *      :0: no contexts (default)
- *      :1-65535: number of contexts (Gen12)
- *
- * _`GUC_KLV_VF_CFG_NUM_DOORBELLS` : 0x0006
- *      Refers to the number of doorbells allocated to this VF.
- *
- *      :0: no doorbells (default)
- *      :1-255: number of doorbells (Gen12)
- *
- * _`GUC_KLV_VF_CFG_EXEC_QUANTUM` : 0x8A01
- *      This config sets the VFs-execution-quantum in milliseconds.
- *      GUC will attempt to obey the maximum values as much as HW is capable
- *      of and this will never be perfectly-exact (accumulated nano-second
- *      granularity) since the GPUs clock time runs off a different crystal
- *      from the CPUs clock. Changing this KLV on a VF that is currently
- *      running a context wont take effect until a new context is scheduled in.
- *      That said, when the PF is changing this value from 0xFFFFFFFF to
- *      something else, it might never take effect if the VF is running an
- *      inifinitely long compute or shader kernel. In such a scenario, the
- *      PF would need to trigger a VM PAUSE and then change the KLV to force
- *      it to take effect. Such cases might typically happen on a 1PF+1VF
- *      Virtualization config enabled for heavier workloads like AI/ML.
- *
- *      :0: infinite exec quantum (default)
- *
- * _`GUC_KLV_VF_CFG_PREEMPT_TIMEOUT` : 0x8A02
- *      This config sets the VF-preemption-timeout in microseconds.
- *      GUC will attempt to obey the minimum and maximum values as much as
- *      HW is capable and this will never be perfectly-exact (accumulated
- *      nano-second granularity) since the GPUs clock time runs off a
- *      different crystal from the CPUs clock. Changing this KLV on a VF
- *      that is currently running a context wont take effect until a new
- *      context is scheduled in.
- *      That said, when the PF is changing this value from 0xFFFFFFFF to
- *      something else, it might never take effect if the VF is running an
- *      inifinitely long compute or shader kernel.
- *      In this case, the PF would need to trigger a VM PAUSE and then change
- *      the KLV to force it to take effect. Such cases might typically happen
- *      on a 1PF+1VF Virtualization config enabled for heavier workloads like
- *      AI/ML.
- *
- *      :0: no preemption timeout (default)
- *
- * _`GUC_KLV_VF_CFG_THRESHOLD_CAT_ERR` : 0x8A03
- *      This config sets threshold for CAT errors caused by the VF.
- *
- *      :0: adverse events or error will not be reported (default)
- *      :n: event occurrence count per sampling interval
- *
- * _`GUC_KLV_VF_CFG_THRESHOLD_ENGINE_RESET` : 0x8A04
- *      This config sets threshold for engine reset caused by the VF.
- *
- *      :0: adverse events or error will not be reported (default)
- *      :n: event occurrence count per sampling interval
- *
- * _`GUC_KLV_VF_CFG_THRESHOLD_PAGE_FAULT` : 0x8A05
- *      This config sets threshold for page fault errors caused by the VF.
- *
- *      :0: adverse events or error will not be reported (default)
- *      :n: event occurrence count per sampling interval
- *
- * _`GUC_KLV_VF_CFG_THRESHOLD_H2G_STORM` : 0x8A06
- *      This config sets threshold for H2G interrupts triggered by the VF.
- *
- *      :0: adverse events or error will not be reported (default)
- *      :n: time (us) per sampling interval
- *
- * _`GUC_KLV_VF_CFG_THRESHOLD_IRQ_STORM` : 0x8A07
- *      This config sets threshold for GT interrupts triggered by the VF's
- *      workloads.
- *
- *      :0: adverse events or error will not be reported (default)
- *      :n: time (us) per sampling interval
- *
- * _`GUC_KLV_VF_CFG_THRESHOLD_DOORBELL_STORM` : 0x8A08
- *      This config sets threshold for doorbell's ring triggered by the VF.
- *
- *      :0: adverse events or error will not be reported (default)
- *      :n: time (us) per sampling interval
- *
- * _`GUC_KLV_VF_CFG_BEGIN_DOORBELL_ID` : 0x8A0A
- *      Refers to the start index of doorbell assigned to this VF.
- *
- *      :0: (default)
- *      :1-255: number of doorbells (Gen12)
- *
- * _`GUC_KLV_VF_CFG_BEGIN_CONTEXT_ID` : 0x8A0B
- *      Refers to the start index in context array allocated to this VFs use.
- *
- *      :0: (default)
- *      :1-65535: number of contexts (Gen12)
- */
-
-#define GUC_KLV_VF_CFG_GGTT_START_KEY		0x0001
-#define GUC_KLV_VF_CFG_GGTT_START_LEN		2u
-
-#define GUC_KLV_VF_CFG_GGTT_SIZE_KEY		0x0002
-#define GUC_KLV_VF_CFG_GGTT_SIZE_LEN		2u
-
-#define GUC_KLV_VF_CFG_NUM_CONTEXTS_KEY		0x0004
-#define GUC_KLV_VF_CFG_NUM_CONTEXTS_LEN		1u
-
-#define GUC_KLV_VF_CFG_NUM_DOORBELLS_KEY	0x0006
-#define GUC_KLV_VF_CFG_NUM_DOORBELLS_LEN	1u
-
-#define GUC_KLV_VF_CFG_EXEC_QUANTUM_KEY		0x8a01
-#define GUC_KLV_VF_CFG_EXEC_QUANTUM_LEN		1u
-
-#define GUC_KLV_VF_CFG_PREEMPT_TIMEOUT_KEY	0x8a02
-#define GUC_KLV_VF_CFG_PREEMPT_TIMEOUT_LEN	1u
-
-#define GUC_KLV_VF_CFG_THRESHOLD_CAT_ERR_KEY		0x8a03
-#define GUC_KLV_VF_CFG_THRESHOLD_CAT_ERR_LEN		1u
-
-#define GUC_KLV_VF_CFG_THRESHOLD_ENGINE_RESET_KEY	0x8a04
-#define GUC_KLV_VF_CFG_THRESHOLD_ENGINE_RESET_LEN	1u
-
-#define GUC_KLV_VF_CFG_THRESHOLD_PAGE_FAULT_KEY		0x8a05
-#define GUC_KLV_VF_CFG_THRESHOLD_PAGE_FAULT_LEN		1u
-
-#define GUC_KLV_VF_CFG_THRESHOLD_H2G_STORM_KEY		0x8a06
-#define GUC_KLV_VF_CFG_THRESHOLD_H2G_STORM_LEN		1u
-
-#define GUC_KLV_VF_CFG_THRESHOLD_IRQ_STORM_KEY		0x8a07
-#define GUC_KLV_VF_CFG_THRESHOLD_IRQ_STORM_LEN		1u
-
-#define GUC_KLV_VF_CFG_THRESHOLD_DOORBELL_STORM_KEY	0x8a08
-#define GUC_KLV_VF_CFG_THRESHOLD_DOORBELL_STORM_LEN	1u
-
-#define GUC_KLV_VF_CFG_BEGIN_DOORBELL_ID_KEY	0x8a0a
-#define GUC_KLV_VF_CFG_BEGIN_DOORBELL_ID_LEN	1u
-
-#define GUC_KLV_VF_CFG_BEGIN_CONTEXT_ID_KEY	0x8a0b
-#define GUC_KLV_VF_CFG_BEGIN_CONTEXT_ID_LEN	1u
-
 #endif /* _ABI_GUC_KLVS_ABI_H */
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_messages_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_messages_abi.h
index 98eb4f46572b..7d5ba4d97d70 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_messages_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_messages_abi.h
@@ -24,7 +24,6 @@
  *  |   | 30:28 | **TYPE** - message type                                      |
  *  |   |       |   - _`GUC_HXG_TYPE_REQUEST` = 0                              |
  *  |   |       |   - _`GUC_HXG_TYPE_EVENT` = 1                                |
- *  |   |       |   - _`GUC_HXG_TYPE_FAST_REQUEST` = 2                         |
  *  |   |       |   - _`GUC_HXG_TYPE_NO_RESPONSE_BUSY` = 3                     |
  *  |   |       |   - _`GUC_HXG_TYPE_NO_RESPONSE_RETRY` = 5                    |
  *  |   |       |   - _`GUC_HXG_TYPE_RESPONSE_FAILURE` = 6                     |
@@ -47,7 +46,6 @@
 #define GUC_HXG_MSG_0_TYPE			(0x7 << 28)
 #define   GUC_HXG_TYPE_REQUEST			0u
 #define   GUC_HXG_TYPE_EVENT			1u
-#define   GUC_HXG_TYPE_FAST_REQUEST		2u
 #define   GUC_HXG_TYPE_NO_RESPONSE_BUSY		3u
 #define   GUC_HXG_TYPE_NO_RESPONSE_RETRY	5u
 #define   GUC_HXG_TYPE_RESPONSE_FAILURE		6u
@@ -91,34 +89,6 @@
 #define GUC_HXG_REQUEST_MSG_0_ACTION		(0xffff << 0)
 #define GUC_HXG_REQUEST_MSG_n_DATAn		GUC_HXG_MSG_n_PAYLOAD
 
-/**
- * DOC: HXG Fast Request
- *
- * The `HXG Request`_ message should be used to initiate asynchronous activity
- * for which confirmation or return data is not expected.
- *
- * If confirmation is required then `HXG Request`_ shall be used instead.
- *
- * The recipient of this message may only use `HXG Failure`_ message if it was
- * unable to accept this request (like invalid data).
- *
- * Format of `HXG Fast Request`_ message is same as `HXG Request`_ except @TYPE.
- *
- *  +---+-------+--------------------------------------------------------------+
- *  |   | Bits  | Description                                                  |
- *  +===+=======+==============================================================+
- *  | 0 |    31 | ORIGIN - see `HXG Message`_                                  |
- *  |   +-------+--------------------------------------------------------------+
- *  |   | 30:28 | TYPE = `GUC_HXG_TYPE_FAST_REQUEST`_                          |
- *  |   +-------+--------------------------------------------------------------+
- *  |   | 27:16 | DATA0 - see `HXG Request`_                                   |
- *  |   +-------+--------------------------------------------------------------+
- *  |   |  15:0 | ACTION - see `HXG Request`_                                  |
- *  +---+-------+--------------------------------------------------------------+
- *  |...|       | DATAn - see `HXG Request`_                                   |
- *  +---+-------+--------------------------------------------------------------+
- */
-
 /**
  * DOC: HXG Event
  *
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.c b/drivers/gpu/drm/i915/gt/uc/intel_guc.c
index 6df1ee351a42..43f6e48869a5 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.c
@@ -16,16 +16,6 @@
 #include "i915_drv.h"
 #include "i915_irq.h"
 
-#ifdef CONFIG_DRM_I915_DEBUG_GUC
-#define GUC_DEBUG(_guc, _fmt, ...) \
-	drm_dbg(&guc_to_gt(_guc)->i915->drm, "GUC: " _fmt, ##__VA_ARGS__)
-#else
-#define GUC_DEBUG(_guc, _fmt, ...) typecheck(struct intel_guc *, _guc)
-#endif
-
-static const struct intel_guc_ops guc_ops_default;
-static const struct intel_guc_ops guc_ops_vf;
-
 /**
  * DOC: GuC
  *
@@ -84,10 +74,6 @@ void intel_guc_init_send_regs(struct intel_guc *guc)
 					FW_REG_READ | FW_REG_WRITE);
 	}
 	guc->send_regs.fw_domains = fw_domains;
-
-	/* XXX: move to init_early when safe to call IS_SRIOV_VF */
-	if (IS_SRIOV_VF(guc_to_gt(guc)->i915))
-		guc->ops = &guc_ops_vf;
 }
 
 static void gen9_reset_guc_interrupts(struct intel_guc *guc)
@@ -202,8 +188,6 @@ void intel_guc_init_early(struct intel_guc *guc)
 
 	intel_guc_enable_msg(guc, INTEL_GUC_RECV_MSG_EXCEPTION |
 				  INTEL_GUC_RECV_MSG_CRASH_DUMP_POSTED);
-
-	guc->ops = &guc_ops_default;
 }
 
 void intel_guc_init_late(struct intel_guc *guc)
@@ -235,8 +219,6 @@ static u32 guc_ctl_feature_flags(struct intel_guc *guc)
 	if (intel_guc_slpc_is_used(guc))
 		flags |= GUC_CTL_ENABLE_SLPC;
 
-	flags |= i915_modparams.guc_feature_flags;
-
 	return flags;
 }
 
@@ -393,7 +375,7 @@ void intel_guc_dump_time_info(struct intel_guc *guc, struct drm_printer *p)
 		   gt->clock_frequency, gt->clock_period_ns);
 }
 
-static int __guc_init(struct intel_guc *guc)
+int intel_guc_init(struct intel_guc *guc)
 {
 	struct intel_gt *gt = guc_to_gt(guc);
 	int ret;
@@ -464,7 +446,7 @@ static int __guc_init(struct intel_guc *guc)
 	return ret;
 }
 
-static void __guc_fini(struct intel_guc *guc)
+void intel_guc_fini(struct intel_guc *guc)
 {
 	struct intel_gt *gt = guc_to_gt(guc);
 
@@ -487,50 +469,6 @@ static void __guc_fini(struct intel_guc *guc)
 	intel_uc_fw_fini(&guc->fw);
 }
 
-static int __vf_guc_init(struct intel_guc *guc)
-{
-	struct intel_gt *gt = guc_to_gt(guc);
-	int err;
-
-	GEM_BUG_ON(!IS_SRIOV_VF(gt->i915));
-
-	err = intel_guc_ct_init(&guc->ct);
-	if (err)
-		return err;
-
-	/* GuC submission is mandatory for VFs */
-	err = intel_guc_submission_init(guc);
-	if (err)
-		goto err_ct;
-
-	/*
-	 * Disable slpc controls for VF. This cannot be done in
-	 * __guc_slpc_selected since the VF probe is not complete
-	 * at that point.
-	 */
-	guc->slpc.supported = false;
-	guc->slpc.selected = false;
-
-	/* Disable GUCRC for VF */
-	guc->rc_supported = false;
-
-	return 0;
-
-err_ct:
-	intel_guc_ct_fini(&guc->ct);
-	return err;
-}
-
-static void __vf_guc_fini(struct intel_guc *guc)
-{
-	struct intel_gt *gt = guc_to_gt(guc);
-
-	GEM_BUG_ON(!IS_SRIOV_VF(gt->i915));
-
-	intel_guc_submission_fini(guc);
-	intel_guc_ct_fini(&guc->ct);
-}
-
 /*
  * This function implements the MMIO based host to GuC interface.
  */
@@ -552,8 +490,6 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 	mutex_lock(&guc->send_mutex);
 	intel_uncore_forcewake_get(uncore, guc->send_regs.fw_domains);
 
-	GUC_DEBUG(guc, "mmio sending %*ph\n", len * 4, request);
-
 retry:
 	for (i = 0; i < len; i++)
 		intel_uncore_write(uncore, guc_send_reg(guc, i), request[i]);
@@ -580,19 +516,11 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 	}
 
 	if (FIELD_GET(GUC_HXG_MSG_0_TYPE, header) == GUC_HXG_TYPE_NO_RESPONSE_BUSY) {
-		int loop = IS_SRIOV_VF(i915) ? 20 : 1;
-
 #define done ({ header = intel_uncore_read(uncore, guc_send_reg(guc, 0)); \
 		FIELD_GET(GUC_HXG_MSG_0_ORIGIN, header) != GUC_HXG_ORIGIN_GUC || \
 		FIELD_GET(GUC_HXG_MSG_0_TYPE, header) != GUC_HXG_TYPE_NO_RESPONSE_BUSY; })
 
-busy_loop:
 		ret = wait_for(done, 1000);
-		if (unlikely(ret && --loop)) {
-			drm_dbg(&i915->drm, "mmio request %#x: still busy, countdown %u\n",
-				request[0], loop);
-			goto busy_loop;
-		}
 		if (unlikely(ret))
 			goto timeout;
 		if (unlikely(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, header) !=
@@ -613,13 +541,6 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 		u32 hint = FIELD_GET(GUC_HXG_FAILURE_MSG_0_HINT, header);
 		u32 error = FIELD_GET(GUC_HXG_FAILURE_MSG_0_ERROR, header);
 
-		if (error == INTEL_GUC_RESPONSE_VF_MIGRATED) {
-			drm_dbg(&i915->drm, "mmio request %#x: migrated!\n", request[0]);
-			i915_sriov_vf_start_migration_recovery(i915);
-			ret = -EREMOTEIO;
-			goto out;
-		}
-
 		drm_err(&i915->drm, "mmio request %#x: failure %x/%u\n",
 			request[0], error, hint);
 		ret = -ENXIO;
@@ -644,13 +565,10 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 		for (i = 1; i < count; i++)
 			response_buf[i] = intel_uncore_read(uncore,
 							    guc_send_reg(guc, i));
-		GUC_DEBUG(guc, "mmio received %*ph\n", count * 4, response_buf);
 
 		/* Use number of copied dwords as our return value */
 		ret = count;
 	} else {
-		GUC_DEBUG(guc, "mmio received %*ph\n", 4, &header);
-
 		/* Use data from the GuC response as our return value */
 		ret = FIELD_GET(GUC_HXG_RESPONSE_MSG_0_DATA0, header);
 	}
@@ -661,7 +579,6 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 
 	return ret;
 }
-ALLOW_ERROR_INJECTION(intel_guc_send_mmio, ERRNO);
 
 int intel_guc_to_host_process_recv_msg(struct intel_guc *guc,
 				       const u32 *payload, u32 len)
@@ -925,35 +842,6 @@ int intel_guc_self_cfg64(struct intel_guc *guc, u16 key, u64 value)
 	return __guc_self_cfg(guc, key, 2, value);
 }
 
-static long must_wait_woken(struct wait_queue_entry *wq_entry, long timeout)
-{
-	/*
-	 * This is equivalent to wait_woken() with the exception that
-	 * we do not wake up early if the kthread task has been completed.
-	 * As we are called from page reclaim in any task context,
-	 * we may be invoked from stopped kthreads, but we *must*
-	 * complete the wait from the HW .
-	 *
-	 * A second problem is that since we are called under reclaim
-	 * and wait_woken() inspected the thread state, it makes an invalid
-	 * assumption that all PF_KTHREAD tasks have set_kthread_struct()
-	 * called upon them, and will trigger a GPF in is_kthread_should_stop().
-	 */
-	do {
-		set_current_state(TASK_UNINTERRUPTIBLE);
-		if (wq_entry->flags & WQ_FLAG_WOKEN)
-			break;
-
-		timeout = schedule_timeout(timeout);
-	} while (timeout);
-	__set_current_state(TASK_RUNNING);
-
-	/* See wait_woken() and woken_wake_function() */
-	smp_store_mb(wq_entry->flags, wq_entry->flags & ~WQ_FLAG_WOKEN);
-
-	return timeout;
-}
-
 static int guc_send_invalidate_tlb(struct intel_guc *guc, u32 *action, u32 size)
 {
 	struct intel_guc_tlb_wait _wq, *wq = &_wq;
@@ -1004,7 +892,8 @@ static int guc_send_invalidate_tlb(struct intel_guc *guc, u32 *action, u32 size)
  * queued in CT buffer.
  */
 #define OUTSTANDING_GUC_TIMEOUT_PERIOD  (HZ)
-	if (!must_wait_woken(&wait, OUTSTANDING_GUC_TIMEOUT_PERIOD)) {
+	if (!wait_woken(&wait, TASK_UNINTERRUPTIBLE,
+			OUTSTANDING_GUC_TIMEOUT_PERIOD)) {
 		/*
 		 * XXX: Failure of tlb invalidation is critical and would
 		 * warrant a gt reset.
@@ -1115,13 +1004,3 @@ void intel_guc_write_barrier(struct intel_guc *guc)
 		wmb();
 	}
 }
-
-static const struct intel_guc_ops guc_ops_default = {
-	.init = __guc_init,
-	.fini = __guc_fini,
-};
-
-static const struct intel_guc_ops guc_ops_vf = {
-	.init = __vf_guc_init,
-	.fini = __vf_guc_fini,
-};
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.h b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
index 87417c455951..ed802ae24368 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
@@ -22,14 +22,8 @@
 #include "i915_vma.h"
 
 struct __guc_ads_blob;
-struct intel_guc;
 struct intel_guc_state_capture;
 
-struct intel_guc_ops {
-	int (*init)(struct intel_guc *guc);
-	void (*fini)(struct intel_guc *guc);
-};
-
 /**
  * struct intel_guc - Top level structure of GuC.
  *
@@ -37,8 +31,6 @@ struct intel_guc_ops {
  * i915_sched_engine for submission.
  */
 struct intel_guc {
-	/** @ops: Operations to init / fini the GuC */
-	struct intel_guc_ops const *ops;
 	/** @fw: the GuC firmware */
 	struct intel_uc_fw fw;
 	/** @log: sub-structure containing GuC log related data and objects */
@@ -112,8 +104,7 @@ struct intel_guc {
 		struct ida guc_ids;
 		/**
 		 * @num_guc_ids: Number of guc_ids, selftest feature to be able
-		 * to reduce this number while testing. Also used on VFs to
-		 * reduce the pool of guc_ids.
+		 * to reduce this number while testing.
 		 */
 		int num_guc_ids;
 		/**
@@ -366,19 +357,6 @@ static inline u32 intel_guc_ggtt_offset(struct intel_guc *guc,
 	return offset;
 }
 
-static inline int intel_guc_init(struct intel_guc *guc)
-{
-	if (guc->ops->init)
-		return guc->ops->init(guc);
-	return 0;
-}
-
-static inline void intel_guc_fini(struct intel_guc *guc)
-{
-	if (guc->ops->fini)
-		guc->ops->fini(guc);
-}
-
 void intel_guc_init_early(struct intel_guc *guc);
 void intel_guc_init_late(struct intel_guc *guc);
 void intel_guc_init_send_regs(struct intel_guc *guc);
@@ -415,8 +393,7 @@ static inline bool intel_guc_is_wanted(struct intel_guc *guc)
 static inline bool intel_guc_is_used(struct intel_guc *guc)
 {
 	GEM_BUG_ON(__intel_uc_fw_status(&guc->fw) == INTEL_UC_FIRMWARE_SELECTED);
-	return intel_uc_fw_is_available(&guc->fw) ||
-	       intel_uc_fw_is_preloaded(&guc->fw);
+	return intel_uc_fw_is_available(&guc->fw);
 }
 
 static inline bool intel_guc_is_fw_running(struct intel_guc *guc)
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c
index 45b74dc21299..1345358a0011 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ads.c
@@ -572,9 +572,6 @@ static void guc_init_golden_context(struct intel_guc *guc)
 	if (!intel_uc_uses_guc_submission(&gt->uc))
 		return;
 
-	if (IS_SRIOV_VF(gt->i915))
-		return;
-
 	GEM_BUG_ON(iosys_map_is_null(&guc->ads_map));
 
 	/*
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
index 7ae0ec2ae06b..3b7ef83751c2 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
@@ -13,10 +13,6 @@
 #include "intel_guc_ct.h"
 #include "intel_pagefault.h"
 #include "gt/intel_gt.h"
-#include "gt/iov/intel_iov_event.h"
-#include "gt/iov/intel_iov_relay.h"
-#include "gt/iov/intel_iov_service.h"
-#include "gt/iov/intel_iov_state.h"
 
 static inline struct intel_guc *ct_to_guc(struct intel_guc_ct *ct)
 {
@@ -385,19 +381,8 @@ static int ct_write(struct intel_guc_ct *ct,
 	u32 *cmds = ctb->cmds;
 	unsigned int i;
 
-	if (unlikely(desc->status)) {
-		/*
-		 * after VF migration H2G is not usable any more
-		 * start recovery procedure and let caller retry
-		 * any other non-migration status is still fatal
-		 */
-		if (desc->status & ~GUC_CTB_STATUS_MIGRATED)
-			goto corrupted;
-		if (!IS_SRIOV_VF(ct_to_i915(ct)))
-			goto corrupted;
-		i915_sriov_vf_start_migration_recovery(ct_to_i915(ct));
-		return -EBUSY;
-	}
+	if (unlikely(desc->status))
+		goto corrupted;
 
 	GEM_BUG_ON(tail > size);
 
@@ -671,7 +656,7 @@ static int ct_send(struct intel_guc_ct *ct,
 
 	GEM_BUG_ON(!ct->enabled);
 	GEM_BUG_ON(!len);
-	GEM_BUG_ON(len > GUC_CTB_HXG_MSG_MAX_LEN - GUC_CTB_HDR_LEN);
+	GEM_BUG_ON(len & ~GUC_CT_MSG_LEN_MASK);
 	GEM_BUG_ON(!response_buf && response_buf_size);
 	might_sleep();
 
@@ -798,9 +783,6 @@ int intel_guc_ct_send(struct intel_guc_ct *ct, const u32 *action, u32 len,
 		return ct_send_nb(ct, action, len, flags);
 
 	ret = ct_send(ct, action, len, response_buf, response_buf_size, &status);
-	if (I915_SELFTEST_ONLY(flags & INTEL_GUC_CT_SEND_SELFTEST))
-		return ret;
-
 	if (unlikely(ret < 0)) {
 		if (ret != -ENODEV)
 			CT_ERROR(ct, "Sending action %#x failed (%pe) status=%#X\n",
@@ -812,7 +794,6 @@ int intel_guc_ct_send(struct intel_guc_ct *ct, const u32 *action, u32 len,
 
 	return ret;
 }
-ALLOW_ERROR_INJECTION(intel_guc_ct_send, ERRNO);
 
 static struct ct_incoming_msg *ct_alloc_msg(u32 num_dwords)
 {
@@ -862,18 +843,8 @@ static int ct_read(struct intel_guc_ct *ct, struct ct_incoming_msg **msg)
 			status &= ~GUC_CTB_STATUS_UNUSED;
 		}
 
-		if (status) {
-			/*
-			 * after VF migration G2H shall be still usable
-			 * only any other non-migration status is fatal
-			 */
-			if (status & ~GUC_CTB_STATUS_MIGRATED)
-				goto corrupted;
-			if (!IS_SRIOV_VF(ct_to_i915(ct)))
-				goto corrupted;
-			desc->status &= ~GUC_CTB_STATUS_MIGRATED;
-			i915_sriov_vf_start_migration_recovery(ct_to_i915(ct));
-		}
+		if (status)
+			goto corrupted;
 	}
 
 	GEM_BUG_ON(head > size);
@@ -997,9 +968,9 @@ static int ct_handle_response(struct intel_guc_ct *ct, struct ct_incoming_msg *r
 		break;
 	}
 	if (!found) {
-		CT_ERROR(ct, "Unsolicited response message %#x (fence %u len %u)\n",
-			 hxg[0], fence, len);
-		CT_ERROR(ct, "Last used fence was %u\n", ct->requests.last_fence);
+		CT_ERROR(ct, "Unsolicited response (fence %u)\n", fence);
+		CT_ERROR(ct, "Could not find fence=%u, last_fence=%u\n", fence,
+			 ct->requests.last_fence);
 		list_for_each_entry(req, &ct->requests.pending, link)
 			CT_ERROR(ct, "request %u awaits response\n",
 				 req->fence);
@@ -1017,8 +988,6 @@ static int ct_handle_response(struct intel_guc_ct *ct, struct ct_incoming_msg *r
 static int ct_process_request(struct intel_guc_ct *ct, struct ct_incoming_msg *request)
 {
 	struct intel_guc *guc = ct_to_guc(ct);
-	struct intel_gt *gt = guc_to_gt(guc);
-	struct intel_iov *iov = &gt->iov;
 	const u32 *hxg;
 	const u32 *payload;
 	u32 hxg_len, action, len;
@@ -1058,21 +1027,6 @@ static int ct_process_request(struct intel_guc_ct *ct, struct ct_incoming_msg *r
 	case INTEL_GUC_ACTION_ENGINE_FAILURE_NOTIFICATION:
 		ret = intel_guc_engine_failure_process_msg(guc, payload, len);
 		break;
-	case GUC_ACTION_GUC2PF_VF_STATE_NOTIFY:
-		ret = intel_iov_state_process_guc2pf(iov, hxg, hxg_len);
-		break;
-	case GUC_ACTION_GUC2PF_ADVERSE_EVENT:
-		ret = intel_iov_event_process_guc2pf(iov, hxg, hxg_len);
-		break;
-	case GUC_ACTION_GUC2PF_RELAY_FROM_VF:
-		ret = intel_iov_relay_process_guc2pf(&iov->relay, hxg, hxg_len);
-		break;
-	case GUC_ACTION_GUC2VF_RELAY_FROM_PF:
-		ret = intel_iov_relay_process_guc2vf(&iov->relay, hxg, hxg_len);
-		break;
-	case GUC_ACTION_GUC2PF_MMIO_RELAY_SERVICE:
-		ret = intel_iov_service_process_mmio_relay(iov, hxg, hxg_len);
-		break;
 	case INTEL_GUC_ACTION_NOTIFY_FLUSH_LOG_BUFFER_TO_FILE:
 		intel_guc_log_handle_flush_event(&guc->log);
 		ret = 0;
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
index b80b66dbc7c4..f709a19c7e21 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
@@ -104,7 +104,6 @@ static inline bool intel_guc_ct_enabled(struct intel_guc_ct *ct)
 }
 
 #define INTEL_GUC_CT_SEND_NB		BIT(31)
-#define INTEL_GUC_CT_SEND_SELFTEST	BIT(30)
 #define INTEL_GUC_CT_SEND_G2H_DW_SHIFT	0
 #define INTEL_GUC_CT_SEND_G2H_DW_MASK	(0xff << INTEL_GUC_CT_SEND_G2H_DW_SHIFT)
 #define MAKE_SEND_FLAGS(len) ({ \
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.c
index 2fec85e53e87..25f09a420561 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_debugfs.c
@@ -5,7 +5,6 @@
 
 #include <drm/drm_print.h>
 
-#include "gt/intel_gt.h"
 #include "gt/intel_gt_debugfs.h"
 #include "gt/uc/intel_guc_ads.h"
 #include "gt/uc/intel_guc_ct.h"
@@ -14,7 +13,6 @@
 #include "intel_guc.h"
 #include "intel_guc_debugfs.h"
 #include "intel_guc_log_debugfs.h"
-#include "intel_runtime_pm.h"
 
 static int guc_info_show(struct seq_file *m, void *data)
 {
@@ -73,79 +71,12 @@ static bool intel_eval_slpc_support(void *data)
 	return intel_guc_slpc_is_used(guc);
 }
 
-#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GUC)
-static ssize_t guc_send_mmio_write(struct file *file, const char __user *user,
-				   size_t count, loff_t *ppos)
-{
-	struct intel_guc *guc = file->private_data;
-	struct intel_runtime_pm *rpm = guc_to_gt(guc)->uncore->rpm;
-	u32 request[GUC_MAX_MMIO_MSG_LEN];
-	u32 response[GUC_MAX_MMIO_MSG_LEN];
-	intel_wakeref_t wakeref;
-	int ret;
-
-	if (*ppos)
-		return 0;
-
-	ret = from_user_to_u32array(user, count, request, ARRAY_SIZE(request));
-	if (ret < 0)
-		return ret;
-
-	with_intel_runtime_pm(rpm, wakeref)
-		ret = intel_guc_send_mmio(guc, request, ret, response, ARRAY_SIZE(response));
-	if (ret < 0)
-		return ret;
-
-	return count;
-}
-
-static const struct file_operations guc_send_mmio_fops = {
-	.write =	guc_send_mmio_write,
-	.open =		simple_open,
-	.llseek =	default_llseek,
-};
-
-static ssize_t guc_send_ctb_write(struct file *file, const char __user *user,
-				  size_t count, loff_t *ppos)
-{
-	struct intel_guc *guc = file->private_data;
-	struct intel_runtime_pm *rpm = guc_to_gt(guc)->uncore->rpm;
-	u32 request[32], response[8];	/* reasonable limits */
-	intel_wakeref_t wakeref;
-	int ret;
-
-	if (*ppos)
-		return 0;
-
-	ret = from_user_to_u32array(user, count, request, ARRAY_SIZE(request));
-	if (ret < 0)
-		return ret;
-
-	with_intel_runtime_pm(rpm, wakeref)
-		ret = intel_guc_send_and_receive(guc, request, ret, response, ARRAY_SIZE(response));
-	if (ret < 0)
-		return ret;
-
-	return count;
-}
-
-static const struct file_operations guc_send_ctb_fops = {
-	.write =	guc_send_ctb_write,
-	.open =		simple_open,
-	.llseek =	default_llseek,
-};
-#endif
-
 void intel_guc_debugfs_register(struct intel_guc *guc, struct dentry *root)
 {
 	static const struct intel_gt_debugfs_file files[] = {
 		{ "guc_info", &guc_info_fops, NULL },
 		{ "guc_registered_contexts", &guc_registered_contexts_fops, NULL },
 		{ "guc_slpc_info", &guc_slpc_info_fops, &intel_eval_slpc_support},
-#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GUC)
-		{ "guc_send_mmio", &guc_send_mmio_fops, NULL },
-		{ "guc_send_ctb", &guc_send_ctb_fops, NULL },
-#endif
 	};
 
 	if (!intel_guc_is_supported(guc))
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
index 4fe3bb4a0dc4..153133b87925 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_fwif.h
@@ -13,34 +13,12 @@
 
 #include "abi/guc_actions_abi.h"
 #include "abi/guc_actions_slpc_abi.h"
-#include "abi/guc_actions_pf_abi.h"
-#include "abi/guc_actions_vf_abi.h"
 #include "abi/guc_errors_abi.h"
 #include "abi/guc_communication_mmio_abi.h"
 #include "abi/guc_communication_ctb_abi.h"
 #include "abi/guc_klvs_abi.h"
 #include "abi/guc_messages_abi.h"
 
-static inline const char *hxg_type_to_string(u32 type)
-{
-	switch (type) {
-	case GUC_HXG_TYPE_REQUEST:
-		return "request";
-	case GUC_HXG_TYPE_EVENT:
-		return "event";
-	case GUC_HXG_TYPE_NO_RESPONSE_BUSY:
-		return "busy";
-	case GUC_HXG_TYPE_NO_RESPONSE_RETRY:
-		return "retry";
-	case GUC_HXG_TYPE_RESPONSE_FAILURE:
-		return "failure";
-	case GUC_HXG_TYPE_RESPONSE_SUCCESS:
-		return "response";
-	default:
-		return "<invalid>";
-	}
-}
-
 /* Payload length only i.e. don't include G2H header length */
 #define G2H_LEN_DW_SCHED_CONTEXT_MODE_SET	2
 #define G2H_LEN_DW_DEREGISTER_CONTEXT		1
@@ -219,11 +197,6 @@ static inline u8 guc_class_to_engine_class(u8 guc_class)
 	return guc_class_engine_class_map[guc_class];
 }
 
-/* Per context engine usage stats: */
-#define PPHWSP_GUC_CONTEXT_USAGE_STAMP_LO	(0x500 / sizeof(u32))
-#define PPHWSP_GUC_CONTEXT_USAGE_STAMP_HI	(PPHWSP_GUC_CONTEXT_USAGE_STAMP_LO + 1)
-#define PPHWSP_GUC_CONTEXT_USAGE_ENGINE_ID	(PPHWSP_GUC_CONTEXT_USAGE_STAMP_HI + 1)
-
 /* Work item for submitting workloads into work queue of GuC. */
 struct guc_wq_item {
 	u32 header;
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
index 35657d80ce60..7cddbbc08880 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
@@ -378,7 +378,7 @@ static inline void set_context_guc_id_invalid(struct intel_context *ce)
 	ce->guc_id.id = GUC_INVALID_CONTEXT_ID;
 }
 
-static inline struct intel_guc *ce_to_guc(const struct intel_context *ce)
+static inline struct intel_guc *ce_to_guc(struct intel_context *ce)
 {
 	return &ce->engine->gt->uc.guc;
 }
@@ -1309,8 +1309,7 @@ static ktime_t guc_engine_busyness(struct intel_engine_cs *engine, ktime_t *now)
 	 * start_gt_clk is derived from GuC state. To get a consistent
 	 * view of activity, we query the GuC state only if gt is awake.
 	 */
-	if (!in_reset && !IS_SRIOV_VF(gt->i915) &&
-	    (wakeref = intel_gt_pm_get_if_awake(gt))) {
+	if (!in_reset && (wakeref = intel_gt_pm_get_if_awake(gt))) {
 		stats_saved = *stats;
 		gt_stamp_saved = guc->timestamp.gt_stamp;
 		/*
@@ -1378,16 +1377,13 @@ static void __update_guc_busyness_stats(struct intel_guc *guc)
 	spin_unlock_irqrestore(&guc->timestamp.lock, flags);
 }
 
-static void __guc_context_update_clks(struct intel_context *ce);
 static void guc_timestamp_ping(struct work_struct *wrk)
 {
 	struct intel_guc *guc = container_of(wrk, typeof(*guc),
 					     timestamp.work.work);
 	struct intel_uc *uc = container_of(guc, typeof(*uc), guc);
 	struct intel_gt *gt = guc_to_gt(guc);
-	struct intel_context *ce;
 	intel_wakeref_t wakeref;
-	unsigned long index;
 	int srcu, ret;
 
 	/*
@@ -1401,10 +1397,6 @@ static void guc_timestamp_ping(struct work_struct *wrk)
 	with_intel_runtime_pm(&gt->i915->runtime_pm, wakeref)
 		__update_guc_busyness_stats(guc);
 
-	/* adjust context stats for overflow */
-	xa_for_each(&guc->context_lookup, index, ce)
-		__guc_context_update_clks(ce);
-
 	intel_gt_reset_unlock(gt, srcu);
 
 	mod_delayed_work(system_highpri_wq, &guc->timestamp.work,
@@ -1444,9 +1436,6 @@ void intel_guc_busyness_park(struct intel_gt *gt)
 {
 	struct intel_guc *guc = &gt->uc.guc;
 
-	if (IS_SRIOV_VF(gt->i915))
-		return;
-
 	if (!guc_submission_initialized(guc))
 		return;
 
@@ -1476,9 +1465,6 @@ void intel_guc_busyness_unpark(struct intel_gt *gt)
 	unsigned long flags;
 	ktime_t unused;
 
-	if (IS_SRIOV_VF(gt->i915))
-		return;
-
 	if (!guc_submission_initialized(guc))
 		return;
 
@@ -1489,48 +1475,6 @@ void intel_guc_busyness_unpark(struct intel_gt *gt)
 			 guc->timestamp.ping_delay);
 }
 
-static void __guc_context_update_clks(struct intel_context *ce)
-{
-	struct intel_guc *guc = ce_to_guc(ce);
-	struct intel_gt *gt = ce->engine->gt;
-	u32 *pphwsp, last_switch, engine_id;
-	u64 start_gt_clk = 0, active = 0;
-	unsigned long flags;
-	ktime_t unused;
-
-	spin_lock_irqsave(&guc->timestamp.lock, flags);
-
-	pphwsp = ((void *)ce->lrc_reg_state) - LRC_STATE_OFFSET;
-	last_switch = READ_ONCE(pphwsp[PPHWSP_GUC_CONTEXT_USAGE_STAMP_LO]);
-	engine_id = READ_ONCE(pphwsp[PPHWSP_GUC_CONTEXT_USAGE_ENGINE_ID]);
-
-	guc_update_pm_timestamp(guc, &unused);
-
-	if (engine_id != 0xffffffff && last_switch) {
-		start_gt_clk = READ_ONCE(ce->stats.runtime.start_gt_clk);
-		__extend_last_switch(guc, &start_gt_clk, last_switch);
-		active = intel_gt_clock_interval_to_ns(gt, guc->timestamp.gt_stamp - start_gt_clk);
-		WRITE_ONCE(ce->stats.runtime.start_gt_clk, start_gt_clk);
-		WRITE_ONCE(ce->stats.active, active);
-	} else {
-		lrc_update_runtime(ce);
-	}
-
-	spin_unlock_irqrestore(&guc->timestamp.lock, flags);
-}
-
-static void guc_context_update_stats(struct intel_context *ce)
-{
-	if (!intel_context_pin_if_active(ce)) {
-		WRITE_ONCE(ce->stats.runtime.start_gt_clk, 0);
-		WRITE_ONCE(ce->stats.active, 0);
-		return;
-	}
-
-	__guc_context_update_clks(ce);
-	intel_context_unpin(ce);
-}
-
 static inline bool
 submission_disabled(struct intel_guc *guc)
 {
@@ -1591,9 +1535,7 @@ void intel_guc_submission_reset_prepare(struct intel_guc *guc)
 	intel_gt_park_heartbeats(guc_to_gt(guc));
 	disable_submission(guc);
 	guc->interrupts.disable(guc);
-
-	if (!IS_SRIOV_VF(guc_to_gt(guc)->i915))
-		__reset_guc_busyness_stats(guc);
+	__reset_guc_busyness_stats(guc);
 
 	/* Flush IRQ handler */
 	spin_lock_irq(guc_to_gt(guc)->irq_lock);
@@ -1987,8 +1929,7 @@ int intel_guc_submission_init(struct intel_guc *guc)
 	if (ret)
 		return ret;
 
-	if ((GET_UC_VER(guc) < MAKE_UC_VER(70, 0, 0)) &&
-			IS_SRIOV_PF(guc_to_gt(guc)->i915)) {
+	if (GET_UC_VER(guc) < MAKE_UC_VER(70, 0, 0)) {
 		ret = guc_lrc_desc_pool_create_v69(guc);
 		if (ret)
 			return ret;
@@ -2088,18 +2029,6 @@ static void guc_submit_request(struct i915_request *rq)
 	spin_unlock_irqrestore(&sched_engine->lock, flags);
 }
 
-int intel_guc_submission_limit_ids(struct intel_guc *guc, u32 limit)
-{
-	if (limit > GUC_MAX_CONTEXT_ID)
-		return -E2BIG;
-
-	if (!ida_is_empty(&guc->submission_state.guc_ids))
-		return -ETXTBSY;
-
-	guc->submission_state.num_guc_ids = limit;
-	return 0;
-}
-
 static int new_guc_id(struct intel_guc *guc, struct intel_context *ce)
 {
 	int ret;
@@ -2434,7 +2363,7 @@ static int register_context(struct intel_context *ce, bool loop)
 	GEM_BUG_ON(intel_context_is_child(ce));
 	trace_intel_context_register(ce);
 
-	if (GET_UC_VER(guc) >= MAKE_UC_VER(70, 0, 0) || IS_SRIOV_VF(guc_to_gt(guc)->i915))
+	if (GET_UC_VER(guc) >= MAKE_UC_VER(70, 0, 0))
 		ret = register_context_v70(guc, ce, loop);
 	else
 		ret = register_context_v69(guc, ce, loop);
@@ -2446,7 +2375,7 @@ static int register_context(struct intel_context *ce, bool loop)
 		set_context_registered(ce);
 		spin_unlock_irqrestore(&ce->guc_state.lock, flags);
 
-		if (GET_UC_VER(guc) >= MAKE_UC_VER(70, 0, 0) || IS_SRIOV_VF(guc_to_gt(guc)->i915))
+		if (GET_UC_VER(guc) >= MAKE_UC_VER(70, 0, 0))
 			guc_context_policy_init_v70(ce, loop);
 	}
 
@@ -2854,7 +2783,6 @@ static void guc_context_unpin(struct intel_context *ce)
 {
 	struct intel_guc *guc = ce_to_guc(ce);
 
-	lrc_update_runtime(ce);
 	unpin_guc_id(guc, ce);
 	lrc_unpin(ce);
 
@@ -3053,7 +2981,7 @@ static void __guc_context_set_preemption_timeout(struct intel_guc *guc,
 						 u16 guc_id,
 						 u32 preemption_timeout)
 {
-	if (GET_UC_VER(guc) >= MAKE_UC_VER(70, 0, 0) || IS_SRIOV_VF(guc_to_gt(guc)->i915)) {
+	if (GET_UC_VER(guc) >= MAKE_UC_VER(70, 0, 0)) {
 		struct context_policy policy;
 
 		__guc_context_policy_start_klv(&policy, guc_id);
@@ -3318,7 +3246,7 @@ static int guc_context_alloc(struct intel_context *ce)
 static void __guc_context_set_prio(struct intel_guc *guc,
 				   struct intel_context *ce)
 {
-	if (GET_UC_VER(guc) >= MAKE_UC_VER(70, 0, 0) || IS_SRIOV_VF(guc_to_gt(guc)->i915)) {
+	if (GET_UC_VER(guc) >= MAKE_UC_VER(70, 0, 0)) {
 		struct context_policy policy;
 
 		__guc_context_policy_start_klv(&policy, ce->guc_id.id);
@@ -3476,7 +3404,6 @@ static void remove_from_context(struct i915_request *rq)
 }
 
 static const struct intel_context_ops guc_context_ops = {
-	.flags = COPS_RUNTIME_CYCLES,
 	.alloc = guc_context_alloc,
 
 	.pre_pin = guc_context_pre_pin,
@@ -3493,8 +3420,6 @@ static const struct intel_context_ops guc_context_ops = {
 
 	.sched_disable = guc_context_sched_disable,
 
-	.update_stats = guc_context_update_stats,
-
 	.reset = lrc_reset,
 	.destroy = guc_context_destroy,
 
@@ -4107,12 +4032,6 @@ static bool guc_sched_engine_disabled(struct i915_sched_engine *sched_engine)
 	return !sched_engine->tasklet.callback;
 }
 
-static int vf_guc_resume(struct intel_engine_cs *engine)
-{
-	intel_breadcrumbs_reset(engine->breadcrumbs);
-	return 0;
-}
-
 static void guc_set_default_submission(struct intel_engine_cs *engine)
 {
 	engine->submit_request = guc_submit_request;
@@ -4304,9 +4223,6 @@ int intel_guc_submission_setup(struct intel_engine_cs *engine)
 	if (engine->flags & I915_ENGINE_HAS_RCS_REG_STATE)
 		rcs_submission_override(engine);
 
-	if (IS_SRIOV_VF(engine->i915))
-		engine->resume = vf_guc_resume;
-
 	lrc_init_wa_ctx(engine);
 
 	/* Finally, take ownership and responsibility for cleanup! */
@@ -4327,9 +4243,7 @@ void intel_guc_submission_enable(struct intel_guc *guc)
 				   GUC_SEM_INTR_ENABLE_ALL);
 
 	guc_init_lrc_mapping(guc);
-
-	if (!IS_SRIOV_VF(gt->i915))
-		guc_init_engine_stats(guc);
+	guc_init_engine_stats(guc);
 }
 
 void intel_guc_submission_disable(struct intel_guc *guc)
@@ -4744,8 +4658,6 @@ void intel_guc_find_hung_context(struct intel_engine_cs *engine)
 
 	xa_lock_irqsave(&guc->context_lookup, flags);
 	xa_for_each(&guc->context_lookup, index, ce) {
-		bool found;
-
 		if (!kref_get_unless_zero(&ce->ref))
 			continue;
 
@@ -4762,18 +4674,10 @@ void intel_guc_find_hung_context(struct intel_engine_cs *engine)
 				goto next;
 		}
 
-		found = false;
-		spin_lock(&ce->guc_state.lock);
 		list_for_each_entry(rq, &ce->guc_state.requests, sched.link) {
 			if (i915_test_request_state(rq) != I915_REQUEST_ACTIVE)
 				continue;
 
-			found = true;
-			break;
-		}
-		spin_unlock(&ce->guc_state.lock);
-
-		if (found) {
 			intel_engine_set_hung_context(engine, ce);
 
 			/* Can only cope with one hang at a time... */
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
index 094d6e7fcd89..5a95a9f0a8e3 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
@@ -14,7 +14,6 @@ struct drm_printer;
 struct intel_engine_cs;
 
 void intel_guc_submission_init_early(struct intel_guc *guc);
-int intel_guc_submission_limit_ids(struct intel_guc *guc, u32 limit);
 int intel_guc_submission_init(struct intel_guc *guc);
 void intel_guc_submission_enable(struct intel_guc *guc);
 void intel_guc_submission_disable(struct intel_guc *guc);
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_huc.h b/drivers/gpu/drm/i915/gt/uc/intel_huc.h
index b6735fbe8723..52db03620c60 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_huc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_huc.h
@@ -73,8 +73,7 @@ static inline bool intel_huc_is_wanted(struct intel_huc *huc)
 static inline bool intel_huc_is_used(struct intel_huc *huc)
 {
 	GEM_BUG_ON(__intel_uc_fw_status(&huc->fw) == INTEL_UC_FIRMWARE_SELECTED);
-	return intel_uc_fw_is_available(&huc->fw) ||
-	       intel_uc_fw_is_preloaded(&huc->fw);
+	return intel_uc_fw_is_available(&huc->fw);
 }
 
 static inline bool intel_huc_is_loaded_by_gsc(const struct intel_huc *huc)
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc.c b/drivers/gpu/drm/i915/gt/uc/intel_uc.c
index f1d07cf4039c..b59e7ad6ef2e 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc.c
@@ -7,7 +7,6 @@
 
 #include "gt/intel_gt.h"
 #include "gt/intel_reset.h"
-#include "gt/iov/intel_iov_query.h"
 #include "intel_guc.h"
 #include "intel_guc_ads.h"
 #include "intel_guc_submission.h"
@@ -18,7 +17,6 @@
 
 static const struct intel_uc_ops uc_ops_off;
 static const struct intel_uc_ops uc_ops_on;
-static const struct intel_uc_ops uc_ops_vf;
 
 static void uc_expand_default_options(struct intel_uc *uc)
 {
@@ -156,10 +154,6 @@ void intel_uc_driver_late_release(struct intel_uc *uc)
 void intel_uc_init_mmio(struct intel_uc *uc)
 {
 	intel_guc_init_send_regs(&uc->guc);
-
-	/* XXX can't do it in intel_uc_init_early, it's too early */
-	if (IS_SRIOV_VF(uc_to_gt(uc)->i915))
-		uc->ops = &uc_ops_vf;
 }
 
 static void __uc_capture_load_err_log(struct intel_uc *uc)
@@ -193,9 +187,6 @@ void intel_uc_driver_remove(struct intel_uc *uc)
  */
 static void guc_clear_mmio_msg(struct intel_guc *guc)
 {
-	if (IS_SRIOV_VF(guc_to_gt(guc)->i915))
-		return;
-
 	intel_uncore_write(guc_to_gt(guc)->uncore, SOFT_SCRATCH(15), 0);
 }
 
@@ -203,9 +194,6 @@ static void guc_get_mmio_msg(struct intel_guc *guc)
 {
 	u32 val;
 
-	if (IS_SRIOV_VF(guc_to_gt(guc)->i915))
-		return;
-
 	spin_lock_irq(&guc->irq_lock);
 
 	val = intel_uncore_read(guc_to_gt(guc)->uncore, SOFT_SCRATCH(15));
@@ -595,96 +583,6 @@ static void __uc_fini_hw(struct intel_uc *uc)
 	__uc_sanitize(uc);
 }
 
-static int __vf_uc_sanitize(struct intel_uc *uc)
-{
-	intel_huc_sanitize(&uc->huc);
-	intel_guc_sanitize(&uc->guc);
-
-	return 0;
-}
-
-static int __vf_uc_init(struct intel_uc *uc)
-{
-	return intel_guc_init(&uc->guc);
-}
-
-static void __vf_uc_fini(struct intel_uc *uc)
-{
-	intel_guc_fini(&uc->guc);
-}
-
-static int __vf_uc_init_hw(struct intel_uc *uc)
-{
-	struct intel_gt *gt = uc_to_gt(uc);
-	struct drm_i915_private *i915 = gt->i915;
-	struct intel_guc *guc = &uc->guc;
-	struct intel_huc *huc = &uc->huc;
-	struct intel_uc_fw *fw = &guc->fw;
-	int err;
-
-	GEM_BUG_ON(!HAS_GT_UC(i915));
-	GEM_BUG_ON(!IS_SRIOV_VF(i915));
-	GEM_BUG_ON(!intel_uc_uses_guc_submission(&gt->uc));
-
-	err = intel_iov_query_bootstrap(&gt->iov);
-	if (unlikely(err))
-		goto err_out;
-
-	if (!intel_uc_fw_is_running(&guc->fw)) {
-		err = intel_uc_fw_status_to_error(guc->fw.status);
-		goto err_out;
-	}
-
-	intel_guc_reset_interrupts(guc);
-
-	err = guc_enable_communication(guc);
-	if (unlikely(err))
-		goto err_out;
-
-	err = intel_iov_query_version(&gt->iov);
-	if (unlikely(err))
-		goto err_out;
-
-	/*
-	 * pretend that HuC is running if it is supported
-	 * for status rely on runtime reg shared by PF
-	 */
-	if (intel_uc_fw_is_supported(&huc->fw)) {
-		/* XXX: We don't know how to get the HuC version yet */
-		intel_uc_fw_set_preloaded(&huc->fw, 0, 0);
-	}
-
-	intel_guc_submission_enable(guc);
-
-	dev_info(i915->drm.dev, "%s firmware %s version %u.%u %s:%s\n",
-		 intel_uc_fw_type_repr(INTEL_UC_FW_TYPE_GUC), fw->file_selected.path,
-		 fw->file_selected.major_ver, fw->file_selected.minor_ver,
-		 "submission", i915_iov_mode_to_string(IOV_MODE(i915)));
-
-	dev_info(i915->drm.dev, "%s firmware %s\n",
-		 intel_uc_fw_type_repr(INTEL_UC_FW_TYPE_HUC),
-		 intel_uc_fw_status_repr(__intel_uc_fw_status(&huc->fw)));
-
-	return 0;
-
-err_out:
-	__vf_uc_sanitize(uc);
-	i915_probe_error(i915, "GuC initialization failed (%pe)\n", ERR_PTR(err));
-	return -EIO;
-}
-
-static void __vf_uc_fini_hw(struct intel_uc *uc)
-{
-	struct intel_guc *guc = &uc->guc;
-
-	intel_guc_submission_disable(guc);
-
-	if (intel_guc_ct_enabled(&guc->ct))
-		guc_disable_communication(guc);
-
-	__vf_uc_sanitize(uc);
-}
-
 /**
  * intel_uc_reset_prepare - Prepare for reset
  * @uc: the intel_uc structure
@@ -709,7 +607,7 @@ void intel_uc_reset_prepare(struct intel_uc *uc)
 		intel_guc_submission_reset_prepare(guc);
 
 sanitize:
-	intel_uc_sanitize(uc);
+	__uc_sanitize(uc);
 }
 
 void intel_uc_reset(struct intel_uc *uc, intel_engine_mask_t stalled)
@@ -841,11 +739,3 @@ static const struct intel_uc_ops uc_ops_on = {
 	.init_hw = __uc_init_hw,
 	.fini_hw = __uc_fini_hw,
 };
-
-static const struct intel_uc_ops uc_ops_vf = {
-	.sanitize = __vf_uc_sanitize,
-	.init = __vf_uc_init,
-	.fini = __vf_uc_fini,
-	.init_hw = __vf_uc_init_hw,
-	.fini_hw = __vf_uc_fini_hw,
-};
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
index c33d90763072..de2843dc1307 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.c
@@ -427,24 +427,6 @@ void intel_uc_fw_init_early(struct intel_uc_fw *uc_fw,
 				  INTEL_UC_FIRMWARE_NOT_SUPPORTED);
 }
 
-/**
- * intel_uc_fw_set_preloaded() - set uC firmware as pre-loaded
- * @uc_fw: uC firmware structure
- * @major: major version of the pre-loaded firmware
- * @minor: minor version of the pre-loaded firmware
- *
- * If the uC firmware was loaded to h/w by other entity, just
- * mark it as loaded.
- */
-void intel_uc_fw_set_preloaded(struct intel_uc_fw *uc_fw, u16 major, u16 minor)
-{
-	uc_fw->file_selected.path = "PRELOADED";
-	uc_fw->file_selected.major_ver = major;
-	uc_fw->file_selected.minor_ver = minor;
-
-	intel_uc_fw_change_status(uc_fw, INTEL_UC_FIRMWARE_PRELOADED);
-}
-
 static void __force_fw_fetch_failures(struct intel_uc_fw *uc_fw, int e)
 {
 	struct drm_i915_private *i915 = __uc_fw_to_gt(uc_fw)->i915;
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h
index 25b6ef80eaf6..cb586f7df270 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc_fw.h
@@ -55,8 +55,7 @@ enum intel_uc_fw_status {
 	INTEL_UC_FIRMWARE_LOADABLE, /* all fw-required objects are ready */
 	INTEL_UC_FIRMWARE_LOAD_FAIL, /* failed to xfer or init/auth the fw */
 	INTEL_UC_FIRMWARE_TRANSFERRED, /* dma xfer done */
-	INTEL_UC_FIRMWARE_RUNNING, /* init/auth done */
-	INTEL_UC_FIRMWARE_PRELOADED, /* already pre-loaded */
+	INTEL_UC_FIRMWARE_RUNNING /* init/auth done */
 };
 
 enum intel_uc_fw_type {
@@ -154,8 +153,6 @@ const char *intel_uc_fw_status_repr(enum intel_uc_fw_status status)
 		return "TRANSFERRED";
 	case INTEL_UC_FIRMWARE_RUNNING:
 		return "RUNNING";
-	case INTEL_UC_FIRMWARE_PRELOADED:
-		return "PRELOADED";
 	}
 	return "<invalid>";
 }
@@ -182,7 +179,6 @@ static inline int intel_uc_fw_status_to_error(enum intel_uc_fw_status status)
 	case INTEL_UC_FIRMWARE_LOADABLE:
 	case INTEL_UC_FIRMWARE_TRANSFERRED:
 	case INTEL_UC_FIRMWARE_RUNNING:
-	case INTEL_UC_FIRMWARE_PRELOADED:
 		return 0;
 	}
 	return -EINVAL;
@@ -219,14 +215,12 @@ static inline bool intel_uc_fw_is_enabled(struct intel_uc_fw *uc_fw)
 
 static inline bool intel_uc_fw_is_available(struct intel_uc_fw *uc_fw)
 {
-	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_AVAILABLE &&
-	       __intel_uc_fw_status(uc_fw) != INTEL_UC_FIRMWARE_PRELOADED;
+	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_AVAILABLE;
 }
 
 static inline bool intel_uc_fw_is_loadable(struct intel_uc_fw *uc_fw)
 {
-	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_LOADABLE &&
-	       __intel_uc_fw_status(uc_fw) != INTEL_UC_FIRMWARE_PRELOADED;
+	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_LOADABLE;
 }
 
 static inline bool intel_uc_fw_is_loaded(struct intel_uc_fw *uc_fw)
@@ -236,12 +230,7 @@ static inline bool intel_uc_fw_is_loaded(struct intel_uc_fw *uc_fw)
 
 static inline bool intel_uc_fw_is_running(struct intel_uc_fw *uc_fw)
 {
-	return __intel_uc_fw_status(uc_fw) >= INTEL_UC_FIRMWARE_RUNNING;
-}
-
-static inline bool intel_uc_fw_is_preloaded(struct intel_uc_fw *uc_fw)
-{
-	return __intel_uc_fw_status(uc_fw) == INTEL_UC_FIRMWARE_PRELOADED;
+	return __intel_uc_fw_status(uc_fw) == INTEL_UC_FIRMWARE_RUNNING;
 }
 
 static inline bool intel_uc_fw_is_overridden(const struct intel_uc_fw *uc_fw)
@@ -251,7 +240,7 @@ static inline bool intel_uc_fw_is_overridden(const struct intel_uc_fw *uc_fw)
 
 static inline void intel_uc_fw_sanitize(struct intel_uc_fw *uc_fw)
 {
-	if (intel_uc_fw_is_loadable(uc_fw))
+	if (intel_uc_fw_is_loaded(uc_fw))
 		intel_uc_fw_change_status(uc_fw, INTEL_UC_FIRMWARE_LOADABLE);
 }
 
@@ -278,7 +267,6 @@ static inline u32 intel_uc_fw_get_upload_size(struct intel_uc_fw *uc_fw)
 
 void intel_uc_fw_init_early(struct intel_uc_fw *uc_fw,
 			    enum intel_uc_fw_type type);
-void intel_uc_fw_set_preloaded(struct intel_uc_fw *uc_fw, u16 major, u16 minor);
 int intel_uc_fw_fetch(struct intel_uc_fw *uc_fw);
 void intel_uc_fw_cleanup_fetch(struct intel_uc_fw *uc_fw);
 int intel_uc_fw_upload(struct intel_uc_fw *uc_fw, u32 offset, u32 dma_flags);
diff --git a/drivers/gpu/drm/i915/gvt/debugfs.c b/drivers/gpu/drm/i915/gvt/debugfs.c
index e08ed0e9f165..9f1c209d9251 100644
--- a/drivers/gpu/drm/i915/gvt/debugfs.c
+++ b/drivers/gpu/drm/i915/gvt/debugfs.c
@@ -175,13 +175,8 @@ void intel_gvt_debugfs_add_vgpu(struct intel_vgpu *vgpu)
  */
 void intel_gvt_debugfs_remove_vgpu(struct intel_vgpu *vgpu)
 {
-	struct intel_gvt *gvt = vgpu->gvt;
-	struct drm_minor *minor = gvt->gt->i915->drm.primary;
-
-	if (minor->debugfs_root && gvt->debugfs_root) {
-		debugfs_remove_recursive(vgpu->debugfs);
-		vgpu->debugfs = NULL;
-	}
+	debugfs_remove_recursive(vgpu->debugfs);
+	vgpu->debugfs = NULL;
 }
 
 /**
@@ -204,10 +199,6 @@ void intel_gvt_debugfs_init(struct intel_gvt *gvt)
  */
 void intel_gvt_debugfs_clean(struct intel_gvt *gvt)
 {
-	struct drm_minor *minor = gvt->gt->i915->drm.primary;
-
-	if (minor->debugfs_root) {
-		debugfs_remove_recursive(gvt->debugfs_root);
-		gvt->debugfs_root = NULL;
-	}
+	debugfs_remove_recursive(gvt->debugfs_root);
+	gvt->debugfs_root = NULL;
 }
diff --git a/drivers/gpu/drm/i915/gvt/gtt.c b/drivers/gpu/drm/i915/gvt/gtt.c
index 63a9e1d3956d..9c5cc2800975 100644
--- a/drivers/gpu/drm/i915/gvt/gtt.c
+++ b/drivers/gpu/drm/i915/gvt/gtt.c
@@ -1215,7 +1215,8 @@ static int split_2MB_gtt_entry(struct intel_vgpu *vgpu,
 		ret = intel_gvt_dma_map_guest_page(vgpu, start_gfn + sub_index,
 						   PAGE_SIZE, &dma_addr);
 		if (ret) {
-			goto err;
+			ppgtt_invalidate_spt(spt);
+			return ret;
 		}
 		sub_se.val64 = se->val64;
 
@@ -1235,17 +1236,6 @@ static int split_2MB_gtt_entry(struct intel_vgpu *vgpu,
 	ops->set_pfn(se, sub_spt->shadow_page.mfn);
 	ppgtt_set_shadow_entry(spt, se, index);
 	return 0;
-err:
-	/* Cancel the existing addess mappings of DMA addr. */
-	for_each_present_shadow_entry(sub_spt, &sub_se, sub_index) {
-		gvt_vdbg_mm("invalidate 4K entry\n");
-		ppgtt_invalidate_pte(sub_spt, &sub_se);
-	}
-	/* Release the new allocated spt. */
-	trace_spt_change(sub_spt->vgpu->id, "release", sub_spt,
-		sub_spt->guest_page.gfn, sub_spt->shadow_page.type);
-	ppgtt_free_spt(sub_spt);
-	return ret;
 }
 
 static int split_64KB_gtt_entry(struct intel_vgpu *vgpu,
diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 8342d95f56cb..d6fe94cd0fdb 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -696,7 +696,6 @@ intel_vgpu_shadow_mm_pin(struct intel_vgpu_workload *workload)
 
 	if (workload->shadow_mm->type != INTEL_GVT_MM_PPGTT ||
 	    !workload->shadow_mm->ppgtt_mm.shadowed) {
-		intel_vgpu_unpin_mm(workload->shadow_mm);
 		gvt_vgpu_err("workload shadow ppgtt isn't ready\n");
 		return -EINVAL;
 	}
diff --git a/drivers/gpu/drm/i915/i915_active.c b/drivers/gpu/drm/i915/i915_active.c
index 053844dc3933..7412abf166a8 100644
--- a/drivers/gpu/drm/i915/i915_active.c
+++ b/drivers/gpu/drm/i915/i915_active.c
@@ -5,7 +5,6 @@
  */
 
 #include <linux/debugobjects.h>
-#include <linux/delay.h>
 
 #include "gt/intel_context.h"
 #include "gt/intel_engine_heartbeat.h"
@@ -93,7 +92,8 @@ static void debug_active_init(struct i915_active *ref)
 static void debug_active_activate(struct i915_active *ref)
 {
 	lockdep_assert_held(&ref->tree_lock);
-	debug_object_activate(ref, &active_debug_desc);
+	if (!atomic_read(&ref->count)) /* before the first inc */
+		debug_object_activate(ref, &active_debug_desc);
 }
 
 static void debug_active_deactivate(struct i915_active *ref)
@@ -422,7 +422,8 @@ replace_barrier(struct i915_active *ref, struct i915_active_fence *active)
 	 * we can use it to substitute for the pending idle-barrer
 	 * request that we want to emit on the kernel_context.
 	 */
-	return __active_del_barrier(ref, node_from_active(active));
+	__active_del_barrier(ref, node_from_active(active));
+	return true;
 }
 
 int i915_active_add_request(struct i915_active *ref, struct i915_request *rq)
@@ -436,19 +437,16 @@ int i915_active_add_request(struct i915_active *ref, struct i915_request *rq)
 	if (err)
 		return err;
 
-	do {
-		active = active_instance(ref, i915_request_timeline(rq)->fence_context);
-		if (!active) {
-			err = -ENOMEM;
-			goto out;
-		}
-
-		if (replace_barrier(ref, active)) {
-			RCU_INIT_POINTER(active->fence, NULL);
-			atomic_dec(&ref->count);
-		}
-	} while (unlikely(is_barrier(active)));
+	active = active_instance(ref, i915_request_timeline(rq)->fence_context);
+	if (!active) {
+		err = -ENOMEM;
+		goto out;
+	}
 
+	if (replace_barrier(ref, active)) {
+		RCU_INIT_POINTER(active->fence, NULL);
+		atomic_dec(&ref->count);
+	}
 	if (!__i915_active_fence_set(active, fence))
 		__i915_active_acquire(ref);
 
@@ -866,8 +864,7 @@ int i915_active_acquire_preallocate_barrier(struct i915_active *ref,
 
 	/* Wait until the previous preallocation is completed */
 	while (!llist_empty(&ref->preallocated_barriers))
-		//cond_resched();
-		usleep_range(10, 50);
+		cond_resched();
 
 	/*
 	 * Preallocate a node for each physical engine supporting the target
diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index b6af31a05cd0..ae987e92251d 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -78,16 +78,6 @@ static int i915_capabilities(struct seq_file *m, void *data)
 	return 0;
 }
 
-static int sriov_info(struct seq_file *m, void *data)
-{
-	struct drm_i915_private *i915 = node_to_i915(m->private);
-	struct drm_printer p = drm_seq_file_printer(m);
-
-	i915_sriov_print_info(i915, &p);
-
-	return 0;
-}
-
 static char get_tiling_flag(struct drm_i915_gem_object *obj)
 {
 	switch (i915_gem_object_get_tiling(obj)) {
@@ -771,7 +761,6 @@ static const struct drm_info_list i915_debugfs_list[] = {
 	{"i915_wa_registers", i915_wa_registers, 0},
 	{"i915_sseu_status", i915_sseu_status, 0},
 	{"i915_rps_boost_info", i915_rps_boost_info, 0},
-	{"i915_sriov_info", sriov_info, 0},
 };
 #define I915_DEBUGFS_ENTRIES ARRAY_SIZE(i915_debugfs_list)
 
diff --git a/drivers/gpu/drm/i915/i915_debugfs_params.c b/drivers/gpu/drm/i915/i915_debugfs_params.c
index 4b65eaa904ce..783c8676eee2 100644
--- a/drivers/gpu/drm/i915/i915_debugfs_params.c
+++ b/drivers/gpu/drm/i915/i915_debugfs_params.c
@@ -40,17 +40,6 @@ static int notify_guc(struct drm_i915_private *i915)
 {
 	int ret = 0;
 
-	/*
-	 * FIXME: This needs to return -EPERM to userland to indicate
-	 * that a VF is not allowed to change the scheduling policies.
-	 * However, doing so will currently 'break' a whole bunch of IGT
-	 * tests that rely on disabling engine reset. Although, they are
-	 * already broken as they will not correctly detect hang failures
-	 * and are potentially returning false successes.
-	 */
-	if (IS_SRIOV_VF(i915))
-		return 0;
-
 	if (intel_uc_uses_guc_submission(&to_gt(i915)->uc))
 		ret = intel_guc_global_policies_update(&to_gt(i915)->uc.guc);
 
diff --git a/drivers/gpu/drm/i915/i915_driver.c b/drivers/gpu/drm/i915/i915_driver.c
index 6fe02630f616..365f5c07aa95 100644
--- a/drivers/gpu/drm/i915/i915_driver.c
+++ b/drivers/gpu/drm/i915/i915_driver.c
@@ -72,7 +72,6 @@
 #include "gt/intel_gt.h"
 #include "gt/intel_gt_pm.h"
 #include "gt/intel_rc6.h"
-#include "gt/iov/intel_iov.h"
 
 #include "pxp/intel_pxp_pm.h"
 
@@ -475,8 +474,6 @@ static int i915_driver_mmio_probe(struct drm_i915_private *dev_priv)
 			goto err_uncore;
 	}
 
-	intel_power_domains_prune(dev_priv);
-
 	/* As early as possible, scrub existing GPU state before clobbering */
 	sanitize_gpu(dev_priv);
 
@@ -648,10 +645,6 @@ static int i915_driver_hw_probe(struct drm_i915_private *dev_priv)
 
 	pci_set_master(pdev);
 
-	/* Assume that VF is up, otherwise we may end with unknown state */
-	if (IS_SRIOV_VF(dev_priv))
-		ret = pci_set_power_state(pdev, PCI_D0);
-
 	/* On the 945G/GM, the chipset reports the MSI capability on the
 	 * integrated graphics even though the support isn't actually there
 	 * according to the published specs.  It doesn't appear to function
@@ -773,8 +766,7 @@ static void i915_driver_register(struct drm_i915_private *dev_priv)
 	for_each_gt(gt, dev_priv, i)
 		intel_gt_driver_register(gt);
 
-	if (!IS_SRIOV_VF(dev_priv))
-		i915_hwmon_register(dev_priv);
+	i915_hwmon_register(dev_priv);
 
 	intel_display_driver_register(dev_priv);
 
@@ -845,8 +837,6 @@ static void i915_welcome_messages(struct drm_i915_private *dev_priv)
 		i915_print_iommu_status(dev_priv, &p);
 		for_each_gt(gt, dev_priv, i)
 			intel_gt_info_print(&gt->info, &p);
-
-		drm_printf(&p, "mode: %s\n", i915_iov_mode_to_string(IOV_MODE(dev_priv)));
 	}
 
 	if (IS_ENABLED(CONFIG_DRM_I915_DEBUG))
@@ -889,29 +879,6 @@ i915_driver_create(struct pci_dev *pdev, const struct pci_device_id *ent)
 	return i915;
 }
 
-static void i915_virtualization_probe(struct drm_i915_private *i915)
-{
-	GEM_BUG_ON(i915->__mode);
-
-	intel_vgpu_detect(i915);
-	if (intel_vgpu_active(i915))
-		i915->__mode = I915_IOV_MODE_GVT_VGPU;
-	else
-		i915->__mode = i915_sriov_probe(i915);
-
-	GEM_BUG_ON(!i915->__mode);
-
-	if (IS_IOV_ACTIVE(i915))
-		dev_info(i915->drm.dev, "Running in %s mode\n",
-			 i915_iov_mode_to_string(IOV_MODE(i915)));
-}
-
-static void i915_virtualization_commit(struct drm_i915_private *i915)
-{
-	if (IS_SRIOV_PF(i915))
-		i915_sriov_pf_confirm(i915);
-}
-
 /**
  * i915_driver_probe - setup chip and create an initial config
  * @pdev: PCI device
@@ -925,8 +892,6 @@ static void i915_virtualization_commit(struct drm_i915_private *i915)
  */
 int i915_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 {
-	const struct intel_device_info *match_info =
-		(struct intel_device_info *)ent->driver_data;
 	struct drm_i915_private *i915;
 	int ret;
 
@@ -934,21 +899,6 @@ int i915_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	if (IS_ERR(i915))
 		return PTR_ERR(i915);
 
-	/*
-	 * Hack to enable CCS and set ppgtt_size to 47
-	 * on TGL and DG1 for testing purpose
-	 *
-	 */
-	if ((match_info->platform == INTEL_DG1 ||
-	    match_info->platform == INTEL_TIGERLAKE ||
-		match_info->platform == INTEL_ALDERLAKE_S ||
-		match_info->platform == INTEL_ALDERLAKE_P) &&
-	    (i915->params.enable_guc & ENABLE_GUC_SUBMISSION)
-	    && i915->params.enable_guc != -1) {
-		RUNTIME_INFO(i915)->ppgtt_size = 47;
-		RUNTIME_INFO(i915)->platform_engine_mask |= BIT(CCS0);
-	}
-
 	/* Disable nuclear pageflip by default on pre-ILK */
 	if (!i915->params.nuclear_pageflip && DISPLAY_VER(i915) < 5)
 		i915->drm.driver_features &= ~DRIVER_ATOMIC;
@@ -963,15 +913,7 @@ int i915_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	disable_rpm_wakeref_asserts(&i915->runtime_pm);
 
-	/* This must be called before any calls to IS/IOV_MODE() macros */
-	i915_virtualization_probe(i915);
-
-	ret = i915_sriov_early_tweaks(i915);
-	if (ret < 0)
-		goto out_pci_disable;
-
-	/* XXX find better place */
-	intel_iov_init_early(&to_gt(i915)->iov);
+	intel_vgpu_detect(i915);
 
 	ret = intel_gt_probe_all(i915);
 	if (ret < 0)
@@ -1009,8 +951,6 @@ int i915_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	enable_rpm_wakeref_asserts(&i915->runtime_pm);
 
-	i915_virtualization_commit(i915);
-
 	i915_welcome_messages(i915);
 
 	i915->do_release = true;
@@ -1135,9 +1075,12 @@ static int i915_driver_open(struct drm_device *dev, struct drm_file *file)
  */
 static void i915_driver_lastclose(struct drm_device *dev)
 {
+	struct drm_i915_private *i915 = to_i915(dev);
+
 	intel_fbdev_restore_mode(dev);
 
-	vga_switcheroo_process_delayed_switch();
+	if (HAS_DISPLAY(i915))
+		vga_switcheroo_process_delayed_switch();
 }
 
 static void i915_driver_postclose(struct drm_device *dev, struct drm_file *file)
@@ -1516,11 +1459,8 @@ static int i915_drm_resume_early(struct drm_device *dev)
 
 	intel_power_domains_resume(dev_priv);
 
-	intel_iov_resume(&to_gt(dev_priv)->iov);
-
 	enable_rpm_wakeref_asserts(&dev_priv->runtime_pm);
 
-
 	return ret;
 }
 
diff --git a/drivers/gpu/drm/i915/i915_drm_client.c b/drivers/gpu/drm/i915/i915_drm_client.c
index 8d81119fff14..b09d1d386574 100644
--- a/drivers/gpu/drm/i915/i915_drm_client.c
+++ b/drivers/gpu/drm/i915/i915_drm_client.c
@@ -147,7 +147,11 @@ void i915_drm_client_fdinfo(struct seq_file *m, struct file *f)
 		   PCI_SLOT(pdev->devfn), PCI_FUNC(pdev->devfn));
 	seq_printf(m, "drm-client-id:\t%u\n", client->id);
 
-	if (GRAPHICS_VER(i915) < 8)
+	/*
+	 * Temporarily skip showing client engine information with GuC submission till
+	 * fetching engine busyness is implemented in the GuC submission backend
+	 */
+	if (GRAPHICS_VER(i915) < 8 || intel_uc_uses_guc_submission(&i915->gt0.uc))
 		return;
 
 	for (i = 0; i < ARRAY_SIZE(uabi_class_names); i++)
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 229eb1b4bdbf..bb24051b1a46 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -56,11 +56,7 @@
 #include "i915_params.h"
 #include "i915_perf_types.h"
 #include "i915_scheduler.h"
-#include "i915_sriov.h"
-#include "i915_sriov_types.h"
-#include "i915_virtualization.h"
 #include "i915_utils.h"
-#include "i915_irq.h"
 #include "intel_device_info.h"
 #include "intel_memory_region.h"
 #include "intel_pch.h"
@@ -208,14 +204,6 @@ struct drm_i915_private {
 	/* i915 device parameters */
 	struct i915_params params;
 
-	/* i915 virtualization mode, use IOV_MODE() to access */
-	enum i915_iov_mode __mode;
-#define IOV_MODE(i915) ({				\
-	BUILD_BUG_ON(!I915_IOV_MODE_NONE);		\
-	GEM_BUG_ON(!(i915)->__mode);			\
-	(i915)->__mode;					\
-})
-
 	const struct intel_device_info __info; /* Use INTEL_INFO() to access. */
 	struct intel_runtime_info __runtime; /* Use RUNTIME_INFO() to access. */
 	struct intel_driver_caps caps;
@@ -247,7 +235,6 @@ struct drm_i915_private {
 	struct intel_uncore uncore;
 	struct intel_uncore_mmio_debug mmio_debug;
 
-	struct i915_sriov sriov;
 	struct i915_virtual_gpu vgpu;
 
 	struct intel_gvt *gvt;
@@ -936,8 +923,6 @@ IS_SUBPLATFORM(const struct drm_i915_private *i915,
 
 #define HAS_GT_UC(dev_priv)	(INTEL_INFO(dev_priv)->has_gt_uc)
 
-#define HAS_SRIOV(dev_priv)	(INTEL_INFO(dev_priv)->has_sriov)
-
 #define HAS_POOLED_EU(dev_priv)	(RUNTIME_INFO(dev_priv)->has_pooled_eu)
 
 #define HAS_GLOBAL_MOCS_REGISTERS(dev_priv)	(INTEL_INFO(dev_priv)->has_global_mocs)
diff --git a/drivers/gpu/drm/i915/i915_gpu_error.c b/drivers/gpu/drm/i915/i915_gpu_error.c
index 48af0a0e791a..54c6e7431ef5 100644
--- a/drivers/gpu/drm/i915/i915_gpu_error.c
+++ b/drivers/gpu/drm/i915/i915_gpu_error.c
@@ -507,7 +507,7 @@ static void error_print_context(struct drm_i915_error_state_buf *m,
 }
 
 static struct i915_vma_coredump *
-__i915_find_vma(struct i915_vma_coredump *vma, const char *name)
+__find_vma1(struct i915_vma_coredump *vma, const char *name)
 {
 	while (vma) {
 		if (strcmp(vma->name, name) == 0)
@@ -521,7 +521,7 @@ __i915_find_vma(struct i915_vma_coredump *vma, const char *name)
 struct i915_vma_coredump *
 intel_gpu_error_find_batch(const struct intel_engine_coredump *ee)
 {
-	return __i915_find_vma(ee->vma, "batch");
+	return __find_vma1(ee->vma, "batch");
 }
 
 static void error_print_engine(struct drm_i915_error_state_buf *m,
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index 9a4dae69486c..ffe5e4b86d90 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -117,9 +117,6 @@ i915_param_named_unsafe(force_probe, charp, 0400,
 	"Force probe the driver for specified devices. "
 	"See CONFIG_DRM_I915_FORCE_PROBE for details.");
 
-i915_param_named_unsafe(enable_secure_batch, bool, 0400,
-	"Enable for legacy tests I915_EXEC_SECURE. (default: 0)");
-
 i915_param_named_unsafe(disable_power_well, int, 0400,
 	"Disable display power wells when possible "
 	"(-1=auto [default], 0=power wells always on, 1=power wells disabled when possible)");
@@ -172,11 +169,7 @@ i915_param_named_unsafe(edp_vswing, int, 0400,
 i915_param_named_unsafe(enable_guc, int, 0400,
 	"Enable GuC load for GuC submission and/or HuC load. "
 	"Required functionality can be selected using bitmask values. "
-	"(-1=auto [default], 0=disable, 1=GuC submission, 2=HuC load, "
-	"4=SR-IOV PF)");
-
-i915_param_named_unsafe(guc_feature_flags, uint, 0400,
-	"GuC feature flags. Requires GuC to be loaded. (0=none [default])");
+	"(-1=auto [default], 0=disable, 1=GuC submission, 2=HuC load)");
 
 i915_param_named(guc_log_level, int, 0400,
 	"GuC firmware logging level. Requires GuC to be loaded. "
@@ -218,17 +211,6 @@ i915_param_named_unsafe(lmem_size, uint, 0400,
 i915_param_named_unsafe(lmem_bar_size, uint, 0400,
 			"Set the lmem bar size(in MiB).");
 
-i915_param_named(max_vfs, uint, 0400,
-	"Limit number of virtual functions to allocate. "
-	"(default: no limit; N=limit to N, 0=no VFs)");
-
-#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_IOV)
-i915_param_named_unsafe(vfs_flr_mask, ulong, 0600,
-	"Bitmask to enable (1) or disable (0) cleaning by PF VF's resources "
-	"(GGTT and LMEM) after FLR (default: ~0 - cleaning enable for all VFs) "
-	"Bit number indicates VF number, e.g. bit 1 indicates VF1");
-#endif
-
 static __always_inline void _print_param(struct drm_printer *p,
 					 const char *name,
 					 const char *type,
diff --git a/drivers/gpu/drm/i915/i915_params.h b/drivers/gpu/drm/i915/i915_params.h
index 7d2b5c30d7a1..5a6310d62590 100644
--- a/drivers/gpu/drm/i915/i915_params.h
+++ b/drivers/gpu/drm/i915/i915_params.h
@@ -32,8 +32,7 @@ struct drm_printer;
 
 #define ENABLE_GUC_SUBMISSION		BIT(0)
 #define ENABLE_GUC_LOAD_HUC		BIT(1)
-#define ENABLE_GUC_SRIOV_PF		BIT(2)
-#define ENABLE_GUC_MASK			GENMASK(2, 0)
+#define ENABLE_GUC_MASK			GENMASK(1, 0)
 
 /*
  * Invoke param, a function-like macro, for each i915 param, with arguments:
@@ -62,7 +61,6 @@ struct drm_printer;
 	param(int, enable_ips, 1, 0600) \
 	param(int, invert_brightness, 0, 0600) \
 	param(int, enable_guc, -1, 0400) \
-	param(unsigned int, guc_feature_flags, 0, 0400) \
 	param(int, guc_log_level, -1, 0400) \
 	param(char *, guc_firmware_path, NULL, 0400) \
 	param(char *, huc_firmware_path, NULL, 0400) \
@@ -78,10 +76,7 @@ struct drm_printer;
 	param(unsigned int, request_timeout_ms, CONFIG_DRM_I915_REQUEST_TIMEOUT, CONFIG_DRM_I915_REQUEST_TIMEOUT ? 0600 : 0) \
 	param(unsigned int, lmem_size, 0, 0400) \
 	param(unsigned int, lmem_bar_size, 0, 0400) \
-	param(unsigned int, max_vfs, ~0, 0400) \
-	param(unsigned long, vfs_flr_mask, ~0, IS_ENABLED(CONFIG_DRM_I915_DEBUG_IOV) ? 0600 : 0) \
 	/* leave bools at the end to not create holes */ \
-	param(bool, enable_secure_batch, false, 0400) \
 	param(bool, enable_hangcheck, true, 0600) \
 	param(bool, load_detect_test, false, 0600) \
 	param(bool, force_reset_modeset_test, false, 0600) \
diff --git a/drivers/gpu/drm/i915/i915_pci.c b/drivers/gpu/drm/i915/i915_pci.c
index e81fa5b903a1..f1344db626f5 100644
--- a/drivers/gpu/drm/i915/i915_pci.c
+++ b/drivers/gpu/drm/i915/i915_pci.c
@@ -423,8 +423,7 @@ static const struct intel_device_info ilk_m_info = {
 	.has_coherent_ggtt = true, \
 	.has_llc = 1, \
 	.has_rc6 = 1, \
-	/* snb does support rc6p, but enabling it causes various issues */ \
-	.has_rc6p = 0, \
+	.has_rc6p = 1, \
 	.has_rps = true, \
 	.dma_mask_size = 40, \
 	.__runtime.ppgtt_type = INTEL_PPGTT_ALIASING, \
@@ -898,7 +897,6 @@ static const struct intel_device_info tgl_info = {
 	.display.has_modular_fia = 1,
 	.__runtime.platform_engine_mask =
 		BIT(RCS0) | BIT(BCS0) | BIT(VECS0) | BIT(VCS0) | BIT(VCS2),
-	.has_sriov = 1,
 };
 
 static const struct intel_device_info rkl_info = {
@@ -945,7 +943,6 @@ static const struct intel_device_info adl_s_info = {
 	.__runtime.platform_engine_mask =
 		BIT(RCS0) | BIT(BCS0) | BIT(VECS0) | BIT(VCS0) | BIT(VCS2),
 	.dma_mask_size = 39,
-	.has_sriov = 1,
 };
 
 #define XE_LPD_FEATURES \
@@ -1003,7 +1000,6 @@ static const struct intel_device_info adl_p_info = {
 		BIT(RCS0) | BIT(BCS0) | BIT(VECS0) | BIT(VCS0) | BIT(VCS2),
 	.__runtime.ppgtt_size = 48,
 	.dma_mask_size = 39,
-	.has_sriov = 1,
 };
 
 #undef GEN
@@ -1079,6 +1075,7 @@ static const struct intel_device_info dg2_info = {
 	XE_LPD_FEATURES,
 	.__runtime.cpu_transcoder_mask = BIT(TRANSCODER_A) | BIT(TRANSCODER_B) |
 			       BIT(TRANSCODER_C) | BIT(TRANSCODER_D),
+	.require_force_probe = 1,
 };
 
 static const struct intel_device_info ats_m_info = {
@@ -1246,9 +1243,6 @@ static void i915_pci_remove(struct pci_dev *pdev)
 	if (!i915) /* driver load aborted, nothing to cleanup */
 		return;
 
-	if (IS_SRIOV_PF(i915))
-		i915_sriov_pf_disable_vfs(i915);
-
 	i915_driver_remove(i915);
 	pci_set_drvdata(pdev, NULL);
 }
@@ -1322,13 +1316,12 @@ static int i915_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		return -ENODEV;
 	}
 
-	/*
-	 * Don't bind to non-zero function, unless it is a virtual function.
-	 * Early generations used function 1 as a placeholder for multi-head.
-	 * This causes us confusion instead, especially on the systems where
-	 * both functions have the same PCI-ID!
+	/* Only bind to function 0 of the device. Early generations
+	 * used function 1 as a placeholder for multi-head. This causes
+	 * us confusion instead, especially on the systems where both
+	 * functions have the same PCI-ID!
 	 */
-	if (PCI_FUNC(pdev->devfn) && !pdev->is_virtfn)
+	if (PCI_FUNC(pdev->devfn))
 		return -ENODEV;
 
 	if (!intel_mmio_bar_valid(pdev, intel_info))
@@ -1366,47 +1359,9 @@ static void i915_pci_shutdown(struct pci_dev *pdev)
 {
 	struct drm_i915_private *i915 = pci_get_drvdata(pdev);
 
-	if (IS_SRIOV_PF(i915))
-		i915_sriov_pf_disable_vfs(i915);
-
 	i915_driver_shutdown(i915);
 }
 
-/**
- * i915_pci_sriov_configure - Configure SR-IOV (enable/disable VFs).
- * @pdev: pci_dev struct
- * @num_vfs: number of VFs to enable (or zero to disable all)
- *
- * This function will be called when user requests SR-IOV configuration via the
- * sysfs interface. Note that VFs configuration can be done only on the PF and
- * after successful PF initialization.
- *
- * Return: number of configured VFs or a negative error code on failure.
- */
-static int i915_pci_sriov_configure(struct pci_dev *pdev, int num_vfs)
-{
-	struct drm_device *dev = pci_get_drvdata(pdev);
-	struct drm_i915_private *i915 = to_i915(dev);
-	int ret;
-
-	/* handled in drivers/pci/pci-sysfs.c */
-	GEM_BUG_ON(num_vfs < 0);
-	GEM_BUG_ON(num_vfs > U16_MAX);
-	GEM_BUG_ON(num_vfs > pci_sriov_get_totalvfs(pdev));
-	GEM_BUG_ON(num_vfs && pci_num_vf(pdev));
-	GEM_BUG_ON(!num_vfs && !pci_num_vf(pdev));
-
-	if (!IS_SRIOV_PF(i915))
-		return -ENODEV;
-
-	if (num_vfs > 0)
-		ret = i915_sriov_pf_enable_vfs(i915, num_vfs);
-	else
-		ret = i915_sriov_pf_disable_vfs(i915);
-
-	return ret;
-}
-
 static struct pci_driver i915_pci_driver = {
 	.name = DRIVER_NAME,
 	.id_table = pciidlist,
@@ -1414,7 +1369,6 @@ static struct pci_driver i915_pci_driver = {
 	.remove = i915_pci_remove,
 	.shutdown = i915_pci_shutdown,
 	.driver.pm = &i915_pm_ops,
-	.sriov_configure = i915_pci_sriov_configure,
 };
 
 int i915_pci_register_driver(void)
diff --git a/drivers/gpu/drm/i915/i915_perf.c b/drivers/gpu/drm/i915/i915_perf.c
index 23769588b5eb..0605a7a2623e 100644
--- a/drivers/gpu/drm/i915/i915_perf.c
+++ b/drivers/gpu/drm/i915/i915_perf.c
@@ -4270,13 +4270,13 @@ int i915_perf_add_config_ioctl(struct drm_device *dev, void *data,
 		err = oa_config->id;
 		goto sysfs_err;
 	}
-	id = oa_config->id;
+
+	mutex_unlock(&perf->metrics_lock);
 
 	drm_dbg(&perf->i915->drm,
 		"Added config %s id=%i\n", oa_config->uuid, oa_config->id);
-	mutex_unlock(&perf->metrics_lock);
 
-	return id;
+	return oa_config->id;
 
 sysfs_err:
 	mutex_unlock(&perf->metrics_lock);
@@ -4451,9 +4451,6 @@ void i915_perf_init(struct drm_i915_private *i915)
 
 	/* XXX const struct i915_perf_ops! */
 
-	if (IS_SRIOV_VF(i915))
-		return;
-
 	/* i915_perf is not enabled for DG2 yet */
 	if (IS_DG2(i915))
 		return;
diff --git a/drivers/gpu/drm/i915/i915_pmu.c b/drivers/gpu/drm/i915/i915_pmu.c
index 298422990aca..91d4940f24b4 100644
--- a/drivers/gpu/drm/i915/i915_pmu.c
+++ b/drivers/gpu/drm/i915/i915_pmu.c
@@ -1148,7 +1148,7 @@ void i915_pmu_register(struct drm_i915_private *i915)
 
 	int ret = -ENOMEM;
 
-	if (GRAPHICS_VER(i915) <= 2 || IS_SRIOV_VF(i915)) {
+	if (GRAPHICS_VER(i915) <= 2) {
 		drm_info(&i915->drm, "PMU not supported for this GPU.");
 		return;
 	}
diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index f2cda99b2b21..ff2c1341e87b 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -5664,14 +5664,6 @@
 #define  GEN11_HOTPLUG_CTL_SHORT_DETECT(hpd_pin)	(1 << (_HPD_PIN_TC(hpd_pin) * 4))
 #define  GEN11_HOTPLUG_CTL_NO_DETECT(hpd_pin)		(0 << (_HPD_PIN_TC(hpd_pin) * 4))
 
-/* VF_CAPABILITY_REGISTER */
-#define GEN12_VF_CAP_REG		_MMIO(0x1901f8)
-#define   GEN12_VF			REG_BIT(0)
-
-/* VIRTUALIZATION CONTROL REGISTER */
-#define GEN12_VIRTUAL_CTRL_REG		_MMIO(0x10108C)
-#define   GEN12_GUEST_GTT_UPDATE_EN	REG_BIT(8)
-
 #define ILK_DISPLAY_CHICKEN2	_MMIO(0x42004)
 /* Required on all Ironlake and Sandybridge according to the B-Spec. */
 #define  ILK_ELPIN_409_SELECT	(1 << 25)
diff --git a/drivers/gpu/drm/i915/i915_switcheroo.c b/drivers/gpu/drm/i915/i915_switcheroo.c
index f45bd6b6cede..23777d500cdf 100644
--- a/drivers/gpu/drm/i915/i915_switcheroo.c
+++ b/drivers/gpu/drm/i915/i915_switcheroo.c
@@ -19,10 +19,6 @@ static void i915_switcheroo_set_state(struct pci_dev *pdev,
 		dev_err(&pdev->dev, "DRM not initialized, aborting switch.\n");
 		return;
 	}
-	if (!HAS_DISPLAY(i915)) {
-		dev_err(&pdev->dev, "Device state not initialized, aborting switch.\n");
-		return;
-	}
 
 	if (state == VGA_SWITCHEROO_ON) {
 		drm_info(&i915->drm, "switched on\n");
@@ -48,7 +44,7 @@ static bool i915_switcheroo_can_switch(struct pci_dev *pdev)
 	 * locking inversion with the driver load path. And the access here is
 	 * completely racy anyway. So don't bother with locking for now.
 	 */
-	return i915 && HAS_DISPLAY(i915) && atomic_read(&i915->drm.open_count) == 0;
+	return i915 && atomic_read(&i915->drm.open_count) == 0;
 }
 
 static const struct vga_switcheroo_client_ops i915_switcheroo_ops = {
diff --git a/drivers/gpu/drm/i915/i915_sysfs.c b/drivers/gpu/drm/i915/i915_sysfs.c
index f3679a4e0a1e..1e2750210831 100644
--- a/drivers/gpu/drm/i915/i915_sysfs.c
+++ b/drivers/gpu/drm/i915/i915_sysfs.c
@@ -34,10 +34,8 @@
 #include "gt/intel_rc6.h"
 #include "gt/intel_rps.h"
 #include "gt/sysfs_engines.h"
-#include "gt/iov/intel_iov_sysfs.h"
 
 #include "i915_drv.h"
-#include "i915_sriov_sysfs.h"
 #include "i915_sysfs.h"
 #include "intel_pm.h"
 
@@ -257,24 +255,17 @@ void i915_setup_sysfs(struct drm_i915_private *dev_priv)
 		drm_warn(&dev_priv->drm,
 			 "failed to register GT sysfs directory\n");
 
-	i915_sriov_sysfs_setup(dev_priv);
-
 	i915_setup_error_capture(kdev);
 
 	intel_engines_add_sysfs(dev_priv);
-
-	intel_iov_sysfs_setup(&to_gt(dev_priv)->iov);
 }
 
 void i915_teardown_sysfs(struct drm_i915_private *dev_priv)
 {
 	struct device *kdev = dev_priv->drm.primary->kdev;
 
-	intel_iov_sysfs_teardown(&to_gt(dev_priv)->iov);
 	i915_teardown_error_capture(kdev);
 
-	i915_sriov_sysfs_teardown(dev_priv);
-
 	device_remove_bin_file(kdev,  &dpf_attrs_1);
 	device_remove_bin_file(kdev,  &dpf_attrs);
 
diff --git a/drivers/gpu/drm/i915/i915_utils.c b/drivers/gpu/drm/i915/i915_utils.c
index 28446c545d75..29fd02bf5ea8 100644
--- a/drivers/gpu/drm/i915/i915_utils.c
+++ b/drivers/gpu/drm/i915/i915_utils.c
@@ -125,61 +125,3 @@ bool i915_vtd_active(struct drm_i915_private *i915)
 	/* Running as a guest, we assume the host is enforcing VT'd */
 	return i915_run_as_guest();
 }
-
-/**
- * from_user_to_u32array - convert user input into array of u32
- * @from: user input
- * @count: number of characters to read
- * @array: array with results
- * @size: size of the array
- *
- * We expect input formatted as comma-separated list of integer values.
- *
- * Returns number of entries parsed or negative errno on failure.
- */
-int from_user_to_u32array(const char __user *from, size_t count,
-			  u32 *array, unsigned int size)
-{
-	unsigned int num = 0;
-	char *buf, *p, save;
-	int ret;
-
-	/* [(sign + longest representation) + comma] + newline + terminator */
-	if (count > (1 + sizeof(u32) * 8 + 1) * size + 1 + 1)
-		return -EFBIG;
-
-	p = buf = kzalloc(count + 1, GFP_USER);
-	if (!buf)
-		return -ENOMEM;
-
-	if (copy_from_user(buf, from, count)) {
-		ret = -EFAULT;
-		goto out_free;
-	}
-
-	do {
-		int len;
-
-		if (num == size) {
-			ret = -EINVAL;
-			goto out_free;
-		}
-		len = strcspn(p, ",");
-
-		/* nul-terminate and parse */
-		save = p[len];
-		p[len] = '\0';
-
-		ret = kstrtou32(p, 0, &array[num]);
-		if (ret)
-			goto out_free;
-
-		p += len + 1;
-		num++;
-	} while (save == ',');
-
-	ret = num;
-out_free:
-	kfree(buf);
-	return ret;
-}
diff --git a/drivers/gpu/drm/i915/i915_utils.h b/drivers/gpu/drm/i915/i915_utils.h
index dcf733136173..6c14d13364bf 100644
--- a/drivers/gpu/drm/i915/i915_utils.h
+++ b/drivers/gpu/drm/i915/i915_utils.h
@@ -397,9 +397,4 @@ static inline bool i915_run_as_guest(void)
 
 bool i915_vtd_active(struct drm_i915_private *i915);
 
-#define make_u64(hi__, low__)  ((u64)(hi__) << 32 | (u64)(low__))
-
-int from_user_to_u32array(const char __user *from, size_t count,
-			  u32 *array, unsigned int size);
-
 #endif /* !__I915_UTILS_H */
diff --git a/drivers/gpu/drm/i915/intel_device_info.h b/drivers/gpu/drm/i915/intel_device_info.h
index e324c880f649..31fbea5b3d3f 100644
--- a/drivers/gpu/drm/i915/intel_device_info.h
+++ b/drivers/gpu/drm/i915/intel_device_info.h
@@ -173,7 +173,6 @@ enum intel_ppgtt_type {
 	func(has_rps); \
 	func(has_runtime_pm); \
 	func(has_snoop); \
-	func(has_sriov); \
 	func(has_coherent_ggtt); \
 	func(tuning_thread_rr_after_dep); \
 	func(unfenced_needs_alignment); \
diff --git a/drivers/gpu/drm/i915/intel_pci_config.h b/drivers/gpu/drm/i915/intel_pci_config.h
index 2a347e1943b9..4977a524ce6f 100644
--- a/drivers/gpu/drm/i915/intel_pci_config.h
+++ b/drivers/gpu/drm/i915/intel_pci_config.h
@@ -13,11 +13,6 @@
 #define GTT_APERTURE_BAR			GFXMEM_BAR
 #define GEN12_LMEM_BAR				GFXMEM_BAR
 
-#ifdef CONFIG_PCI_IOV
-#define GEN12_VF_GTTMMADR_BAR			(PCI_IOV_RESOURCES + GTTMMADR_BAR)
-#define GEN12_VF_LMEM_BAR			(PCI_IOV_RESOURCES + GEN12_LMEM_BAR)
-#endif
-
 /* BSM in include/drm/i915_drm.h */
 
 #define MCHBAR_I915				0x44
diff --git a/drivers/gpu/drm/i915/intel_uncore.c b/drivers/gpu/drm/i915/intel_uncore.c
index d4ea11c9bc7d..5cd423c7b646 100644
--- a/drivers/gpu/drm/i915/intel_uncore.c
+++ b/drivers/gpu/drm/i915/intel_uncore.c
@@ -1952,43 +1952,6 @@ __vgpu_write(8)
 __vgpu_write(16)
 __vgpu_write(32)
 
-static int __vf_runtime_reg_cmp(u32 key, const struct vf_runtime_reg *reg)
-{
-	u32 offset = reg->offset;
-
-	if (key < offset)
-		return -1;
-	else if (key > offset)
-		return 1;
-	else
-		return 0;
-}
-
-static const struct vf_runtime_reg *
-__vf_runtime_reg_find(struct drm_i915_private *i915, u32 offset)
-{
-	const struct vf_runtime_reg *regs = to_gt(i915)->iov.vf.runtime.regs;
-	u32 regs_num = to_gt(i915)->iov.vf.runtime.regs_size;
-
-	return BSEARCH(offset, regs, regs_num, __vf_runtime_reg_cmp);
-}
-
-#define __vf_read(x) \
-static u##x vf_read##x(struct intel_uncore *uncore, \
-		       i915_reg_t reg, bool trace) \
-{ \
-	u32 offset = i915_mmio_reg_offset(reg); \
-	const struct vf_runtime_reg *vf_reg = __vf_runtime_reg_find(uncore->i915, offset); \
-	if (vf_reg) \
-		return vf_reg->value; \
-	return gen2_read##x(uncore, reg, trace); \
-}
-
-__vf_read(8)
-__vf_read(16)
-__vf_read(32)
-#define vf_read64 gen2_read64 /* no support for 64 */
-
 #define ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, x) \
 do { \
 	(uncore)->funcs.mmio_writeb = x##_write8; \
@@ -2331,9 +2294,6 @@ static void uncore_raw_init(struct intel_uncore *uncore)
 	} else if (GRAPHICS_VER(uncore->i915) == 5) {
 		ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, gen5);
 		ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, gen5);
-	} else if (IS_SRIOV_VF(uncore->i915)) {
-		ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, gen2);
-		ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, vf);
 	} else {
 		ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, gen2);
 		ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, gen2);
@@ -2417,9 +2377,7 @@ int intel_uncore_init_mmio(struct intel_uncore *uncore)
 		return -ENODEV;
 	}
 
-	if (GRAPHICS_VER(i915) > 5 &&
-	    !IS_SRIOV_VF(i915) &&
-	    !intel_vgpu_active(i915))
+	if (GRAPHICS_VER(i915) > 5 && !intel_vgpu_active(i915))
 		uncore->flags |= UNCORE_HAS_FORCEWAKE;
 
 	if (!intel_uncore_has_forcewake(uncore)) {
diff --git a/drivers/gpu/drm/i915/selftests/i915_live_selftests.h b/drivers/gpu/drm/i915/selftests/i915_live_selftests.h
index 1559e3ca0818..aaf8a380e5c7 100644
--- a/drivers/gpu/drm/i915/selftests/i915_live_selftests.h
+++ b/drivers/gpu/drm/i915/selftests/i915_live_selftests.h
@@ -50,8 +50,5 @@ selftest(slpc, intel_slpc_live_selftests)
 selftest(guc, intel_guc_live_selftests)
 selftest(guc_multi_lrc, intel_guc_multi_lrc_live_selftests)
 selftest(guc_hang, intel_guc_hang_check)
-selftest(iov_provisioning, selftest_live_iov_provisioning)
-selftest(iov_relay, selftest_live_iov_relay)
-selftest(iov_ggtt, intel_iov_ggtt_live_selftests)
 /* Here be dragons: keep last to run last! */
 selftest(late_gt_pm, intel_gt_pm_late_selftests)
diff --git a/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h b/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
index ceb0c5713037..0c22e0fc9059 100644
--- a/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
+++ b/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
@@ -33,4 +33,3 @@ selftest(evict, i915_gem_evict_mock_selftests)
 selftest(gtt, i915_gem_gtt_mock_selftests)
 selftest(hugepages, i915_gem_huge_page_mock_selftests)
 selftest(memory_region, intel_memory_region_mock_selftests)
-selftest(iov_relay, selftest_mock_iov_relay)
diff --git a/drivers/gpu/drm/i915/selftests/i915_perf_selftests.h b/drivers/gpu/drm/i915/selftests/i915_perf_selftests.h
index 6dbde97ea17b..058450d351f7 100644
--- a/drivers/gpu/drm/i915/selftests/i915_perf_selftests.h
+++ b/drivers/gpu/drm/i915/selftests/i915_perf_selftests.h
@@ -19,4 +19,3 @@ selftest(engine_cs, intel_engine_cs_perf_selftests)
 selftest(request, i915_request_perf_selftests)
 selftest(migrate, intel_migrate_perf_selftests)
 selftest(region, intel_memory_region_perf_selftests)
-selftest(iov_relay, selftest_perf_iov_relay)
diff --git a/drivers/gpu/drm/i915/selftests/i915_request.c b/drivers/gpu/drm/i915/selftests/i915_request.c
index a46350c37e9d..818a4909c1f3 100644
--- a/drivers/gpu/drm/i915/selftests/i915_request.c
+++ b/drivers/gpu/drm/i915/selftests/i915_request.c
@@ -299,18 +299,9 @@ __live_request_alloc(struct intel_context *ce)
 	return intel_context_create_request(ce);
 }
 
-struct smoke_thread {
-	struct kthread_worker *worker;
-	struct kthread_work work;
-	struct smoketest *t;
-	bool stop;
-	int result;
-};
-
-static void __igt_breadcrumbs_smoketest(struct kthread_work *work)
+static int __igt_breadcrumbs_smoketest(void *arg)
 {
-	struct smoke_thread *thread = container_of(work, typeof(*thread), work);
-	struct smoketest *t = thread->t;
+	struct smoketest *t = arg;
 	const unsigned int max_batch = min(t->ncontexts, t->max_batch) - 1;
 	const unsigned int total = 4 * t->ncontexts + 1;
 	unsigned int num_waits = 0, num_fences = 0;
@@ -329,10 +320,8 @@ static void __igt_breadcrumbs_smoketest(struct kthread_work *work)
 	 */
 
 	requests = kcalloc(total, sizeof(*requests), GFP_KERNEL);
-	if (!requests) {
-		thread->result = -ENOMEM;
-		return;
-	}
+	if (!requests)
+		return -ENOMEM;
 
 	order = i915_random_order(total, &prng);
 	if (!order) {
@@ -340,7 +329,7 @@ static void __igt_breadcrumbs_smoketest(struct kthread_work *work)
 		goto out_requests;
 	}
 
-	while (!READ_ONCE(thread->stop)) {
+	while (!kthread_should_stop()) {
 		struct i915_sw_fence *submit, *wait;
 		unsigned int n, count;
 
@@ -448,7 +437,7 @@ static void __igt_breadcrumbs_smoketest(struct kthread_work *work)
 	kfree(order);
 out_requests:
 	kfree(requests);
-	thread->result = err;
+	return err;
 }
 
 static int mock_breadcrumbs_smoketest(void *arg)
@@ -461,7 +450,7 @@ static int mock_breadcrumbs_smoketest(void *arg)
 		.request_alloc = __mock_request_alloc
 	};
 	unsigned int ncpus = num_online_cpus();
-	struct smoke_thread *threads;
+	struct task_struct **threads;
 	unsigned int n;
 	int ret = 0;
 
@@ -490,37 +479,28 @@ static int mock_breadcrumbs_smoketest(void *arg)
 	}
 
 	for (n = 0; n < ncpus; n++) {
-		struct kthread_worker *worker;
-
-		worker = kthread_create_worker(0, "igt/%d", n);
-		if (IS_ERR(worker)) {
-			ret = PTR_ERR(worker);
+		threads[n] = kthread_run(__igt_breadcrumbs_smoketest,
+					 &t, "igt/%d", n);
+		if (IS_ERR(threads[n])) {
+			ret = PTR_ERR(threads[n]);
 			ncpus = n;
 			break;
 		}
 
-		threads[n].worker = worker;
-		threads[n].t = &t;
-		threads[n].stop = false;
-		threads[n].result = 0;
-
-		kthread_init_work(&threads[n].work,
-				  __igt_breadcrumbs_smoketest);
-		kthread_queue_work(worker, &threads[n].work);
+		get_task_struct(threads[n]);
 	}
 
+	yield(); /* start all threads before we begin */
 	msleep(jiffies_to_msecs(i915_selftest.timeout_jiffies));
 
 	for (n = 0; n < ncpus; n++) {
 		int err;
 
-		WRITE_ONCE(threads[n].stop, true);
-		kthread_flush_work(&threads[n].work);
-		err = READ_ONCE(threads[n].result);
+		err = kthread_stop(threads[n]);
 		if (err < 0 && !ret)
 			ret = err;
 
-		kthread_destroy_worker(threads[n].worker);
+		put_task_struct(threads[n]);
 	}
 	pr_info("Completed %lu waits for %lu fence across %d cpus\n",
 		atomic_long_read(&t.num_waits),
@@ -1439,18 +1419,9 @@ static int live_sequential_engines(void *arg)
 	return err;
 }
 
-struct parallel_thread {
-	struct kthread_worker *worker;
-	struct kthread_work work;
-	struct intel_engine_cs *engine;
-	int result;
-};
-
-static void __live_parallel_engine1(struct kthread_work *work)
+static int __live_parallel_engine1(void *arg)
 {
-	struct parallel_thread *thread =
-		container_of(work, typeof(*thread), work);
-	struct intel_engine_cs *engine = thread->engine;
+	struct intel_engine_cs *engine = arg;
 	IGT_TIMEOUT(end_time);
 	unsigned long count;
 	int err = 0;
@@ -1481,14 +1452,12 @@ static void __live_parallel_engine1(struct kthread_work *work)
 	intel_engine_pm_put(engine);
 
 	pr_info("%s: %lu request + sync\n", engine->name, count);
-	thread->result = err;
+	return err;
 }
 
-static void __live_parallel_engineN(struct kthread_work *work)
+static int __live_parallel_engineN(void *arg)
 {
-	struct parallel_thread *thread =
-		container_of(work, typeof(*thread), work);
-	struct intel_engine_cs *engine = thread->engine;
+	struct intel_engine_cs *engine = arg;
 	IGT_TIMEOUT(end_time);
 	unsigned long count;
 	int err = 0;
@@ -1510,7 +1479,7 @@ static void __live_parallel_engineN(struct kthread_work *work)
 	intel_engine_pm_put(engine);
 
 	pr_info("%s: %lu requests\n", engine->name, count);
-	thread->result = err;
+	return err;
 }
 
 static bool wake_all(struct drm_i915_private *i915)
@@ -1536,11 +1505,9 @@ static int wait_for_all(struct drm_i915_private *i915)
 	return -ETIME;
 }
 
-static void __live_parallel_spin(struct kthread_work *work)
+static int __live_parallel_spin(void *arg)
 {
-	struct parallel_thread *thread =
-		container_of(work, typeof(*thread), work);
-	struct intel_engine_cs *engine = thread->engine;
+	struct intel_engine_cs *engine = arg;
 	struct igt_spinner spin;
 	struct i915_request *rq;
 	int err = 0;
@@ -1553,8 +1520,7 @@ static void __live_parallel_spin(struct kthread_work *work)
 
 	if (igt_spinner_init(&spin, engine->gt)) {
 		wake_all(engine->i915);
-		thread->result = -ENOMEM;
-		return;
+		return -ENOMEM;
 	}
 
 	intel_engine_pm_get(engine);
@@ -1587,22 +1553,22 @@ static void __live_parallel_spin(struct kthread_work *work)
 
 out_spin:
 	igt_spinner_fini(&spin);
-	thread->result = err;
+	return err;
 }
 
 static int live_parallel_engines(void *arg)
 {
 	struct drm_i915_private *i915 = arg;
-	static void (* const func[])(struct kthread_work *) = {
+	static int (* const func[])(void *arg) = {
 		__live_parallel_engine1,
 		__live_parallel_engineN,
 		__live_parallel_spin,
 		NULL,
 	};
 	const unsigned int nengines = num_uabi_engines(i915);
-	struct parallel_thread *threads;
 	struct intel_engine_cs *engine;
-	void (* const *fn)(struct kthread_work *);
+	int (* const *fn)(void *arg);
+	struct task_struct **tsk;
 	int err = 0;
 
 	/*
@@ -1610,8 +1576,8 @@ static int live_parallel_engines(void *arg)
 	 * tests that we load up the system maximally.
 	 */
 
-	threads = kcalloc(nengines, sizeof(*threads), GFP_KERNEL);
-	if (!threads)
+	tsk = kcalloc(nengines, sizeof(*tsk), GFP_KERNEL);
+	if (!tsk)
 		return -ENOMEM;
 
 	for (fn = func; !err && *fn; fn++) {
@@ -1628,44 +1594,37 @@ static int live_parallel_engines(void *arg)
 
 		idx = 0;
 		for_each_uabi_engine(engine, i915) {
-			struct kthread_worker *worker;
-
-			worker = kthread_create_worker(0, "igt/parallel:%s",
-						       engine->name);
-			if (IS_ERR(worker)) {
-				err = PTR_ERR(worker);
+			tsk[idx] = kthread_run(*fn, engine,
+					       "igt/parallel:%s",
+					       engine->name);
+			if (IS_ERR(tsk[idx])) {
+				err = PTR_ERR(tsk[idx]);
 				break;
 			}
-
-			threads[idx].worker = worker;
-			threads[idx].result = 0;
-			threads[idx].engine = engine;
-
-			kthread_init_work(&threads[idx].work, *fn);
-			kthread_queue_work(worker, &threads[idx].work);
-			idx++;
+			get_task_struct(tsk[idx++]);
 		}
 
+		yield(); /* start all threads before we kthread_stop() */
+
 		idx = 0;
 		for_each_uabi_engine(engine, i915) {
 			int status;
 
-			if (!threads[idx].worker)
+			if (IS_ERR(tsk[idx]))
 				break;
 
-			kthread_flush_work(&threads[idx].work);
-			status = READ_ONCE(threads[idx].result);
+			status = kthread_stop(tsk[idx]);
 			if (status && !err)
 				err = status;
 
-			kthread_destroy_worker(threads[idx++].worker);
+			put_task_struct(tsk[idx++]);
 		}
 
 		if (igt_live_test_end(&t))
 			err = -EIO;
 	}
 
-	kfree(threads);
+	kfree(tsk);
 	return err;
 }
 
@@ -1713,7 +1672,7 @@ static int live_breadcrumbs_smoketest(void *arg)
 	const unsigned int ncpus = num_online_cpus();
 	unsigned long num_waits, num_fences;
 	struct intel_engine_cs *engine;
-	struct smoke_thread *threads;
+	struct task_struct **threads;
 	struct igt_live_test live;
 	intel_wakeref_t wakeref;
 	struct smoketest *smoke;
@@ -1787,26 +1746,23 @@ static int live_breadcrumbs_smoketest(void *arg)
 			 smoke[idx].max_batch, engine->name);
 
 		for (n = 0; n < ncpus; n++) {
-			unsigned int i = idx * ncpus + n;
-			struct kthread_worker *worker;
+			struct task_struct *tsk;
 
-			worker = kthread_create_worker(0, "igt/%d.%d", idx, n);
-			if (IS_ERR(worker)) {
-				ret = PTR_ERR(worker);
+			tsk = kthread_run(__igt_breadcrumbs_smoketest,
+					  &smoke[idx], "igt/%d.%d", idx, n);
+			if (IS_ERR(tsk)) {
+				ret = PTR_ERR(tsk);
 				goto out_flush;
 			}
 
-			threads[i].worker = worker;
-			threads[i].t = &smoke[idx];
-
-			kthread_init_work(&threads[i].work,
-					  __igt_breadcrumbs_smoketest);
-			kthread_queue_work(worker, &threads[i].work);
+			get_task_struct(tsk);
+			threads[idx * ncpus + n] = tsk;
 		}
 
 		idx++;
 	}
 
+	yield(); /* start all threads before we begin */
 	msleep(jiffies_to_msecs(i915_selftest.timeout_jiffies));
 
 out_flush:
@@ -1815,19 +1771,17 @@ static int live_breadcrumbs_smoketest(void *arg)
 	num_fences = 0;
 	for_each_uabi_engine(engine, i915) {
 		for (n = 0; n < ncpus; n++) {
-			unsigned int i = idx * ncpus + n;
+			struct task_struct *tsk = threads[idx * ncpus + n];
 			int err;
 
-			if (!threads[i].worker)
+			if (!tsk)
 				continue;
 
-			WRITE_ONCE(threads[i].stop, true);
-			kthread_flush_work(&threads[i].work);
-			err = READ_ONCE(threads[i].result);
+			err = kthread_stop(tsk);
 			if (err < 0 && !ret)
 				ret = err;
 
-			kthread_destroy_worker(threads[i].worker);
+			put_task_struct(tsk);
 		}
 
 		num_waits += atomic_long_read(&smoke[idx].num_waits);
@@ -2937,18 +2891,9 @@ static int perf_series_engines(void *arg)
 	return err;
 }
 
-struct p_thread {
-	struct perf_stats p;
-	struct kthread_worker *worker;
-	struct kthread_work work;
-	struct intel_engine_cs *engine;
-	int result;
-};
-
-static void p_sync0(struct kthread_work *work)
+static int p_sync0(void *arg)
 {
-	struct p_thread *thread = container_of(work, typeof(*thread), work);
-	struct perf_stats *p = &thread->p;
+	struct perf_stats *p = arg;
 	struct intel_engine_cs *engine = p->engine;
 	struct intel_context *ce;
 	IGT_TIMEOUT(end_time);
@@ -2957,16 +2902,13 @@ static void p_sync0(struct kthread_work *work)
 	int err = 0;
 
 	ce = intel_context_create(engine);
-	if (IS_ERR(ce)) {
-		thread->result = PTR_ERR(ce);
-		return;
-	}
+	if (IS_ERR(ce))
+		return PTR_ERR(ce);
 
 	err = intel_context_pin(ce);
 	if (err) {
 		intel_context_put(ce);
-		thread->result = err;
-		return;
+		return err;
 	}
 
 	if (intel_engine_supports_stats(engine)) {
@@ -3016,13 +2958,12 @@ static void p_sync0(struct kthread_work *work)
 
 	intel_context_unpin(ce);
 	intel_context_put(ce);
-	thread->result = err;
+	return err;
 }
 
-static void p_sync1(struct kthread_work *work)
+static int p_sync1(void *arg)
 {
-	struct p_thread *thread = container_of(work, typeof(*thread), work);
-	struct perf_stats *p = &thread->p;
+	struct perf_stats *p = arg;
 	struct intel_engine_cs *engine = p->engine;
 	struct i915_request *prev = NULL;
 	struct intel_context *ce;
@@ -3032,16 +2973,13 @@ static void p_sync1(struct kthread_work *work)
 	int err = 0;
 
 	ce = intel_context_create(engine);
-	if (IS_ERR(ce)) {
-		thread->result = PTR_ERR(ce);
-		return;
-	}
+	if (IS_ERR(ce))
+		return PTR_ERR(ce);
 
 	err = intel_context_pin(ce);
 	if (err) {
 		intel_context_put(ce);
-		thread->result = err;
-		return;
+		return err;
 	}
 
 	if (intel_engine_supports_stats(engine)) {
@@ -3093,13 +3031,12 @@ static void p_sync1(struct kthread_work *work)
 
 	intel_context_unpin(ce);
 	intel_context_put(ce);
-	thread->result = err;
+	return err;
 }
 
-static void p_many(struct kthread_work *work)
+static int p_many(void *arg)
 {
-	struct p_thread *thread = container_of(work, typeof(*thread), work);
-	struct perf_stats *p = &thread->p;
+	struct perf_stats *p = arg;
 	struct intel_engine_cs *engine = p->engine;
 	struct intel_context *ce;
 	IGT_TIMEOUT(end_time);
@@ -3108,16 +3045,13 @@ static void p_many(struct kthread_work *work)
 	bool busy;
 
 	ce = intel_context_create(engine);
-	if (IS_ERR(ce)) {
-		thread->result = PTR_ERR(ce);
-		return;
-	}
+	if (IS_ERR(ce))
+		return PTR_ERR(ce);
 
 	err = intel_context_pin(ce);
 	if (err) {
 		intel_context_put(ce);
-		thread->result = err;
-		return;
+		return err;
 	}
 
 	if (intel_engine_supports_stats(engine)) {
@@ -3158,23 +3092,26 @@ static void p_many(struct kthread_work *work)
 
 	intel_context_unpin(ce);
 	intel_context_put(ce);
-	thread->result = err;
+	return err;
 }
 
 static int perf_parallel_engines(void *arg)
 {
 	struct drm_i915_private *i915 = arg;
-	static void (* const func[])(struct kthread_work *) = {
+	static int (* const func[])(void *arg) = {
 		p_sync0,
 		p_sync1,
 		p_many,
 		NULL,
 	};
 	const unsigned int nengines = num_uabi_engines(i915);
-	void (* const *fn)(struct kthread_work *);
 	struct intel_engine_cs *engine;
+	int (* const *fn)(void *arg);
 	struct pm_qos_request qos;
-	struct p_thread *engines;
+	struct {
+		struct perf_stats p;
+		struct task_struct *tsk;
+	} *engines;
 	int err = 0;
 
 	engines = kcalloc(nengines, sizeof(*engines), GFP_KERNEL);
@@ -3197,45 +3134,36 @@ static int perf_parallel_engines(void *arg)
 
 		idx = 0;
 		for_each_uabi_engine(engine, i915) {
-			struct kthread_worker *worker;
-
 			intel_engine_pm_get(engine);
 
 			memset(&engines[idx].p, 0, sizeof(engines[idx].p));
+			engines[idx].p.engine = engine;
 
-			worker = kthread_create_worker(0, "igt:%s",
-						       engine->name);
-			if (IS_ERR(worker)) {
-				err = PTR_ERR(worker);
+			engines[idx].tsk = kthread_run(*fn, &engines[idx].p,
+						       "igt:%s", engine->name);
+			if (IS_ERR(engines[idx].tsk)) {
+				err = PTR_ERR(engines[idx].tsk);
 				intel_engine_pm_put(engine);
 				break;
 			}
-			engines[idx].worker = worker;
-			engines[idx].result = 0;
-			engines[idx].p.engine = engine;
-			engines[idx].engine = engine;
-
-			kthread_init_work(&engines[idx].work, *fn);
-			kthread_queue_work(worker, &engines[idx].work);
-			idx++;
+			get_task_struct(engines[idx++].tsk);
 		}
 
+		yield(); /* start all threads before we kthread_stop() */
+
 		idx = 0;
 		for_each_uabi_engine(engine, i915) {
 			int status;
 
-			if (!engines[idx].worker)
+			if (IS_ERR(engines[idx].tsk))
 				break;
 
-			kthread_flush_work(&engines[idx].work);
-			status = READ_ONCE(engines[idx].result);
+			status = kthread_stop(engines[idx].tsk);
 			if (status && !err)
 				err = status;
 
 			intel_engine_pm_put(engine);
-
-			kthread_destroy_worker(engines[idx].worker);
-			idx++;
+			put_task_struct(engines[idx++].tsk);
 		}
 
 		if (igt_live_test_end(&t))
diff --git a/drivers/gpu/drm/i915/selftests/intel_scheduler_helpers.c b/drivers/gpu/drm/i915/selftests/intel_scheduler_helpers.c
index 2990dd4d4a0d..310fb83c527e 100644
--- a/drivers/gpu/drm/i915/selftests/intel_scheduler_helpers.c
+++ b/drivers/gpu/drm/i915/selftests/intel_scheduler_helpers.c
@@ -28,7 +28,8 @@ struct intel_engine_cs *intel_selftest_find_any_engine(struct intel_gt *gt)
 
 int intel_selftest_modify_policy(struct intel_engine_cs *engine,
 				 struct intel_selftest_saved_policy *saved,
-				 enum selftest_scheduler_modify modify_type)
+				 u32 modify_type)
+
 {
 	int err;
 
diff --git a/drivers/gpu/drm/i915/selftests/mock_gem_device.c b/drivers/gpu/drm/i915/selftests/mock_gem_device.c
index 6387f38ec071..fff11c90f1fa 100644
--- a/drivers/gpu/drm/i915/selftests/mock_gem_device.c
+++ b/drivers/gpu/drm/i915/selftests/mock_gem_device.c
@@ -185,7 +185,6 @@ struct drm_i915_private *mock_gem_device(void)
 
 	spin_lock_init(&i915->gpu_error.lock);
 
-	i915->__mode = I915_IOV_MODE_NONE;
 	i915_gem_init__mm(i915);
 	intel_root_gt_init_early(i915);
 	mock_uncore_init(&i915->uncore, i915);
-- 
2.34.1

